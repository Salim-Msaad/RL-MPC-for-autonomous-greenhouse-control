{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "import casadi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from SHARED.model import *\n",
    "from matplotlib.lines import Line2D\n",
    "from SHARED.display_trajectories import *\n",
    "import time\n",
    "import timeit\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from SHARED.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ODE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "T = (5) #Time horizon (hours)\n",
    "N = (T*3600)//dT #Number of Control Actions\n",
    "F,g = model_functions()\n",
    "stochastic = True\n",
    "num_simulations = 30 if stochastic else 1\n",
    "save_data = True\n",
    "\n",
    "# N = 1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print (N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Functions: $\\newline$\n",
    "J1 = $$\\min_{u(k),y(k)} \\sum_{k = k_0}^{k_0 + N_p} (V(y(k),u(k),d(k)))= \\sum_{k = k_0}^{k_0 + N_p} -q_{y_1} \\cdot y_1(k) + q_{u_1}\\cdot u_1(k) +  q_{u_2}\\cdot u_2(k) + q_{u_3}\\cdot u_3(k)$$\n",
    "$$s.t.$$\n",
    "$$x(k+1) = F(x(k),u(k),d(k),p), \\quad y(k) = g(x(k),p)$$\n",
    "$$u_{min} \\le u(k) \\le u_{max}, \\quad |u(k) - u(k-1)| \\le \\delta u$$\n",
    "$$y_{min}(k) \\le y(k) \\le y_{max}(k), \\quad \\forall k=k_0,\\dots,k0+N_p $$\n",
    "$$x(k_0) = x_0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print (N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "opti = casadi.Opti()\n",
    "mpc_reward = partial(reward_function, return_type = \"DM\")\n",
    "\n",
    "\n",
    "\n",
    "#State Variables\n",
    "X = opti.variable(4,N+1)    #->Drymass              (kg/m2)\n",
    "                            #->Indoor C02           (kg/m3)\n",
    "                            #->Indoor Temp          (deg C)\n",
    "                            #->Indoor Humidity      (kg/m3)\n",
    "#Output Variables\n",
    "Y  = opti.variable(4,N)     #->Drymass              (kg/m2)\n",
    "                            #->Indoor C02           (ppm)\n",
    "                            #->Indoor Temp          (deg C)\n",
    "                            #->Indoor Humidity      (%)\n",
    "#Disturbance Variables\n",
    "D = opti.parameter(4,N)     #->Irradiance           (W/m2)\n",
    "                            #->Outdoor C02          (kg/m3)\n",
    "                            #->Outdoor Temp         (deg C)\n",
    "                            #->Outdoor Humidity     (kg/m3)\n",
    "#Control Variables\n",
    "U = opti.variable(3,N)      #->C02 Injection        (mg/(m2.s))\n",
    "                            #->Ventilation          (mm/s)\n",
    "                            #->Heating              (W/m2)\n",
    "\n",
    "P = opti.variable (4,N)     #Penalties for temp ub and lb, co2 ub and lb                  \n",
    "                            \n",
    "#Initial Values\n",
    "px = opti.parameter(4,1)             #initial state\n",
    "pu = opti.parameter(3,1)             #initial control action\n",
    "\n",
    "# lam_g = opti.\n",
    "# lamx = \n",
    "\n",
    "\n",
    "\n",
    "#Set Parameters\n",
    "opti.set_value(D,casadi.DM.zeros(D.shape))\n",
    "opti.set_value(px,x0)\n",
    "opti.set_value(pu,u0)\n",
    "\n",
    "\n",
    "#Cost Function\n",
    "J = 0\n",
    "\n",
    "#Set Constraints and Cost Function\n",
    "for k in range(0,N):\n",
    "    \n",
    "    \n",
    "    opti.subject_to(X[:,k+1] == F(X[:,k],U[:,k],D[:,k],nominal_params))                                #Dynamic Constraints\n",
    "    opti.subject_to(Y[:,k] == g(X[:,k+1],nominal_params))                                                #Output  Constraints\n",
    "    opti.subject_to(u_min <= (U[:,k] <= u_max))                                         #Input   Contraints\n",
    "    \n",
    "    #Linear penalty functions\n",
    "    opti.subject_to(P[:,k] >= 0)\n",
    "    opti.subject_to(P[0,k] >= pen_c02     * (C02_MIN_CONSTRAIN_MPC - Y[1,k]))   #C02 lb\n",
    "    opti.subject_to(P[1,k] >= pen_c02     * (Y[1,k] - C02_MAX_CONSTRAIN_MPC))   #C02 ub\n",
    "    opti.subject_to(P[2,k] >= pen_temp_lb * (TEMP_MIN_CONSTRAIN_MPC - Y[2,k]))             #Temp lb\n",
    "    opti.subject_to(P[3,k] >= pen_temp_ub * (Y[2,k] - TEMP_MAX_CONSTRAIN_MPC))                  #Temp ub\n",
    "    \n",
    "         \n",
    "    \n",
    "    #Hard Constraints\n",
    "    J -= mpc_reward(delta_drymass=(X[0,k+1]-X[0,k]), control_inputs=U[:,k])       \n",
    "    # J -= mpc_reward(control_inputs=U[:,k])  \n",
    "    J += (P[0,k] + P[1,k] + P[2,k] + P[3,k])    \n",
    "    \n",
    "    # opti.subject_to(C02_MIN_CONSTRAIN_MPC     <= (Y[1,k] <= C02_MAX_CONSTRAIN_MPC))     #C02 min max constraints\n",
    "    # opti.subject_to(T_mins[0,k] <= (Y[2,k]    <= T_maxs[0,k])) #Temp min max constraints\n",
    "    # opti.subject_to(HUM_MIN_CONSTRAIN         <= (Y[3,k] <= HUM_MAX_CONSTRAIN))         #Hum min max constraints\n",
    "    \n",
    "    if k < N-1:                                         \n",
    "        opti.subject_to(-delta_u<=(U[:,k+1] - U[:,k]<=delta_u))               #Change in input Constraint\n",
    "\n",
    "# J -= mpc_reward(delta_drymass=(X[0,N])) \n",
    "opti.subject_to(-delta_u <= (U[:,0]-pu <= delta_u))                             #Initial change in input Constraint\n",
    "opti.subject_to(X[:,0] == px)                                                           #Initial Condition Constraint\n",
    "opti.minimize(J)\n",
    "\n",
    "#Solver --> ipopt, bonmin, snopt, blocksqp,sqpmethod, scpgen\n",
    "opts = {}\n",
    "opts[\"print_time\"] = False\n",
    "opts[\"ipopt.print_level\"]= False\n",
    "opts[\"verbose\"] =  False\n",
    "opts[\"ipopt.max_iter\"] = 5000\n",
    "\n",
    "# opts[\"ipopt.tol\"] = 1 #Default: 0.0001\n",
    "# opts[\"ipopt.constr_viol_tol\"] = 1 #Default: 0.0001\n",
    "# opts[\"ipopt.acceptable_constr_viol_tol\"] = 1 #Default: 0.0001\n",
    "# opts[\"ipopt.acceptable_tol\"] = 1 #Default: 0.0001\n",
    "# opts[\"ipopt.jac_d_constant\"] = 'yes'\n",
    "# opts[\"ipopt.acceptable_tol\"] = 1e-5\n",
    "opts[\"ipopt.bound_relax_factor\"] = 1e-3\n",
    "\n",
    "opts[\"ipopt.nlp_scaling_method\"]       = 'gradient-based'\n",
    "# opts[\"ipopt.warm_start_same_structure\"]       = 'yes'\n",
    "# opts[\"ipopt.warm_start_init_point\"]    = 'yes'\n",
    "opts[\"ipopt.warm_start_entire_iterate\"]    = 'yes'\n",
    "# opts[\"ipopt.bound_push\"] = 0.5\n",
    "# opts[\"ipopt.bound_frac\"] = 0.5\n",
    "# opts[\"ipopt.slack_bound_push\"] = 0.5\n",
    "# opts[\"ipopt.slack_bound_frac\"] = 0.5\n",
    "# opts[\"ipopt.max_cpu_time\"] = 0.1\n",
    "\n",
    "\n",
    "# s_opts = {\"max_cpu_time\": 0.1, \n",
    "# \t\t\t\t  \"print_level\": 0, \n",
    "# \t\t\t\t  \"tol\": 5e-1, \n",
    "# \t\t\t\t  \"dual_inf_tol\": 5.0, \n",
    "# \t\t\t\t  \"constr_viol_tol\": 1e-1,\n",
    "# \t\t\t\t  \"compl_inf_tol\": 1e-1, \n",
    "# \t\t\t\t  \"acceptable_tol\": 1e-2, \n",
    "# \t\t\t\t  \"acceptable_constr_viol_tol\": 0.01, \n",
    "# \t\t\t\t  \"acceptable_dual_inf_tol\": 1e10,\n",
    "# \t\t\t\t  \"acceptable_compl_inf_tol\": 0.01,\n",
    "# \t\t\t\t  \"acceptable_obj_change_tol\": 1e20,\n",
    "# \t\t\t\t  \"diverging_iterates_tol\": 1e20}\n",
    "s_opts = { \"constr_viol_tol\": 0.01,\n",
    "            \"acceptable_constr_viol_tol\": 0.1, \n",
    "            \"acceptable_tol\": 1e-2, \n",
    "}\n",
    "\n",
    "opti.solver('ipopt',opts,s_opts,)\n",
    "# sol = opti.solve()\n",
    "MPC = opti.to_function('MPC',[px,D,pu],[U[:,0],U,X],['px','D','pu'],['u_opt','u_traj','x_traj'])\n",
    "MPC.save('MPC/MPC_controller_'+str(T) + 'hr')\n",
    "\n",
    "# print (opti.debug.value(J))\n",
    "\n",
    "# print (MPC([0.0025,0.001,15,0.008],np.zeros((4,12)),[0,0,50],10*np.ones((1,12)),20*np.ones((1,12))))\n",
    "# MPC =casadi.Function.load('MPC.casadi')\n",
    "# casadi.DM.set_precision(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "get_d = partial(get_disturbance,weather_data = weather_data,start_time=start_time,Np=N, dt=dT)\n",
    "    \n",
    "\n",
    "\n",
    "for sim_num in range(num_simulations):\n",
    "    print (f\"Sim {sim_num}\")\n",
    "    \n",
    "    #Save directories\n",
    "    file_path = \"Stochastic/\" + str (T) + '_hr/scale_'+str(noise_scale) if stochastic else \"Deterministic/\" + str (T)\n",
    "    directory = 'results/MPC/' + file_path + '/Sim_' + str(sim_num) + '/'\n",
    "    if save_data:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "   \n",
    "    #Initial Conditions\n",
    "    x = x0\n",
    "    u_opt = u0\n",
    "    \n",
    "\n",
    "\n",
    "    #Reward evaluation function\n",
    "    evaluate_rewards = partial(reward_evaluation,\n",
    "                        constraint_mins=np.array([C02_MIN_CONSTRAIN_MPC, TEMP_MIN_CONSTRAIN_MPC, HUM_MIN_CONSTRAIN]),\n",
    "                        constraint_maxs=np.array([C02_MAX_CONSTRAIN_MPC, TEMP_MAX_CONSTRAIN_MPC, HUM_MAX_CONSTRAIN]))\n",
    "\n",
    "\n",
    "\n",
    "    #Logging states, outputs, disturbances and control inputs\n",
    "    X_log, D_log, Y_log, U_log = [],[],[],[]\n",
    "\n",
    "\n",
    "    #Logging reward and penalties\n",
    "    cum_reward_log = [0]\n",
    "    cum_penalties = [0]\n",
    "    x_traj = np.zeros((4,N+1))\n",
    "    u_traj = np.zeros((3,N))\n",
    "    total_reward=0\n",
    "    total_penalty=0\n",
    "\n",
    "    #Logging compute time for control action\n",
    "    comp_time_log = []\n",
    "    for k in tqdm(range(0,max_steps)):\n",
    "        \n",
    "        if N == max_steps:\n",
    "            \n",
    "            if k == 0:\n",
    "                print (\"Getting optimal trajectory\")\n",
    "                timer = time.perf_counter()\n",
    "                u_opt,u_traj,x_traj = MPC(x,d,u_opt)\n",
    "                \n",
    "                u_opt  = np.array(u_opt)      \n",
    "                u_traj = np.array(u_traj)\n",
    "                x_traj = np.array(x_traj)\n",
    "                \n",
    "                timer = time.perf_counter() - timer\n",
    "                print (\"Finished Optimizing\")\n",
    "              \n",
    "            d_now  = d[:,k]\n",
    "            u_opt = u_traj[:,k]\n",
    "        else:\n",
    "            d = get_d(k)  \n",
    "            d_now  = d[:,0]\n",
    "            timer = time.perf_counter()\n",
    "            \n",
    "            u_traj = np.roll(u_traj,shift=-1,axis=1)\n",
    "            u_traj[:,-1] = np.copy(u_traj[:,-2])\n",
    "            \n",
    "            x_traj = np.roll(x_traj,shift=-1,axis=1)\n",
    "            x_traj[:,-1] = np.copy(x_traj[:,-2])\n",
    "            \n",
    "            u_opt,u_traj,x_traj = MPC(x,d,u_opt)\n",
    "            u_opt  = np.array(u_opt)      \n",
    "            u_traj = np.array(u_traj)\n",
    "            x_traj = np.array(x_traj)\n",
    "            u_opt = np.clip(u_opt,u_min.reshape(3,1),u_max.reshape(3,1))\n",
    "            timer = time.perf_counter() - timer\n",
    "            \n",
    "        #Evolve State\n",
    "        sys_params = noisy.parametric_uncertainty() if stochastic else nominal_params\n",
    "        x_next = F(x,u_opt,d_now,sys_params).toarray().ravel()\n",
    "        # x_next = noisy.add_noise(x,x_next) if stochastic else x_next\n",
    "        y = g(x_next,sys_params).toarray().ravel()\n",
    "        \n",
    "        #Reward Evaluation\n",
    "        reward, penalties = evaluate_rewards(delta_drymass=x_next[0] - x[0],control_inputs=u_opt,\n",
    "                    outputs2constrain=y[1:],)\n",
    "\n",
    "        #Store Rewards and penalties recieved\n",
    "        rr = reward-penalties\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_penalty += penalties\n",
    "        cum_reward_log.append(cum_reward_log[-1] + reward-penalties)\n",
    "        cum_penalties.append(cum_penalties[-1] + penalties)\n",
    "        \n",
    "    \n",
    "        #Log Values\n",
    "        X_log.append(x_next)\n",
    "        D_log.append(d_now)\n",
    "        Y_log.append(y)\n",
    "        U_log.append(u_opt)\n",
    "        comp_time_log.append(timer)\n",
    "        \n",
    "        #Repeat\n",
    "        x=x_next\n",
    "        \n",
    "    # #Reshaping arrays\n",
    "    U_log = np.array(U_log)\n",
    "    Y_log = np.array(Y_log)\n",
    "    D_log = np.array(D_log)\n",
    "    comp_time_log =np.array(comp_time_log)\n",
    "    \n",
    "    if save_data:\n",
    "        savetxt(os.path.join(directory, 'Y_log.csv'), Y_log[1:], delimiter=',')\n",
    "        savetxt(os.path.join(directory, 'U_log.csv'), U_log[:,:,0], delimiter=',')\n",
    "        savetxt(os.path.join(directory, 'D_log.csv'), D_log, delimiter=',')\n",
    "        savetxt(os.path.join(directory, 'comp_time_log.csv'), comp_time_log, delimiter=',')\n",
    "        savetxt(os.path.join(directory, 'rewards_log.csv'), np.array(cum_reward_log), delimiter=',')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#Printing metrics\n",
    "print (f\"Total Reward/EPI: {total_reward}\")#this should be the same or very similar to epi\n",
    "print (f\"Total Penalty: {total_penalty}\")\n",
    "print_metrics(Y_log[:-1], U_log, D_log, day_range=(0,40), rewards=np.array(cum_reward_log), time_log=comp_time_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
