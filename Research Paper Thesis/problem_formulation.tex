\section{Problem Formulation}

\subsection{RL}
The Soft-Actor-Critic (SAC) algorithm is used in developing the Rl agent and is effective due to its ability to balance exploration and exploitation, handle continuous state and action spaces, and provide stable and efficient learning. SAC leverages both policy gradient methods and value function approximation, incorporating a value function to guide policy improvement and entropy regularization to encourage exploration. Moreover, such an actor-critic method provides both a policy (actor, $\pi (\cdot) $) and a critic (value function, $V_\pi (\cdot)$), that can be incorporated into the MPC's objective function. Interested readers are referred to \cite{SoftActorCriticSpinning} to learn how $\pi(\cdot)$ and $V_\pi(\cdot)$ are learned. It is noted that both the actor and critic are represented by neural networks.

\subsubsection{Agent Description}
The agent's description is influenced by its observation tuple (agent's state), its discount factor and reward function. The model of the environment is the same for all controllers used, i.e. RL, MPC and RL-MPC.

\begin{multline}\label{eq:obs-tuple}
	s(k) = (y_1(k),y_2(k),y_3(k),y_4(k), \\u_1(k-1), u_2(k-1), u_3(k-1), k, d(k))
\end{multline}

The agent's state, \autoref{eq:obs-tuple}, includes all the measurement outputs, the previous control inputs, the current time and weather disturbances. \\
The reward function is modelled after the optimisation goal, as defined in \autoref{eq:optimisation-goal}. State constraints cannot be directly imposed but can be indirectly incorporated by a penalty function in the reward function. It is common practice to impose a linear penalty function for state violations when learning a policy with RL. Therefore, the resulting reward function becomes:

\begin{multline}\label{eq:reward_fn}
	R(k)  =  - (c_{p_1} \cdot (u_1(k)) + c_{p_2} \cdot (u_3(k))) + \\ c_{p_3} \cdot (y_1(k)- y_1(k-1)) \\
	- (P_{c02} \cdot (y_2(k) ) + P_T \cdot (y_3(k)) + P_H \cdot (y_4(k)) )
\end{multline}

where the penalty terms $P_{c02},P_T,P_H$ are defined in \autoref{eq:penalty-terms}:

\begin{equation}\label{eq:penalty-terms}
	\begin{aligned}
		& P_{\text{CO2}} = 
		\begin{cases} 
			c_{p_{\text{CO2}}} \cdot (y_2(k) - y_2^{\text{max}}) & \text{if } y_2(k) > y_2^{\text{max}} , \\
			c_{p_{\text{CO2}}} \cdot (y_2^{\text{min}} - y_2(k)) & \text{if } y_2(k) < y_2^{\text{min}} , \\
			0 & \text{otherwise}
		\end{cases}
		\\
		& P_{T} = 
		\begin{cases} 
			c_{p_{T_{ub}}} \cdot (y_3(k) - y_3^{\text{max}}) & \text{if } y_3(k) > y_3^{\text{max}} , \\
			c_{p_{T_{lb}}} \cdot (y_3^{\text{min}} - y_3(k)) & \text{if } y_3(k) < y_3^{\text{min}} , \\
			0 & \text{otherwise}
		\end{cases}
		\\
		& P_{H} = 
		\begin{cases} 
			c_{p_{H}} \cdot (y_4(k) - y_4^{\text{max}}) & \text{if } y_4(k) > y_4^{\text{max}} , \\
			c_{p_{H}} \cdot (y_4^{\text{min}} - y_4(k)) & \text{if } y_4(k) < y_4^{\text{min}} , \\
			0 & \text{otherwise}
		\end{cases}
		\\
	\end{aligned}
\end{equation}

The penalty constants $c_{p_{CO_2}}, c_{p_{T_{ub}}},c_{p_{T_{lb}}},c_{p_{H}}$ were found empirically in \citet{jansenOptimalControlLettuce2023} to effectively account for deviations from desired states and their impact on the economic benefit. The penalty constants and their respective units are displayed in \autoref{tab:pen-constants}.\\
To accommodate for the control input constraints (\autoref{eq:constraint-u-limits}, \autoref{eq:constraint-delta-u}), the actor ($\pi(\cdot)$) has a continuous action space, denoted as \( \mathcal{A} \),  and is defined as \( \mathcal{A} = [-1, 1]^3 \), where \( \mathcal{A} \subseteq \mathbb{R}^3 \). The agentâ€™s action, denoted as  $a(k) = \pi (s(k))$, where $a \in \mathcal{A}$, is regarded as a modification to the control input. Consequently, the current control input can be determined as follows:
$$
u(k) = \max(u_{\min}, \min(u(k-1) + a(k) \cdot \delta u_{\max}, u_{\max}))
$$

where $\delta u_{max}(k),u_{min}, u_{max}$ are defined in \autoref{eq:constraints}. 


\begin{table}[h]
	\centering
	\begin{tabular}{|>{\bfseries}l|c|c|}
		\hline
		\textbf{Parameter} & \textbf{Value} & \textbf{Units} \\
		\hline
		$c_{p_{\text{CO2}}}$ & $\frac{10^{-3}}{20}$ & \euro$\cdot (ppm \cdot m^2)^{-1}$ \\
		$c_{p_{T_{ub}}}$ & $\frac{1}{200}$ & \euro$\cdot (C^{\circ} \cdot m^2)^{-1}$ \\
		$c_{p_{T_{lb}}}$ & $\frac{1}{300}$ & \euro$\cdot (C^{\circ} \cdot m^2)^{-1}$ \\
		$c_{p_{H}}$ & $\frac{1}{50}$ & \euro$\cdot (RH_{\%} \cdot m^2)^{-1}$ \\
		$y_2^{max}$ & $1600$ & ppm \\
		$y_2^{min}$ & $500$ & ppm \\
		$y_3^{max}$ & $20$ & $C^{\circ}$ \\
		$y_3^{min}$ & $10$ & $C^{\circ}$ \\
		$y_4^{max}$ & $100$ & $RH_{\%}$ \\
		$y_4^{min}$ & $0$ & $RH_{\%}$ \\       
		\hline
	\end{tabular}
	\caption{Penalty Constants}
	\label{tab:pen-constants}
\end{table}

\subsubsection{Agent Training}
Stable baselines 3 (SB3) \cite{raffinStableBaselines3ReliableReinforcement2021} was used to facilitate the development and training of the SAC algorithm. The final hyper-parameters and actor-critic network structures are posted in \autoref{tab:hyper-params} and were found empirically. The defaults provided by SB3 are used for hyper-parameters that are not reported. All random generators in the experiments were seeded with a value of 4 to ensure reproducibility of the results.

\begin{table}[H]
	\centering
	\begin{tabular}{|>{\bfseries}l|c|}
		\hline
		\textbf{Parameter} & \textbf{Value} \\
		\hline
		Training episodes & $100$  \\
		Warm-up episodes &  $9$ \\
		Hidden layers & $2$ \\
		Neurons per hidden layer & $128$ \\
		Batch size & $1024$ \\
		Learning rate & $5 \cdot 10^{-3}$ \\
		Buffer size & $100000$ \\
		Discount Factor & $0.95$ \\
		Activation Function & ReLu \\
		\hline
	\end{tabular}
	\caption{Hyper-parameters}
	\label{tab:hyper-params}
\end{table}

In RL problems with a fixed, long-term horizon, a discount factor ($\gamma$)
of 1 is often desired. This setting ensures that the RL agent's prediction horizon covers the entire growing period, allowing the critic to evaluate and retain information about the full state trajectory and the actor to make long-term decisions. Consequently, the critic can provide accurate value estimations based on long-term outcomes.\\
However, using $\gamma = 1$ can make it challenging to find a competitive policy. To address this, a lower discount factor was employed, which facilitated the discovery of a more effective policy. Nevertheless, this adjustment means that the critic's value estimates are based on discounted returns rather than the true long-term expected return.

To reconcile these issues, we trained a separate critic with $\gamma = 1$ on the fixed policy obtained through SAC. This approach leverages the advantages of a practical policy while ensuring that the critic provides accurate value estimations over the entire trajectory.


\subsubsection{Value Function Learning}
In order to train a value function, multiple trajectories are sampled and for each state visited, the expected return is calculated as per \autoref{eq:return_function}. 

\begin{equation}
	\begin{aligned}
		G_t  = R_{t+1} + R_{t+2} + R_{t+3} + \dots + R_{T} = \sum_{k=0}^TR_{t+k+1}
	\end{aligned}
	\label{eq:return_function}
\end{equation}

All initial states and inputs were uniformly sampled around a region of the nominal trajectory at time $k$. Initial states and inputs were sampled from $\hat{\mathbb{X}^4}$ and $\hat{\mathbb{U}^3}$ and the initial time step $k$ is uniformly sampled across the entire time horizon as shown in \autoref{eq:TR-sample-space}.


\begin{equation}\label{eq:TR-sample-space}
	\begin{split}
		\hat{\mathbb{X}}^4 &= \{ (\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat{x}_4) \mid\ \hat{x}_1 \in [\hat{x}_{1\min}(x_{1_k}), \hat{x}_{1\max}(x_{1_k})], \\
		&\quad \hat{x}_2 \in [\hat{x}_{2\min}(x_{2_k}), \hat{x}_{2\max}(x_{2_k})], \\
		&\quad \hat{x}_3 \in [\hat{x}_{3\min}(x_{3_k}), \hat{x}_{3\max}(x_{3_k})], \\
		&\quad \hat{x}_4 \in [\hat{x}_{4\min}(x_{4_k}), \hat{x}_{4\max}(x_{4_k})] \} \\
		\hat{\mathbb{U}}^3 &= \{ (\hat{u}_1, \hat{u}_2, \hat{u}_3) \mid\ \hat{u}_1 \in [\hat{u}_{1\min}(u_{1_k}), \hat{u}_{1\max}(u_{1_k})], \\
		&\quad \hat{u}_2 \in [\hat{u}_{2\min}(u_{2_k}), \hat{u}_{2\max}(u_{2_k})], \\
		&\quad \hat{u}_3 \in [\hat{u}_{3\min}(u_{3_k}), \hat{u}_{3\max}(u_{3_k})] \} \\
		& k \sim U(0,1919)  \\
	\end{split}
\end{equation}

where the minimum and maximum limits are calculated as per \autoref{eq:min-max-tr-sample-space}

\begin{equation}\label{eq:min-max-tr-sample-space}
	\begin{aligned}
		&\hat{z}_{min} = z_k \cdot (1-\sigma)\\
		&\hat{z}_{max} = z_k \cdot (1+\sigma)
	\end{aligned}
\end{equation}

which represent the minimum and maximum range of the sample state space for a specific state and input where ${z}_{k}$ represents the nominal trajectory, $\sigma$ denotes the desired spread of sampled initial states, which is expressed as a percentage. Given that the actor was found to compute a control action in $0.2 ms$, 1000 trajectories were sampled to achieve appropriate coverage of state and input spaces.
A neural network  was trained with inputs as the state, $s_k$, and labels as the total return,$G_t$ and the loss function in \autoref{eq:vf_tr_loss} is minimised with the Adam optimiser:
\begin{equation}
	\label{eq:vf_tr_loss}
	L(\phi, \mathcal{D}) =   V_{\phi}(s_k) - \mathbb{E}(G_t(s_k))
\end{equation}
where $V_{\phi}$ is the function approximator with weights $\phi$ and $G_t(s_k)$ is the total return of state $s_k$. Parameters include a 2 hidden layer network with 128 neurons per layer, a $1\cdot 10^{-3}$ learning rate and a batch size of 1024 trained on 200 epochs. Initially,  input, $s(k)$, for the value function was the same as that used by the RL agent (\autoref{eq:obs-tuple}). However, it was discovered that the expected return of a state could be reasonably estimated using only the current dry mass and time. This simplification also reduced the non-linearity of the neural network, making it suitable for use in the MPC formulation. While training on the full state of the agent allowed for more accurate predictions of the expected return, the increased non-linearity of the neural network adversely affected the MPC optimizer when integrated as a cost function.


\subsection{MPC}
Similarly to the RL agent, the MPC aims to optimize the objective function defined in \autoref{eq:optimisation-goal}. To facilitate direct comparisons between RL, MPC, and RL-MPC, the same linear penalty constraints, as described in \autoref{eq:penalty-terms}, were used in the form of slack variables. Therefore, the following optimization goal is solved at every time step:
\begin{equation} \label{eq:mpc_ocp}
	\begin{aligned}
		&\min_{u(k),x(k)}  \sum_{k = k_0}^{k_0 + N_p-1} \Bigg[ l(y(k),u(k)) + \sum_{i = 1}^6 s_i(k) \Bigg]  \\
		&\text{s.t.} \quad x(k+1) = f(x(k), u(k), d(k), \mu_p), \\
		 &y(k) = g(x(k+1), \mu_p), \\
		 &-\delta u_{max} \leq u(k) - u(k-1) \leq \delta u_{max}, \\
		 &u_{\min} \leq u(k) \leq u_{\max}, \\
		 &x(k_0) = x_{k_0}, \\
		 &s_i(k) \geq 0, \\
		 &s_1(k) \geq c_{p_{C02}} \cdot (y_2^{min} - y_2(k)), \\& s_2(k) \geq c_{p_{C02}} \cdot (y_2(k) + y_2^{max}), \\ 
		 &s_3(k) \geq c_{p_{T_{lb}}} \cdot (y_3^{min} - y_3(k)),\\& s_4(k) \geq c_{p_{T_{ub}}} \cdot (y_3(k) + y_3^{max}), \\ 
		 &s_5(k) \geq c_{p_{H}} \cdot (y_4^{min} - y_4(k)),\\& s_6(k) \geq c_{p_{H}} \cdot (y_4(k) + y_4^{max}).
	\end{aligned}
\end{equation}

The policy generated by the MPC is denoted $\kappa(x,u,d,p)$ where the optimal control action to take at time $k$ is

\begin{equation}\label{eq:mpc_policy_notation}
	u_k^* = \kappa(x_k,u_{k-1}^*, d_k, \mu_p)
\end{equation}

The open-source software CasADi  \cite{anderssonCasADiSoftwareFramework2019} and solver IPOPT \cite{wachterImplementationInteriorpointFilter2006} are used in Python to solve \autoref{eq:mpc_ocp}.


\subsection{RL-MPC}
According to \citet{ellisTutorialReviewEconomic2014} and \citet{amritEconomicOptimizationUsing2011}, an EMPC without a terminal constraint and terminal cost function faces challenges in proving performance and stability guarantees. Specifically, \citet{ellisTutorialReviewEconomic2014} asserts that a terminal point constraint is necessary to ensure closed-loop performance. Additionally, \citet{amritEconomicOptimizationUsing2011} argues that a terminal cost function with a terminal region constraint is superior to a terminal point constraint, as it enlarges the feasible set of initial conditions and may enhance closed-loop performance. However, identifying suitable terminal constraints and cost functions is challenging. This study aims to determine whether an RL agent can provide these elements. Furthermore, from an RL perspective, the learned value function is only an approximation. When used in MPC, this value function is effectively unrolled, executing value iterations, which can lead to an improved policy compared to the original policy that generated the value function.

The solution to the OCP in \autoref{eq:mpc_ocp} at time $k$ can be denoted as:

\begin{equation}\label{eq:sol-mpc-ocp}
	\begin{aligned}
		&\mathbf{x}_{k|k} = [x_{k|k},x_{k_+ 1|k},x_{k + 2|k}, ...,x_{k + N_p|k}]^T \\ 
		&\mathbf{u}_{k|k} = [u_{k|k},u_{k + 1|k}, ...,u_{k + N_p-1|k}]^T \\
	\end{aligned}
\end{equation}

Where the initial guess generated by the RL agent at time $k$ is denoted as:

\begin{equation}\label{eq:initial-guess}
	\begin{aligned}
		&\tilde{\mathbf{x}}_{k|k} = [\tilde{x}_{k|k},\tilde{x}_{k+1|k},...,\tilde{x}_{k + N_p|k}]^T \\ 
		&\tilde{\mathbf{u}}_{k|k} = [\tilde{u}_{k|k},\tilde{u}_{k + 1|k},...,\tilde{u}_{k + N_p - 1|k}]^T\\ 
	\end{aligned}
\end{equation}

such that

\begin{equation}\label{eq:horizon_extension}
	\begin{aligned}
		&\tilde{\mathbf{x}}_{k|k} = [\mathbf{x}_{k|k-1}, \\ & f(x_{k-1 + N_p|k-1}, \pi(x_{k-1 + N_p|k-1}), d_{k+Np|k},p)]^T \\ 
		&\tilde{\mathbf{u}}_{k|k} = [\mathbf{u}_{k|k-1},\pi(x_{k-1 + N_p|k-1})]^T\\
	\end{aligned}
\end{equation}

\autoref{eq:horizon_extension} takes the previous time steps solution, shifts it in time and uses the policy $\pi(\cdot)$, as provided by the actor, to calculate the optimal action and resulting state to take at the last time step. This method can be interpreted as extending the MPC's horizon. It must be noted that, for the first time step, $k=0$, the intial guess is generated by unrolling the RL policy for the entire horizon. Based on prior analyses, generating these initial guesses is extremely fast. Furthermore, since it comes from a policy comparable to the MPCâ€™s which optimises the same goal, these initial guesses can be used for more than just initial guesses, but also to generate terminal constraints. The terminal region is defined as:

\begin{equation}\label{eq:terminal-region}
	\begin{aligned}
		& (1-\delta_T)\tilde{x}_{k+Np|k} \leq x_{k+Np|k} \leq (1+\delta_T)\tilde{x}_{k+Np|k}\\
		&(1-\delta_T)\tilde{u}_{k+Np-1|k} \leq u_{k+Np-1|k} \leq (1+\delta_T) \tilde{u}_{k+Np-1|k}\\
	\end{aligned}
\end{equation}

The terminal cost function takes the form of the value function learned $V_\phi$ and therefore, the resulting RL-MPC OCP is defined as:

\begin{equation} \label{eq:rl-mpc-ocp}
	\begin{aligned}
		\min_{u(k),x(k)} & \sum_{k = k_0}^{k_0 + N_p-1} l(u(k), y(k)) - V_{\phi}(y_1(k_0+N_p), k_0+N_p) \\
		\text{s.t.} \quad & x(k+1) = f(x(k), u(k), d(k), p), \\
		& y(k) = g(x(k+1), p), \\
		& -\delta u \leq u(k) - u(k-1) \leq \delta u, \\
		& u_{\min} \leq u(k) \leq u_{\max}, \\
		& x(k_0) = x_{k_0}, \\
		& \tilde{\mathbf{x}}_{k|k} = [\mathbf{x}_{k|k-1}, f(x_{k-1 + N_p|k-1}, \pi(x_{k-1 + N_p|k-1}), d_{k+Np|k}, p)]^T, \\
		& \tilde{\mathbf{u}}_{k|k} = [\mathbf{u}_{k|k-1}, \pi(x_{k-1 + N_p|k-1})]^T, \\
		& (1-\delta_T)\tilde{x}_{k+Np|k} \leq x_{k+Np|k} \leq (1+\delta_T)\tilde{x}_{k+Np|k}, \\
		& (1-\delta_T)\tilde{u}_{k+Np-1|k} \leq u_{k+Np-1|k} \leq (1+\delta_T)\tilde{u}_{k+Np-1|k}.
	\end{aligned}
\end{equation}


This implementation was investigated to determine whether RL could provide an adequate terminal region and cost function to improve the original EMPC's performance, thereby producing a high-performing EMPC, also known as RL-MPC. The goal was to achieve these improvements for shorter prediction horizons and to transfer knowledge of the system's uncertainty to enhance the EMPC's ability to handle such uncertainties. In conjuction with Casadi and IPOPT, the open-source software L4Casadi \cite{salzmannLearningCasADiDatadriven2023,salzmannRealtimeNeuralMPCDeep2023} was used to develop the RL-MPC framework and solve \autoref{eq:rl-mpc-ocp}.
