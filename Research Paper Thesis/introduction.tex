\section{Introduction}
Smart greenhouses are designed to enhance crop yield per hectare using climate-controlled environments \cite{morcegoReinforcementLearningModel2023}. These smart greenhouses are essential in combating the degrading effects of climate change on crop quality and yield. However, efficiently maintaining such an environment requires advanced control methods. These control methods must be able to adjust factors such as temperature, humidity, lighting, and CO2 levels to accommodate ideal conditions for crop growth \cite{devopsGreenhouseClimateControl2021} while keeping energy costs at a minimum. The advent of smart and advanced greenhouses necessitates skilled labor for operation, yet there is a scarcity of qualified personnel \cite{rusnakWhatCurrentState2018}. Along with escalating labor costs, the move to autonomous greenhouses is attractive.\\
Control strategies such as Reinforcement Learning (RL) and Model Predictive Control (MPC) have been extensively used in autonomous greenhouses \cite{morcegoReinforcementLearningModel2023, boersmaRobustSamplebasedModel2022, lubbersAutonomousGreenhouseClimate2023, jansenOptimalControlLettuce2023}. Both control schemes have advantages and disadvantages. However, there is a notable similarity between the two, suggesting that combining them could lead to a more efficient solution for autonomous greenhouse control.\\
Several methods seek to integrate the two control schemes, and successful implementations have been demonstrated in \cite{lubbersAutonomousGreenhouseClimate2023, arroyoReinforcedModelPredictive2022, linReinforcementLearningBasedModel2023}. However, very little research exists on combining RL and MPC for optimizing economic benefit. \citet{lubbersAutonomousGreenhouseClimate2023} investigates using MPC as the function approximator for the RL agent in the context of greenhouse control. While this is one manner of combining them to optimise for economic benefit, this study aims to investigate the effect of modifying MPC's objective function with terminal constraints and a cost function provided by an independently trained RL agent. This approach allows RL to provide knowledge of the uncertainties present and offers future insights beyond MPC's prediction horizon.

\subsection{MPC and RL}
RL and MPC each have distinct strengths and limitations in controlling complex environments. RL aims to learn optimal policies through interactions with the environment, even in the presence of uncertainty. However, RL lacks online flexibility due to its reliance on feed-forward policy evaluation. Additionally, training RL models often requires a large amount of data, which can be laborious.\\
MPC, on the other hand, relies on an accurate prediction model, which is often simplified for computational efficiency, potentially leading to sub-optimal control. Despite this, MPC is known for its easier implementation, better constraint handling, and sample efficiency. However, model mismatches and unforeseen uncertainties can significantly degrade MPC's performance.\\
A key aspect of both strategies is their prediction horizon. MPC uses explicit optimization over a finite horizon, which can lead to short-sighted decisions, especially with sparse rewards and slow system dynamics. RL employs a discounted infinite horizon, allowing it to consider long-term rewards and bears similarities to MPC's prediction horizon.\\
Combining both approaches can mitigate their respective drawbacks. An MPC controller can optimize a short prediction horizon while incorporating future information from RL, particularly useful in contexts like greenhouse dynamics where actions have long-term impacts. Moreover, RL's knowledge of the system's uncertainty is also valuable and can be transferred to the MPC. This hybrid approach leverages MPC's computational efficiency and RL's ability to handle uncertainty and long-term planning, thereby improving overall control performance.

