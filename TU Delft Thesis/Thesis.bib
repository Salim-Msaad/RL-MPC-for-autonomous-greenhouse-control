@misc{AgriculturalGreenhouseGas,
  title = {Agricultural {{Greenhouse Gas Emissions}} 101},
  journal = {Resources for the Future},
  urldate = {2023-12-01},
  abstract = {This explainer provides an overview of agriculture's contributions to US greenhouse gas emissions, detailing major emissions sources and technology options for emissions mitigation.},
  howpublished = {https://www.rff.org/publications/explainers/agricultural-greenhouse-gas-emissions-101/},
  langid = {american},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/DW8L99XW/agricultural-greenhouse-gas-emissions-101.html}
}

@article{ajagekarDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning Based Automatic Control}} in {{Semi-Closed Greenhouse Systems}}},
  author = {Ajagekar, Akshay and You, Fengqi},
  year = {2022},
  journal = {IFAC-PapersOnLine},
  volume = {55},
  number = {7},
  pages = {406--411},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2022.07.477},
  urldate = {2023-12-01},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2023-12-06T09:20:22.243Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/S4A9C5JS/Ajagekar and You - 2022 - Deep Reinforcement Learning Based Automatic Contro.pdf}
}

@article{ajagekarEnergyefficientAIbasedControl2023,
  title = {Energy-Efficient {{AI-based Control}} of {{Semi-closed Greenhouses Leveraging Robust Optimization}} in {{Deep Reinforcement Learning}}},
  author = {Ajagekar, Akshay and Mattson, Neil S. and You, Fengqi},
  year = {2023},
  month = feb,
  journal = {Advances in Applied Energy},
  volume = {9},
  pages = {100119},
  issn = {2666-7924},
  doi = {10.1016/j.adapen.2022.100119},
  urldate = {2023-12-05},
  abstract = {As greenhouses are being widely adopted worldwide, it is important to improve the energy efficiency of the control systems while accurately regulating their indoor climate to realize sustainable agricultural practices for food production. In this work, we propose an artificial intelligence (AI)-based control framework that combines deep reinforcement learning techniques to generate insights into greenhouse operation combined with robust optimization to produce energy-efficient controls by hedging against associated uncertainties. The proposed control strategy is capable of learning from historical greenhouse climate trajectories while adapting to current climatic conditions and disturbances like time-varying crop growth and outdoor weather. We evaluate the performance of the proposed AI-based control strategy against state-of-the-art model-based and model-free approaches like certainty-equivalent model predictive control, robust model predictive control (RMPC), and deep deterministic policy gradient. Based on the computational results obtained for the tomato crop's greenhouse climate control case study, the proposed control technique demonstrates a significant reduction in energy consumption of 57\% over traditional control techniques. The AI-based control framework also produces robust controls that are not overly conservative, with an improvement in deviation from setpoints of over 26.8\% as compared to the baseline control approach RMPC.},
  keywords = {Artificial intelligence,climate control,deep reinforcement learning,energy efficiency,greenhouse,robust optimization},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:26:41.561Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/HQPDGN7A/Ajagekar et al_2023_Energy-efficient AI-based Control of Semi-closed Greenhouses Leveraging Robust.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/ELNMX8RG/S2666792422000373.html}
}

@misc{alvarezWhatSoaringEnergy2021,
  title = {What Is behind Soaring Energy Prices and What Happens next? -- {{Analysis}}},
  shorttitle = {What Is behind Soaring Energy Prices and What Happens Next?},
  author = {Alvarez, Carlos Fernandez and Molnar, Gergely},
  year = {2021},
  month = oct,
  journal = {IEA},
  urldate = {2023-12-01},
  abstract = {What is behind soaring energy prices and what happens next? - A commentary by Carlos Fern{\'a}ndez Alvarez, Gergely Molnar},
  howpublished = {https://www.iea.org/commentaries/what-is-behind-soaring-energy-prices-and-what-happens-next},
  langid = {british},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/9WCZX7PQ/what-is-behind-soaring-energy-prices-and-what-happens-next.html}
}

@article{amritEconomicOptimizationUsing2011,
  title = {Economic Optimization Using Model Predictive Control with a Terminal Cost},
  author = {Amrit, Rishi and Rawlings, James B. and Angeli, David},
  year = {2011},
  journal = {Annual Reviews in Control},
  volume = {2},
  number = {35},
  pages = {178--186},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2011.10.011},
  urldate = {2023-12-05},
  abstract = {In the standard model predictive control implementation, first a steady-state optimization yields the equilibrium point with minimal economic cost. Then, the deviation from the computed best steady state is chosen as the stage cost for the dynamic regulation problem. The computed best equilibrium point may not be the global minimum of the economic cost, and hence, choosing the economic cost as the stage cost for the dynamic regulation problem, rather than the deviation from the best steady state, offers potential for improving the economic performance of the system. It has been previously shown that the existing framework for MPC stability analysis, which addresses to the standard class of problems with a regulation objective, does not extend to economic MPC. Previous work on economic MPC developed new tools for stability analysis and identified sufficient conditions for asymptotic stability. These tools were developed for the terminal constraint MPC formulation, in which the system is stabilized by forcing the state to the best equilibrium point at the end of the horizon. In this work, we relax this constraint by imposing a region constraint on the terminal state instead of a point constraint, and adding a penalty on the terminal state to the regulator cost. We extend the stability analysis tools, developed for terminal constraint economic MPC, to the proposed formulation and establish that strict dissipativity is sufficient for guaranteeing asymptotic stability of the closed-loop system. We also show that the average closed-loop performance outperforms the best steady-state performance. For implementing the proposed formulation, a rigorous analysis for computing the appropriate terminal penalty and the terminal region is presented. A further extension, in which the terminal constraint is completely removed by modifying the regulator cost function, is also presented along with its stability analysis. Finally, an illustrative example is presented to demonstrate the differences between the terminal constraint and the proposed terminal penalty formulation.},
  langid = {english},
  keywords = {Closed-loop stability,Dissipative systems,Model predictive control,Process economics,Terminal penalty},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ZMAEQR45/Amrit et al. - 2011 - Economic optimization using model predictive contr.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/G2DCGS7X/S136757881100040X.html;/home/murray/snap/zotero-snap/common/Zotero/storage/JPQZIMFS/bwmeta1.element.html}
}

@article{arroyoReinforcedModelPredictive2022,
  title = {Reinforced Model Predictive Control ({{RL-MPC}}) for Building Energy Management},
  author = {Arroyo, Javier and Manna, Carlo and Spiessens, Fred and Helsen, Lieve},
  year = {2022},
  month = mar,
  journal = {Applied Energy},
  volume = {309},
  pages = {118346},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2021.118346},
  urldate = {2023-12-01},
  abstract = {Buildings need advanced control for the efficient and climate-neutral use of their energy systems. Model predictive control (MPC) and reinforcement learning (RL) arise as two powerful control techniques that have been extensively investigated in the literature for their application to building energy management. These methods show complementary qualities in terms of constraint satisfaction, computational demand, adaptability, and intelligibility, but usually a choice is made between both approaches. This paper compares both control approaches and proposes a novel algorithm called reinforced predictive control (RL-MPC) that merges their relative merits. First, the complementarity between RL and MPC is emphasized on a conceptual level by commenting on the main aspects of each method. Second, the RL-MPC algorithm is described that effectively combines features from each approach, namely state estimation, dynamic optimization, and learning. Finally, MPC, RL, and RL-MPC are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction when using a control formulation equivalent to MPC and the same controller model for learning. The new RL-MPC algorithm can meet constraints and provide similar performance to MPC while enabling continuous learning and the possibility to deal with uncertain environments.},
  keywords = {BOPTEST,Building automation,Model predictive control,Reinforced model predictive control,Reinforcement learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:28.314Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/KIPS5MIY/Arroyo et al. - 2022 - Reinforced model predictive control (RL-MPC) for b.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/8CKYYN5K/S0306261921015932.html}
}

@article{arroyoReinforcedModelPredictive2022a,
  title = {Reinforced Model Predictive Control ({{RL-MPC}}) for Building Energy Management},
  author = {Arroyo, Javier and Manna, Carlo and Spiessens, Fred and Helsen, Lieve},
  year = {2022},
  month = mar,
  journal = {Applied Energy},
  volume = {309},
  pages = {118346},
  issn = {03062619},
  doi = {10.1016/j.apenergy.2021.118346},
  urldate = {2024-05-29},
  abstract = {Buildings need advanced control for the efficient and climate-neutral use of their energy systems. Model predictive control (MPC) and reinforcement learning (RL) arise as two powerful control techniques that have been extensively investigated in the literature for their application to building energy management. These methods show complementary qualities in terms of constraint satisfaction, computational demand, adaptability, and intelligibility, but usually a choice is made between both approaches. This paper compares both control approaches and proposes a novel algorithm called reinforced predictive control (RL-MPC) that merges their relative merits. First, the complementarity between RL and MPC is emphasized on a conceptual level by commenting on the main aspects of each method. Second, the RL-MPC algorithm is described that effectively combines features from each approach, namely state estimation, dynamic optimization, and learning. Finally, MPC, RL, and RL-MPC are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction when using a control formulation equivalent to MPC and the same controller model for learning. The new RL-MPC algorithm can meet constraints and provide similar performance to MPC while enabling continuous learning and the possibility to deal with uncertain environments.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/AYTI2ZY5/Arroyo et al. - 2022 - Reinforced model predictive control (RL-MPC) for b.pdf}
}

@misc{baeldungEpsilonGreedyQlearningBaeldung2020,
  title = {Epsilon-{{Greedy Q-learning}} {\textbar} {{Baeldung}} on {{Computer Science}}},
  author = {{baeldung}},
  year = {2020},
  month = dec,
  urldate = {2023-12-11},
  abstract = {Learn about q-learning, a reinforcement learning technique.},
  howpublished = {https://www.baeldung.com/cs/epsilon-greedy-q-learning},
  langid = {american},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/227CWIDH/epsilon-greedy-q-learning.html}
}

@article{beckenbachAddressingInfinitehorizonOptimization2018,
  title = {Addressing Infinite-Horizon Optimization in {{MPC}} via {{Q-learning}}},
  author = {Beckenbach, Lukas and Osinenko, Pavel and Streif, Stefan},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {6th {{IFAC Conference}} on {{Nonlinear Model Predictive Control NMPC}} 2018},
  volume = {51},
  number = {20},
  pages = {60--65},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.10.175},
  urldate = {2023-12-13},
  abstract = {Model predictive control (MPC) is the standard approach to infinite-horizon optimal control which usually optimizes a finite initial fragment of the cost function so as to make the problem computationally tractable. Globally optimal controllers are usually found by Dynamic Programming (DP). The computations involved in DP are notoriously hard to perform, especially in online control. Therefore, different approximation schemes of DP, the so-called ``critics'', were suggested for infinite-horizon cost functions. This work proposes to incorporate such a critic into dual-mode MPC as a particular means of addressing infinite-horizon optimal control. The proposed critic is based on Q-learning and is used for online approximation of the infinite-horizon cost. Stability of the new approach is analyzed and certain sufficient stabilizing constraints on the critic are derived. A case study demonstrates the applicability.},
  keywords = {infinite-horizon optimization,Nonlinear MPC,reinforcement learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ZXEUZ9SM/Beckenbach et al. - 2018 - Addressing infinite-horizon optimization in MPC vi.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/HT5VRX53/S2405896318326478.html}
}

@article{beckenbachAddressingInfinitehorizonOptimization2018a,
  title = {Addressing Infinite-Horizon Optimization in {{MPC}} via {{Q-learning}}},
  author = {Beckenbach, Lukas and Osinenko, Pavel and Streif, Stefan},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {6th {{IFAC Conference}} on {{Nonlinear Model Predictive Control NMPC}} 2018},
  volume = {51},
  number = {20},
  pages = {60--65},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.10.175},
  urldate = {2024-05-28},
  abstract = {Model predictive control (MPC) is the standard approach to infinite-horizon optimal control which usually optimizes a finite initial fragment of the cost function so as to make the problem computationally tractable. Globally optimal controllers are usually found by Dynamic Programming (DP). The computations involved in DP are notoriously hard to perform, especially in online control. Therefore, different approximation schemes of DP, the so-called ``critics'', were suggested for infinite-horizon cost functions. This work proposes to incorporate such a critic into dual-mode MPC as a particular means of addressing infinite-horizon optimal control. The proposed critic is based on Q-learning and is used for online approximation of the infinite-horizon cost. Stability of the new approach is analyzed and certain sufficient stabilizing constraints on the critic are derived. A case study demonstrates the applicability.},
  keywords = {infinite-horizon optimization,Nonlinear MPC,reinforcement learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/G5GJ9R9M/S2405896318326478.html}
}

@article{bellmanDynamicProgramming1966,
  title = {Dynamic Programming},
  author = {Bellman, R.},
  year = {1966},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {153},
  number = {3731},
  pages = {34--37},
  issn = {0036-8075},
  doi = {10.1126/science.153.3731.34},
  abstract = {Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a "theory." What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology. The more we study the information processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them. In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?},
  langid = {english},
  pmid = {17730601},
  keywords = {/unread}
}

@article{bersaniModelPredictiveControl2020,
  title = {Model {{Predictive Control}} of {{Smart Greenhouses}} as the {{Path}} towards {{Near Zero Energy Consumption}}},
  author = {Bersani, Chiara and Ouammi, Ahmed and Sacile, Roberto and Zero, Enrico},
  year = {2020},
  month = jan,
  journal = {Energies},
  volume = {13},
  number = {14},
  pages = {3647},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1996-1073},
  doi = {10.3390/en13143647},
  urldate = {2023-12-01},
  abstract = {Modern agriculture represents an economic sector that can mainly benefit from technology innovation according to the principles suggested by Industry 4.0 for smart farming systems. Greenhouse industry is significantly becoming more and more technological and automatized to improve the quality and efficiency of crop production. Smart greenhouses are equipped with forefront IoT- and ICT-based monitoring and control systems. New remote sensors, devices, networking communication, and control strategies can make available real-time information about crop health, soil, temperature, humidity, and other indoor parameters. Energy efficiency plays a key role in this context, as a fundamental path towards sustainability of the production. This paper is a review of the precision and sustainable agriculture approaches focusing on the current advance technological solution to monitor, track, and control greenhouse systems to enhance production in a more sustainable way. Thus, we compared and analyzed traditional versus model predictive control methods with the aim to enhance indoor microclimate condition management under an energy-saving approach. We also reviewed applications of sustainable approaches to reach nearly zero energy consumption, while achieving nearly zero water and pesticide use.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {control strategies,energy saving,greenhouse,model predictive control,precision agriculture,sustainability},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:28:50.921Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/GIKGFRGF/Bersani et al. - 2020 - Model Predictive Control of Smart Greenhouses as t.pdf}
}

@article{bertsekasLessonsAlphaZeroOptimal,
  title = {Lessons from {{AlphaZero}} for {{Optimal}}, {{Model Predictive}}, and {{Adaptive Control}}},
  author = {Bertsekas, Dimitri P},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ZER2BVBI/Bertsekas - Lessons from AlphaZero for Optimal, Model Predicti.pdf}
}

@article{bertsekasNewtonMethodReinforcement2022,
  title = {Newton's Method for Reinforcement Learning and Model Predictive Control},
  author = {Bertsekas, Dimitri},
  year = {2022},
  month = jun,
  journal = {Results in Control and Optimization},
  volume = {7},
  pages = {100121},
  issn = {2666-7207},
  doi = {10.1016/j.rico.2022.100121},
  urldate = {2023-12-01},
  abstract = {The purpose of this paper is to propose and develop a new conceptual framework for approximate Dynamic Programming (DP) and Reinforcement Learning (RL). This framework centers around two algorithms, which are designed largely independently of each other and operate in synergy through the powerful mechanism of Newton's method. We call these the off-line training and the on-line play algorithms; the names are borrowed from some of the major successes of RL involving games. Primary examples are the recent (2017) AlphaZero program (which plays chess), and the similarly structured and earlier (1990s) TD-Gammon program (which plays backgammon). In these game contexts, the off-line training algorithm is the method used to teach the program how to evaluate positions and to generate good moves at any given position, while the on-line play algorithm is the method used to play in real time against human or computer opponents. Both AlphaZero and TD-Gammon were trained off-line extensively using neural networks and an approximate version of the fundamental DP algorithm of policy iteration. Yet the AlphaZero player that was obtained off-line is not used directly during on-line play (it is too inaccurate due to approximation errors that are inherent in off-line neural network training). Instead a separate on-line player is used to select moves, based on multistep lookahead minimization and a terminal position evaluator that was trained using experience with the off-line player. The on-line player performs a form of policy improvement, which is not degraded by neural network approximations. As a result, it greatly improves the performance of the off-line player. Similarly, TD-Gammon performs on-line a policy improvement step using one-step or two-step lookahead minimization, which is not degraded by neural network approximations. To this end it uses an off-line neural network-trained terminal position evaluator, and importantly it also extends its on-line lookahead by rollout (simulation with the one-step lookahead player that is based on the position evaluator). An important lesson from AlphaZero and TD-Gammon is that the performance of an off-line trained policy can be greatly improved by on-line approximation in value space, with long lookahead (involving minimization or rollout with the off-line policy, or both), and terminal cost approximation that is obtained off-line. This performance enhancement is often dramatic and is due to a simple fact, which is couched on algorithmic mathematics and is the focal point of this work: (a) Approximation in value space with one-step lookahead minimization amounts to a step of Newton's method for solving Bellman's equation. (b) The starting point for the Newton step is based on the results of off-line training, and may be enhanced by longer lookahead minimization and on-line rollout. Indeed the major determinant of the quality of the on-line policy is the Newton step that is performed on-line, while off-line training plays a secondary role by comparison. Significantly, the synergy between off-line training and on-line play also underlies Model Predictive Control (MPC), a major control system design methodology that has been extensively developed since the 1980s. This synergy can be understood in terms of abstract models of infinite horizon DP and simple geometrical constructions, and helps to explain the all-important stability issues within the MPC context. In this work we aim to provide insights (often based on visualization), which explain the beneficial effects of on-line decision making on top of off-line training. In the process, we will bring out the strong connections between the artificial intelligence view of RL, and the control theory views of MPC and adaptive control. While we will deemphasize mathematical proofs, there is considerable related analysis, which supports our conclusions and can be found in the author's recent RL books (Bertsekas, 2019; Bertsekas, 2020), and the abstract DP monograph (Bertsekas, 2022). One of our principal aims is to show, through the algorithmic ideas of Newton's method and the unifying principles of abstract DP, that the AlphaZero/TD-Gammon methodology of approximation in value space and rollout applies very broadly to deterministic and stochastic optimal control problems, involving both discrete and continuous search spaces, as well as finite and infinite horizon.},
  keywords = {AlphaZero,Dynamic programming over an infinite horizon,Model predictive control,Off-line training,On-line play,Reinforcement learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/TVN52CBX/Bertsekas - 2022 - Newton’s method for reinforcement learning and mod.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/SXJ525DZ/S2666720722000157.html}
}

@article{bieglerRetrospectiveOptimization2004,
  title = {Retrospective on Optimization},
  author = {Biegler, Lorenz T. and Grossmann, Ignacio E.},
  year = {2004},
  month = jul,
  journal = {Computers \& Chemical Engineering},
  volume = {28},
  number = {8},
  pages = {1169--1192},
  issn = {00981354},
  doi = {10.1016/j.compchemeng.2003.11.003},
  urldate = {2023-12-19},
  abstract = {In this paper we provide a general classification of mathematical optimization problems, followed by a matrix of applications that shows the areas in which these problems have been typically applied in process systems engineering. We then provide a review of solution methods of the major types of optimization problems for continuous and discrete variable optimization, particularly nonlinear and mixed-integer nonlinear programming. We also review their extensions to dynamic optimization and optimization under uncertainty. While these areas are still subject to significant research efforts, the emphasis in this paper is on major developments that have taken place over the last twenty five years.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/IW4PAJEM/Biegler and Grossmann - 2004 - Retrospective on optimization.pdf}
}

@misc{blazhevskaGrowingSlowerPace2019,
  title = {Growing at a Slower Pace, World Population Is Expected to Reach 9.7 Billion in 2050 and Could Peak at Nearly 11 Billion around 2100: {{UN Report}}},
  shorttitle = {Growing at a Slower Pace, World Population Is Expected to Reach 9.7 Billion in 2050 and Could Peak at Nearly 11 Billion around 2100},
  author = {Blazhevska, Vesna},
  year = {2019},
  month = jun,
  journal = {United Nations Sustainable Development},
  urldate = {2023-12-01},
  abstract = {United Nations Sustainable Development Goals - Time for Global Action for People and Planet},
  langid = {american},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/BPE4DSEB/growing-at-a-slower-pace-world-population-is-expected-to-reach-9-7-billion-in-2050-and-could-pe.html}
}

@article{boersmaRobustSamplebasedModel2022,
  title = {Robust Sample-Based Model Predictive Control of a Greenhouse System with Parametric Uncertainty},
  author = {Boersma, Sjoerd and Sun, Congcong and {van Mourik}, Simon},
  year = {2022},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {7th {{IFAC Conference}} on {{Sensing}}, {{Control}} and {{Automation Technologies}} for {{Agriculture AGRICONTROL}} 2022},
  volume = {55},
  number = {32},
  pages = {177--182},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2022.11.135},
  urldate = {2023-12-05},
  abstract = {Achieving optimal resource use efficiency is a key challenge in modern greenhouse production systems. Optimal performance in terms of crop yield and resource efficiency can in theory be achieved via optimal control. Standard optimal controllers are not designed to deal with uncertainty, whereas considerable model prediction errors occur due to the mismatch between the model and the real system. This paper explores the relation between parametric uncertainty, and performance with respect to crop yield, CO2 demand, ventilation demand, and heating energy. This is done using the following steps 1) extension of an existing controller model with parametric uncertainty, 2) design of a sample-based robust model predictive controller and 3) analysis of control performance under increasing parametric uncertainty. The results predict that control performance is significantly sensitive to parametric uncertainty. A relative parameter uncertainty of 20\%, reduced crop yield with 11\% compared to the case without uncertainty. Furthermore, a 20\% uncertainty decreased CO2 demand with 80\%, whereas it increased ventilation demand with 96\%, and increased heating energy demand with 90\%.},
  keywords = {lettuce greenhouse,parametric uncertainties,robust MPC,sample-based MPC},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:41.330Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/WAFF4E2D/Boersma et al. - 2022 - Robust sample-based model predictive control of a .pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/MQIUME67/S2405896322027689.html}
}

@book{boersmaRobustSamplebasedModel2022a,
  title = {Robust Sample-Based Model Predictive Control of a Greenhouse System with Parametric Uncertainty},
  author = {Boersma, Sjoerd and Congcong, Sun and Mourik, Simon},
  year = {2022},
  month = may,
  abstract = {Achieving optimal resource use efficiency is a key challenge in modern greenhouse production systems. Optimal performance in terms of crop yield and resource efficiency can in theory be achieved via optimal control. Standard optimal controllers are not designed to deal with uncertainty, whereas considerable model prediction errors occur due to the mismatch between the model and the real system. This paper explores the relation between prediction uncertainty, and performance with respect to crop yield, CO 2 demand, ventilation demand, and heating energy. This is done using the following steps 1) formulation of parametric uncertainty underlying prediction uncertainty, 2) extension of an existing controller model with parametric uncertainty, 3) design of a sample-based robust model predictive controller and 4) analysis of control performance under increasing parametric uncertainty. The results predict that control performance is highly sensitive to parametric uncertainty. A relative parameter uncertainty of 20\%, reduced crop yield with 11\% compared to the case without uncertainty. Furthermore, a 20\% uncertainty decreased CO 2 demand with 80\%, whereas it increased ventilation demand with 96\%, and increased heating energy demand with 90\%.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/NGCZPEAC/Boersma et al. - 2022 - Robust sample-based model predictive control of a .pdf}
}

@misc{bonsaiWhyReinforcementLearning2017,
  title = {Why {{Reinforcement Learning Might Be}} the {{Best AI Technique}} for {{Complex Industrial Systems}}},
  author = {Bonsai},
  year = {2017},
  month = nov,
  journal = {Medium},
  urldate = {2023-12-06},
  abstract = {One of the first things to know about machine learning is that you will be working with one of three types of algorithms: supervised learning, unsupervised learning and reinforcement learning. Here's{\dots}},
  langid = {english},
  keywords = {/unread},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-06T09:43:40.590Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/Z4LTWQFA/why-reinforcement-learning-might-be-the-best-ai-technique-for-complex-industrial-systems-fde8b0.html}
}

@article{breukersPowerDutchGreenhouse,
  title = {The Power of {{Dutch}} Greenhouse Vegetable Horticulture : An Analysis of the Private Sector and Its Institutional Framework},
  author = {Breukers, A and Hietbrink, O and Ruijs, M N A},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/KYI3PMEQ/Breukers et al. - The power of Dutch greenhouse vegetable horticultu.pdf}
}

@misc{BriefSurveyModelBased,
  title = {Brief {{Survey}} of {{Model-Based Reinforcement Learning Techniques}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2023-12-09},
  howpublished = {https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/9259716},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/S4MEGXL9/9259716.html}
}

@inproceedings{BriefSurveyModelBased2020,
  title = {Brief {{Survey}} of {{Model-Based Reinforcement Learning Techniques}}},
  booktitle = {International {{Conference}} on {{System Theory}}, {{Control}} and {{Computing}}},
  year = {2020},
  month = oct,
  pages = {92--97},
  publisher = {IEEE},
  doi = {10.1109/ICSTCC50638.2020.9259716},
  urldate = {2023-12-09},
  abstract = {Model-free reinforcement learning (MFRL) usually has better asymptotic performance than the model-based reinforcement (MBRL) learning algorithms, especially in complex environments. But MBRL algorithms are very often much more sample-efficient, and sometimes are able to learn control tasks in just a handful of trials. In addition, in some domains, the MBRL algorithms can reach the MFRL performance with better sample efficiency. In recent years, MBRL research has increased in various application domains, such as robot control tasks, or game environments with complex observations. In this paper, we review the most popular techniques used in MBRL and look at some useful classification of algorithms in this area.},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/PZCDZVGH/brief-survey-of-model-based-reinforcement-learning-3tptshxfv8.html}
}

@article{congrevesUrbanHorticultureSustainable2022,
  title = {Urban Horticulture for Sustainable Food Systems},
  author = {Congreves, Kate A.},
  year = {2022},
  journal = {Frontiers in Sustainable Food Systems},
  volume = {6},
  issn = {2571-581X},
  urldate = {2023-12-01},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/Y3U7DDSD/Congreves - 2022 - Urban horticulture for sustainable food systems.pdf}
}

@misc{daaboulUncertaintyPredictionModelbased2020,
  title = {Uncertainty and {{Prediction}} in {{Model-based Reinforcement Learning}}},
  author = {Daaboul, Karam},
  year = {2020},
  month = oct,
  journal = {Medium},
  urldate = {2023-12-06},
  abstract = {Reinforcement learning (RL) is a framework work to deal with delayed reward signals. In Deep Reinforcement Learning (DRL), a neural{\dots}},
  langid = {english},
  keywords = {/unread},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-06T10:15:00.586Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/IV4T83KL/uncertainty-and-prediction-in-model-based-reinforcement-learning-20546643028c.html}
}

@incollection{daiDiscreteTimeModelPredictive2012,
  title = {Discrete-{{Time Model Predictive Control}}},
  booktitle = {Advances in {{Discrete Time Systems}}},
  author = {Dai, Li and Xia, Yuanqing and Fu, Mengyin and Mahmoud, Magdi S. and Dai, Li and Xia, Yuanqing and Fu, Mengyin and Mahmoud, Magdi S.},
  year = {2012},
  month = dec,
  publisher = {IntechOpen},
  doi = {10.5772/51122},
  urldate = {2023-12-05},
  abstract = {Open access peer-reviewed chapter},
  isbn = {978-953-51-0875-7},
  langid = {english},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:22.698Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LFZ3LD6P/Dai et al. - 2012 - Discrete-Time Model Predictive Control.pdf}
}

@misc{daveUnderstandingBellmanOptimality2021,
  title = {Understanding the {{Bellman Optimality Equation}} in {{Reinforcement Learning}}},
  author = {Dave, Hardik},
  year = {2021},
  month = feb,
  journal = {Analytics Vidhya},
  urldate = {2023-12-11},
  abstract = {The aim of this article is to give an intuition about Reinforcement Learning as well as what the Bellman Optimality Equation is.},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/MPGUEAAH/understanding-the-bellman-optimality-equation-in-reinforcement-learning.html}
}

@article{decardi-nelsonbenjaminImprovingResourceUse2023,
  title = {Improving {{Resource Use Efficiency}} in {{Plant Factories Using Deep Reinforcement Learning}} for {{Sustainable Food Production}}},
  author = {{Decardi-Nelson Benjamin} and {You Fengqi}},
  year = {2023},
  month = oct,
  journal = {Chemical Engineering Transactions},
  volume = {103},
  pages = {79--84},
  doi = {10.3303/CET23103014},
  urldate = {2023-12-05},
  langid = {english},
  keywords = {/unread},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-05T19:33:36.029Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/KSYWGRV2/Decardi-Nelson Benjamin and You Fengqi - 2023 - Improving Resource Use Efficiency in Plant Factori.pdf}
}

@misc{DeepDeterministicPolicy,
  title = {Deep {{Deterministic Policy Gradient}} --- {{Spinning Up}} Documentation},
  urldate = {2023-12-11},
  howpublished = {https://spinningup.openai.com/en/latest/algorithms/ddpg.html},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/XUAYU9ES/ddpg.html}
}

@article{delatorre-geaComputationalFluidDynamics2011,
  title = {Computational Fluid Dynamics in Greenhouses: {{A}} Review},
  shorttitle = {Computational Fluid Dynamics in Greenhouses},
  author = {{De la Torre-Gea}, Guillermo and {Soto-Zaraz{\'u}a}, Dr and {Lopez-Cruz}, Irineo and Pacheco, Irineo and {Rico-Garc{\'i}a}, Enrique},
  year = {2011},
  month = dec,
  journal = {AFRICAN JOURNAL OF BIOTECHNOLOGY},
  volume = {10},
  pages = {17651--17662},
  doi = {10.5897/AJB10.2488},
  abstract = {Computational fluid dynamics is a tool that has been used in recent years to develop numerical models that improve our understanding of the interaction of variables that make up the climate inside greenhouses. In the past five years, more realistic studies have appeared due mainly to the development of more powerful software and hardware. However, it is necessary to perform an analysis to show us the trends, strengths and weaknesses in the use of this tool. In this study, we reviewed the state of the art of CFD in studies of airflow and climate inside greenhouses, analyzing the most important issues that help us understand how it has evolved, as well as trends and limitations on their use.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LYBZFG5E/De la Torre-Gea et al_2011_Computational fluid dynamics in greenhouses.pdf}
}

@misc{devopsGreenhouseClimateControl2021,
  title = {Greenhouse {{Climate Control}} -- {{How}} to {{Improve Plant Growth}} - {{DryGair}}},
  author = {DevOps},
  year = {2021},
  month = aug,
  journal = {Drygair Greenhouse Dehumidifiers},
  urldate = {2023-12-01},
  abstract = {Greenhouse climate control is one of the most important aspects of modern horticulture, letting growers grow high-quality produce year-round.},
  langid = {american},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/FRJBAXMD/greenhouse-climate-control.html}
}

@misc{dulac-arnoldChallengesRealWorldReinforcement2019,
  title = {Challenges of {{Real-World Reinforcement Learning}}},
  author = {{Dulac-Arnold}, Gabriel and Mankowitz, Daniel and Hester, Todd},
  year = {2019},
  month = apr,
  number = {arXiv:1904.12901},
  eprint = {1904.12901},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-01-16},
  abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/X28RGE8S/Dulac-Arnold et al_2019_Challenges of Real-World Reinforcement Learning.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/XMPVG2VS/1904.html}
}

@article{ellisTutorialReviewEconomic2014,
  title = {A Tutorial Review of Economic Model Predictive Control Methods},
  author = {Ellis, Matthew and Durand, Helen and Christofides, Panagiotis D.},
  year = {2014},
  month = aug,
  journal = {Journal of Process Control},
  series = {Economic Nonlinear Model Predictive Control},
  volume = {24},
  number = {8},
  pages = {1156--1178},
  issn = {0959-1524},
  doi = {10.1016/j.jprocont.2014.03.010},
  urldate = {2023-12-05},
  abstract = {An overview of the recent results on economic model predictive control (EMPC) is presented and discussed addressing both closed-loop stability and performance for nonlinear systems. A chemical process example is used to provide a demonstration of a few of the various approaches. The paper concludes with a brief discussion of the current status of EMPC and future research directions to promote and stimulate further research potential in this area.},
  keywords = {Economic model predictive control,Nonlinear systems,Process control,Process economics,Process optimization},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:03.464Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/CPRF96GE/Ellis et al. - 2014 - A tutorial review of economic model predictive con.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/4DP45VVV/S0959152414000900.html}
}

@misc{ExplorationVsExploitation,
  title = {Exploration vs. {{Exploitation}} - {{Learning}} the {{Optimal Reinforcement Learning Policy}}},
  urldate = {2023-12-11},
  abstract = {Welcome back to this series on reinforcement learning! Last time, we left our discussion of Q-learning with the question of how an agent chooses to either explore the environment or to exploit it in},
  howpublished = {https://deeplizard.com/learn/video/mo96Nqlo1L8},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/45N99FIF/mo96Nqlo1L8.html}
}

@book{faoFutureFoodAgriculture2017,
  title = {The Future of Food and Agriculture: Trends and Challenges},
  shorttitle = {The Future of Food and Agriculture},
  author = {FAO},
  year = {2017},
  publisher = {{Food and Agriculture Organization of the United Nations}},
  address = {Rome},
  isbn = {978-92-5-109551-5},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/YRUR5WSY/FAO - 2017 - The future of food and agriculture trends and cha.pdf}
}

@inproceedings{ferrandez-pastorReinforcementLearningModel2023,
  title = {Reinforcement {{Learning Model}} in~{{Automated Greenhouse Control}}},
  booktitle = {Proceedings of the 15th {{International Conference}} on {{Ubiquitous Computing}} \& {{Ambient Intelligence}} ({{UCAmI}} 2023)},
  author = {{Ferr{\'a}ndez-Pastor}, F. Javier and {C{\'a}mara-Zapata}, Jos{\'e} M. and {Alca{\~n}iz-Lucas}, Sara and Pardo, Sof{\'i}a and Brenes, Jose A.},
  editor = {Bravo, Jos{\'e} and Urz{\'a}iz, Gabriel},
  year = {2023},
  series = {Lecture {{Notes}} in {{Networks}} and {{Systems}}},
  pages = {3--13},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-48642-5_1},
  abstract = {Automated systems, controlled with programmed reactive rules and set-point values for feedback regulation, require supervision and adjustment by experienced technicians. These technicians must be familiar with the scenario where the controlled processes are carried out. In automated greenhouses, achieving optimal environmental values requires the expertise of a specialist technician. This introduces the need for an expert in the installation and the problem of depending on them.},
  isbn = {978-3-031-48642-5},
  langid = {english},
  keywords = {Q-Learning,Reinforcement Learning,Smart Greenhouse},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:05.848Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/SVSYSKWL/Ferrández-Pastor et al. - 2023 - Reinforcement Learning Model in Automated Greenhou.pdf}
}

@inproceedings{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  year = {2018},
  month = jul,
  pages = {1587--1596},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-12-13},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/4YTPGXM5/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/DZRU8TN6/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf}
}

@misc{fujimotoAddressingFunctionApproximation2018a,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = oct,
  number = {arXiv:1802.09477},
  eprint = {1802.09477},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1802.09477},
  urldate = {2023-12-11},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/KW6ZSC5J/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/NGFL56DQ/1802.html}
}

@article{ghoumariNonlinearConstrainedMPC2005,
  title = {Non-Linear Constrained {{MPC}}: {{Real-time}} Implementation of Greenhouse Air Temperature Control},
  shorttitle = {Non-Linear Constrained {{MPC}}},
  author = {Ghoumari, M. and Tantau, Hans-J{\"u}rgen and Serrano, Javier},
  year = {2005},
  month = dec,
  journal = {Computers and Electronics in Agriculture - COMPUT ELECTRON AGRIC},
  volume = {49},
  pages = {345--356},
  doi = {10.1016/j.compag.2005.08.005},
  abstract = {This work describes the application of model predictive control (MPC) for temperature regulation in agricultural processes. The main objective is to achieve temperature control of a greenhouse built in the Institute for Horticultural and Agricultural Engineering (ITG) at the University of Hannover (Germany). The MPC algorithm used here takes in account the constraints in both manipulated and controlled variables using an on-line linearisation with a very low computational burden. Several important advantages of the MPC algorithm, primarily performance and energy savings, are shown by means of a real-time experiment using a soft optimal control effort. This MPC scheme is compared with an adaptive PID controller.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/4NVX4BVU/El Ghoumari et al. - 2005 - Non-linear constrained MPC Real-time implementati.pdf}
}

@misc{GoingDeeperReinforcement,
  title = {Going {{Deeper Into Reinforcement Learning}}: {{Fundamentals}} of {{Policy Gradients}}},
  urldate = {2023-12-11},
  howpublished = {https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ZI38ARTD/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients.html}
}

@article{gongDeepLearningBased2021,
  title = {Deep {{Learning Based Prediction}} on {{Greenhouse Crop Yield Combined TCN}} and {{RNN}}},
  author = {Gong, Liyun and Yu, Miao and Jiang, Shouyong and Cutsuridis, Vassilis and Pearson, Simon},
  year = {2021},
  month = jan,
  journal = {Sensors},
  volume = {21},
  number = {13},
  pages = {4537},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s21134537},
  urldate = {2023-12-05},
  abstract = {Currently, greenhouses are widely applied for plant growth, and environmental parameters can also be controlled in the modern greenhouse to guarantee the maximum crop yield. In order to optimally control greenhouses' environmental parameters, one indispensable requirement is to accurately predict crop yields based on given environmental parameter settings. In addition, crop yield forecasting in greenhouses plays an important role in greenhouse farming planning and management, which allows cultivators and farmers to utilize the yield prediction results to make knowledgeable management and financial decisions. It is thus important to accurately predict the crop yield in a greenhouse considering the benefits that can be brought by accurate greenhouse crop yield prediction. In this work, we have developed a new greenhouse crop yield prediction technique, by combining two state-of-the-arts networks for temporal sequence processing---temporal convolutional network (TCN) and recurrent neural network (RNN). Comprehensive evaluations of the proposed algorithm have been made on multiple datasets obtained from multiple real greenhouse sites for tomato growing. Based on a statistical analysis of the root mean square errors (RMSEs) between the predicted and actual crop yields, it is shown that the proposed approach achieves more accurate yield prediction performance than both traditional machine learning methods and other classical deep neural networks. Moreover, the experimental study also shows that the historical yield information is the most important factor for accurately predicting future crop yields.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {crop yield prediction,deep learning,greenhouse,recurrent neural network (RNN),temporal convolutional network (TCN)},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-05T19:32:30.868Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/EFVBNQCQ/Gong et al_2021_Deep Learning Based Prediction on Greenhouse Crop Yield Combined TCN and RNN.pdf}
}

@article{graamansPlantFactoriesGreenhouses2018,
  title = {Plant Factories versus Greenhouses: {{Comparison}} of Resource Use Efficiency},
  shorttitle = {Plant Factories versus Greenhouses},
  author = {Graamans, Luuk and Baeza, Esteban and Dobbelsteen, Andy Van Den and Tsafaras, Ilias and Stanghellini, Cecilia},
  year = {2018},
  month = feb,
  journal = {Agricultural Systems},
  volume = {160},
  pages = {31--43},
  publisher = {Elsevier},
  issn = {0308-521X},
  doi = {10.1016/j.agsy.2017.11.003},
  urldate = {2023-12-01},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/83FFINEK/plant-factories-versus-greenhouses-comparison-of-resource-use-eff.html}
}

@article{grosLinearNonlinearMPC2020,
  title = {From Linear to Nonlinear {{MPC}}: Bridging the Gap via the Real-Time Iteration},
  shorttitle = {From Linear to Nonlinear {{MPC}}},
  author = {Gros, S{\'e}bastien and Zanon, Mario and Quirynen, Rien and Bemporad, Alberto and Diehl, Moritz},
  year = {2020},
  month = jan,
  journal = {International Journal of Control},
  volume = {93},
  number = {1},
  pages = {62--80},
  publisher = {Taylor \& Francis},
  issn = {0020-7179},
  doi = {10.1080/00207179.2016.1222553},
  urldate = {2023-12-05},
  abstract = {Linear model predictive control (MPC) can be currently deployed at outstanding speeds, thanks to recent progress in algorithms for solving online the underlying structured quadratic programs. In contrast, nonlinear MPC (NMPC) requires the deployment of more elaborate algorithms, which require longer computation times than linear MPC. Nonetheless, computational speeds for NMPC comparable to those of MPC are now regularly reported, provided that the adequate algorithms are used. In this paper, we aim at clarifying the similarities and differences between linear MPC and NMPC. In particular, we focus our analysis on NMPC based on the real-time iteration (RTI) scheme, as this technique has been successfully tested and, in some applications, requires computational times that are only marginally larger than linear MPC. The goal of the paper is to promote the understanding of RTI-based NMPC within the linear MPC community.},
  keywords = {Linear MPC,real-time NMPC},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:33.867Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/NSCJV23Q/Gros et al. - 2020 - From linear to nonlinear MPC bridging the gap via.pdf}
}

@article{gruberNonlinearMPCBased2011,
  title = {Nonlinear {{MPC}} Based on a {{Volterra}} Series Model for Greenhouse Temperature Control Using Natural Ventilation},
  author = {Gruber, J. K. and Guzm{\'a}n, J. L. and Rodr{\'i}guez, F. and Bordons, C. and Berenguel, M. and S{\'a}nchez, J. A.},
  year = {2011},
  journal = {Control Engineering Practice},
  volume = {4},
  number = {19},
  pages = {354--366},
  issn = {0967-0661},
  doi = {10.1016/j.conengprac.2010.12.004},
  urldate = {2023-12-13},
  abstract = {Suitable environmental conditions are a fundamental issue in greenhouse crop growth and can be achieved by advanced climate control strategies. In different climatic zones, natural ventilation is used to regulate both the greenhouse temperature and humidity. In mild climates, the greatest problem faced by far in greenhouse climate control is cooling, which, for dynamical reasons, leads to natural ventilation as a standard tool. This work addresses the design of a nonlinear model predictive control (NMPC) strategy for greenhouse temperature control using natural ventilation. The NMPC strategy is based on a second-order Volterra series model identified from experimental input/output data of a greenhouse. These models, representing the simple and logical extension of convolution models, can be used to approximate the nonlinear dynamic effect of the ventilation and other environmental conditions on the greenhouse temperature. The developed NMPC is applied to a greenhouse and the control performance of the proposed strategy will be illustrated by means of experimental results.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/SDPPEV2J/bwmeta1.element.html}
}

@article{gutierrezDualModeMPC2008,
  title = {A {{Dual Mode MPC Scheme}} for {{Nonlinear Processes}}},
  author = {Guti{\'e}rrez, L. P. and Odloak, D. and Sotomayor, O. A. Z. and {\'A}lvarez, H. D.},
  year = {2008},
  month = jan,
  journal = {IFAC Proceedings Volumes},
  series = {17th {{IFAC World Congress}}},
  volume = {41},
  number = {2},
  pages = {12159--12164},
  issn = {1474-6670},
  doi = {10.3182/20080706-5-KR-1001.02059},
  urldate = {2023-12-23},
  abstract = {This paper presents a new dual mode nonlinear model predictive controller (NMPC) that is based on the combination of the finite horizon NMPC with the infinite horizon predictive controller (IHMPC). The resulting nonlinear controller is shown to be stable when the IHMPC is globally stabilizing. The main advantage of the proposed controller in comparison to the IHMPC is a better performance as the model nonlinearities are taken into account in the computation of the control law. The advantage of the proposed controller compared to the existing dual mode NMPC is that constraints are also considered in the linear controller that is supposed to control the system when the state enters the terminal set. The performance of the proposed controller is compared to the stable IHMPC through simulation of an industrial styrene polymerization reactor.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/J7AN3HKH/Gutiérrez et al. - 2008 - A Dual Mode MPC Scheme for Nonlinear Processes.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/YWVZS7PY/S1474667016409250.html}
}

@misc{haarnojaSoftActorCriticOffPolicy2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = aug,
  number = {arXiv:1801.01290},
  eprint = {1801.01290},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1801.01290},
  urldate = {2023-12-11},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/URSK6K8J/Haarnoja et al_2018_Soft Actor-Critic.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/YJ8VW2U7/1801.html}
}

@article{hemmingCherryTomatoProduction2020,
  title = {Cherry {{Tomato Production}} in {{Intelligent Greenhouses}}---{{Sensors}} and {{AI}} for {{Control}} of {{Climate}}, {{Irrigation}}, {{Crop Yield}}, and {{Quality}}},
  author = {Hemming, Silke and de Zwart, Feije and Elings, Anne and Petropoulou, Anna and Righini, Isabella},
  year = {2020},
  month = jan,
  journal = {Sensors},
  volume = {20},
  number = {22},
  pages = {6430},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s20226430},
  urldate = {2023-12-01},
  abstract = {Greenhouses and indoor farming systems play an important role in providing fresh and nutritious food for the growing global population. Farms are becoming larger and greenhouse growers need to make complex decisions to maximize production and minimize resource use while meeting market requirements. However, highly skilled labor is increasingly lacking in the greenhouse sector. Moreover, extreme events such as the COVID-19 pandemic, can make farms temporarily less accessible. This highlights the need for more autonomous and remote-control strategies for greenhouse production. This paper describes and analyzes the results of the second ``Autonomous Greenhouse Challenge''. In this challenge, an experiment was conducted in six high-tech greenhouse compartments during a period of six months of cherry tomato growing. The primary goal of the greenhouse operation was to maximize net profit, by controlling the greenhouse climate and crop with AI techniques. Five international teams with backgrounds in AI and horticulture were challenged in a competition to operate their own compartment remotely. They developed intelligent algorithms and use sensor data to determine climate setpoints and crop management strategy. All AI supported teams outperformed a human-operated greenhouse that served as reference. From the results obtained by the teams and from the analysis of the different climate-crop strategies, it was possible to detect challenges and opportunities for the future implementation of remote-control systems in greenhouse production.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,autonomous greenhouses,climate control,data driven growing,indoor farming,irrigation control,remote control,resource use efficiency,sensors,tomato yield},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2023-12-06T09:19:52.514Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/IZLHDVJB/Hemming et al. - 2020 - Cherry Tomato Production in Intelligent Greenhouse.pdf}
}

@article{hemmingRemoteControlGreenhouse2019,
  title = {Remote {{Control}} of {{Greenhouse Vegetable Production}} with {{Artificial Intelligence}}---{{Greenhouse Climate}}, {{Irrigation}}, and {{Crop Production}}},
  author = {Hemming, Silke and {de Zwart}, Feije and Elings, Anne and Righini, Isabella and Petropoulou, Anna},
  year = {2019},
  month = jan,
  journal = {Sensors},
  volume = {19},
  number = {8},
  pages = {1807},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s19081807},
  urldate = {2023-12-01},
  abstract = {The global population is increasing rapidly, together with the demand for healthy fresh food. The greenhouse industry can play an important role, but encounters difficulties finding skilled staff to manage crop production. Artificial intelligence (AI) has reached breakthroughs in several areas, however, not yet in horticulture. An international competition on ``autonomous greenhouses'' aimed to combine horticultural expertise with AI to make breakthroughs in fresh food production with fewer resources. Five international teams, consisting of scientists, professionals, and students with different backgrounds in horticulture and AI, participated in a greenhouse growing experiment. Each team had a 96 m2 modern greenhouse compartment to grow a cucumber crop remotely during a 4-month-period. Each compartment was equipped with standard actuators (heating, ventilation, screening, lighting, fogging, CO2 supply, water and nutrient supply). Control setpoints were remotely determined by teams using their own AI algorithms. Actuators were operated by a process computer. Different sensors continuously collected measurements. Setpoints and measurements were exchanged via a digital interface. Achievements in AI-controlled compartments were compared with a manually operated reference. Detailed results on cucumber yield, resource use, and net profit obtained by teams are explained in this paper. We can conclude that in general AI performed well in controlling a greenhouse. One team outperformed the manually-grown reference.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,crop production,indoor farming,resource use efficiency,sensors},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:10.810Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/6VRHFHRV/Hemming et al. - 2019 - Remote Control of Greenhouse Vegetable Production .pdf}
}

@book{hentenGreenhouseClimateManagement1994,
  title = {{Greenhouse climate management: an optical control approach}},
  shorttitle = {{Greenhouse climate management}},
  author = {van Henten, Eldert Jan},
  year = {1994},
  isbn = {978-90-5485-321-3},
  langid = {dutch},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ARDTWLB8/Henten - 1994 - Greenhouse climate management an optical control .pdf}
}

@article{Index197420042005,
  title = {Index (1974-2004)},
  year = {2005},
  journal = {Journal of Southern African Studies},
  volume = {31},
  eprint = {25064965},
  eprinttype = {jstor},
  pages = {1--198},
  publisher = {[Taylor \& Francis, Ltd., Journal of Southern African Studies]},
  issn = {0305-7070},
  urldate = {2023-12-01}
}

@mastersthesis{jansenOptimalControlLettuce2023,
  title = {Optimal {{Control}} of {{Lettuce Greenhouse Horticulture}} Using {{Model-Free Reinforcement Learning}}},
  author = {Jansen, Yde},
  year = {2023},
  urldate = {2023-12-01},
  abstract = {A greenhouse is an important growing system that can provide a controlled climate environment and allow for crop growth in a changing outdoor climate. Due to the high energy cost, labor and resource scarcity, optimal and automated control of greenhouse horticulture is becoming more and more important with the aim of optimizing resource usage while maximizing crop production. Outdoor weather is a critical disturbance when controlling greenhouse climate and it complicates the modelling and optimization processes. With the development of Artificial Intelligence (AI) and improved sensing techniques, Reinforcement Learning (RL) is getting more and more attention due to its learning-based control strategies, independent from having a good model. Up to now, most of the RL applications in greenhouse climate control do not consider outdoor weather forecast while making control decisions, which means plenty of useful information is missed and this might lead to control actions which are not optimal. Therefore in this project, we investigated how weather forecast horizons will affect optimal control of greenhouse horticulture using reinforcement learning. After going through different deep RL approaches, Soft Actor-Critic (SAC) and Twin-Delayed Deep Deterministic Policy Gradient (TD3) stand out because of their capacity to consider continuous state-action space. As the weather prediction will mainly work well in the short-term due to forecast uncertainty, moreover, long-term weather forecast will bring various unnecessary noise and a large state space. As a result, our work mainly focused on short-term weather forecast horizons of 0, 3, 7, 11, 15, 19, 23 time steps of fifteen minutes. To investigate how horizons can affect the control performance, these seven different horizons were used in experiments using the state-of-the-art continuous control algorithms, SAC and TD3. After demonstrating the proposed approaches in a lettuce greenhouse, we found that SAC consistently performed better than TD3 with higher rewards in terms of crop production and net profit, while resource use was comparable. Furthermore, inclusion of weather forecasts proved essential for both algorithm learning stability, as well as its training and generalization performance, resulting in increased yields and net profits, while reducing resource use and the amount of indoor climate constraint violations. Moreover, we can also conclude that four hours of weather forecast is the best option. Longer predictions did not increase performance, whereas using shorter forecasts quickly degraded performance.},
  copyright = {CC-BY-NC-ND},
  langid = {english},
  annotation = {Accepted: 2023-08-30T23:01:00Z\\
Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:59.297Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/JCCK8MDU/Jansen - 2023 - Optimal Control of Lettuce Greenhouse Horticulture.pdf}
}

@article{jonkerModelbasedReinforcementLearning,
  title = {Model-Based {{Reinforcement Learning}}: {{A Survey}}},
  author = {Jonker, M},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/8VWKVMGW/Jonker - Model-based Reinforcement Learning A Survey.pdf}
}

@article{knibbeDigitalTwinsGreen2022,
  title = {Digital Twins in the Green Life Sciences},
  author = {Knibbe, Willem Jan and Afman, Lydia and Boersma, Sjoerd and Bogaardt, Marc-Jeroen and Evers, Jochem and Van Evert, Frits and Van Der Heide, Jene and Hoving, Idse and Van Mourik, Simon and De Ridder, Dick and De Wit, Allard},
  year = {2022},
  month = dec,
  journal = {NJAS: Impact in Agricultural and Life Sciences},
  volume = {94},
  number = {1},
  pages = {249--279},
  issn = {2768-5241},
  doi = {10.1080/27685241.2022.2150571},
  urldate = {2024-01-16},
  abstract = {Digital twins provide a new paradigm for the integrated use of sensor data, process-based and data-driven modelling, and user interaction, to explore the behaviour of individual objects and processes. Digital twins originate from an engineering context and were developed for machines and mainly physical and chemical processes. In this paper, we further develop an understanding of digital twins for the green life sciences, which also include biological and social processes. We report on three use cases, in precision farming, greenhouse control and personalized dietary advice, focusing on practical benefits and challenges of digital twins compared with other research methods. This research extends earlier more conceptual research on digital twins in this domain. We find benefits in increased accuracy and impact because of the realtime data connection of digital twins to their real-life counterparts. Specification, availability and accuracy of relevant data sources are still major challenges. Specifically, when using digital twins for personalized advice, further research is needed on nontechnical aspects so that users will comply with the advice from the digital twins. We have outlined four directions of future research and expect that further research will include data-driven mod\- elling to simulate the complex character of living objects and processes and at the same time develop approaches to limit the amount of required input data.},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/85XF7FWB/Knibbe et al. - 2022 - Digital twins in the green life sciences.pdf}
}

@misc{KnowledgeBasedControlSystems,
  title = {Knowledge-{{Based Control Systems}} ({{SC42050}})},
  urldate = {2023-12-08},
  howpublished = {https://www.dcsc.tudelft.nl/{\textasciitilde}sc4081/2017/index.html},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-08T10:10:39.785Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/Z9YZQAKU/index.html}
}

@article{kuijpersModelSelectionCommon2019,
  title = {Model Selection with a Common Structure: {{Tomato}} Crop Growth Models},
  shorttitle = {Model Selection with a Common Structure},
  author = {Kuijpers, Wouter J. P. and {van de Molengraft}, Marinus J. G. and {van Mourik}, Simon and {van 't Ooster}, Albertus and Hemming, Silke and {van Henten}, Eldert J.},
  year = {2019},
  month = nov,
  journal = {Biosystems Engineering},
  volume = {187},
  pages = {247--257},
  issn = {1537-5110},
  doi = {10.1016/j.biosystemseng.2019.09.010},
  urldate = {2024-01-16},
  abstract = {Crop modelling is an essential part of biosystems engineering; selecting or developing a crop model for a specific application, having its requirements and desires, is difficult if not impossible without the required domain knowledge. This paper presents a fundamentally different model selection approach based on biological functionalities. This is enabled by a common structure, which allows for a combining of components, yielding new models. This increased design space allows the development of models which are better suited to the application than the original models. The use of a common structure, and its potential, are demonstrated by a use-case involving the selection of a tomato crop model for a model-based control application, but the rationales and methodologies can apply to other crops and applications as well. In this paper, 27 valid model combinations have been created from 4 models. In the use-case presented, the models are compared to data originating from a real system. The predictive performance of a model is quantified by the Root-Mean-Squared-Error (RSME) between the predictions and data. One trade-off is model accuracy versus computational speed. With the model set used, a 13\% decrease in RSME was obtained by allowing a 7.5\% increase in model computation time compared to one of the original models.},
  keywords = {/unread,Common structure,Model selection,Tomato crop growth models},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/QAN5NZ69/Kuijpers et al_2019_Model selection with a common structure.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/NQPMKUWG/S1537511019308323.html}
}

@incollection{lawrynczukMPCAlgorithms2014,
  title = {{{MPC Algorithms}}},
  booktitle = {Computationally {{Efficient Model Predictive Control Algorithms}}: {{A Neural Network Approach}}},
  author = {{\L}awry{\'n}czuk, Maciej},
  editor = {{\L}awry{\'n}czuk, Maciej},
  year = {2014},
  series = {Studies in {{Systems}}, {{Decision}} and {{Control}}},
  pages = {1--30},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-04229-9_1},
  urldate = {2023-12-19},
  abstract = {This chapter introduces the reader into the field of MPC. The basic MPC optimisation problem is defined, the fundamental role of the model is emphasised. The general classification of MPC algorithms is given, i.e. linear and nonlinear approaches are characterised. Next, some methods which make it possible to reduce computational burden of nonlinear MPC algorithms are shortly described, including the on-line linearisation approach. A history of MPC algorithms is given. Finally, a short review of nonlinear model structures is included, their advantages and disadvantages as well as possibilities of using them in MPC are pointed out.},
  isbn = {978-3-319-04229-9},
  langid = {english},
  keywords = {Hide Node,Internal Model Control,Manipulate Variable,Prediction Horizon,Sampling Instant},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/YD5TLKZQ/Ławryńczuk_2014_MPC Algorithms.pdf}
}

@misc{lillicrapContinuousControlDeep2019,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2019},
  month = jul,
  number = {arXiv:1509.02971},
  eprint = {1509.02971},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.02971},
  urldate = {2023-12-11},
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/EK2G5K5J/Lillicrap et al_2019_Continuous control with deep reinforcement learning.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/MFAS6Q3B/1509.html}
}

@article{linReinforcementLearningBasedModel2023,
  title = {Reinforcement {{Learning-Based Model Predictive Control}} for {{Discrete-Time Systems}}},
  author = {Lin, Min and Sun, Zhongqi and Xia, Yuanqing and Zhang, Jinhui},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--13},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2023.3273590},
  urldate = {2023-12-01},
  abstract = {This article proposes a novel reinforcement learning-based model predictive control (RLMPC) scheme for discrete-time systems. The scheme integrates model predictive control (MPC) and reinforcement learning (RL) through policy iteration (PI), where MPC is a policy generator and the RL technique is employed to evaluate the policy. Then the obtained value function is taken as the terminal cost of MPC, thus improving the generated policy. The advantage of doing so is that it rules out the need for the offline design paradigm of the terminal cost, the auxiliary controller, and the terminal constraint in traditional MPC. Moreover, RLMPC proposed in this article enables a more flexible choice of prediction horizon due to the elimination of the terminal constraint, which has great potential in reducing the computational burden. We provide a rigorous analysis of the convergence, feasibility, and stability properties of RLMPC. Simulation results show that RLMPC achieves nearly the same performance as traditional MPC in the control of linear systems and exhibits superiority over traditional MPC for nonlinear ones.},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:22:07.338Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/6XQI22BM/Lin et al. - 2023 - Reinforcement Learning-Based Model Predictive Cont.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/QP6JXDQS/10129251.html}
}

@article{lopez-cruzDevelopmentAnalysisDynamical2018,
  title = {Development and Analysis of Dynamical Mathematical Models of Greenhouse Climate: {{A}} Review},
  shorttitle = {Development and Analysis of Dynamical Mathematical Models of Greenhouse Climate},
  author = {{Lopez-Cruz}, Irineo and {Fitz-Rodr{\'i}guez}, Efr{\'e}n and Raquel, Salazar and {Rojano-Aguilar}, Abraham and Kacira, Murat},
  year = {2018},
  month = nov,
  journal = {European Journal of Horticultural Science},
  volume = {83},
  pages = {269--279},
  doi = {10.17660/eJHS.2018/83.5.1},
  abstract = {This paper summarizes the main developments achieved up to now on dynamical models of the greenhouse climate, regarding their structure, analysis, parameter estimation and model performance. The systems state-space approach is followed in order to describe main models' structure features. The physical processes included in the dynamic equations of greenhouse climate are emphasized. The type of equations used, either differential equations, difference equations or transfer functions are described. The dynamic models of greenhouse climate are classified in mechanistic and black-box models. Mechanistic models are mainly focused on the knowledge of the greenhouse system whereas black-box models are more used for applications, including: control, optimization and design of the greenhouse system. Main results of this study are that models of greenhouse climate used mostly ordinary differential equations either to know more the system or to control and optimize it. Only few models used difference equations or ARX. Also more complex greenhouse climate models have been developed to get insight of the greenhouse system while models with few states are more useful for control and optimization purposes. The dynamic models of greenhouse climate have mostly founded on the first law of thermodynamics, namely (energy/enthalpy) analysis and conservation of mass. Furthermore, although almost all the models have been calibrated and evaluated using measured data from the system, there is a lack of uncertainty and sensitivity analysis in the development of greenhouse climate models. In fact, none of the revised models were subjected to an uncertainty analysis. Some models of greenhouse environment have been reported with only a preliminary sensitivity analysis; a few of them with a formal local sensitivity analysis and none with a global sensitivity analysis. https://doi.org/10.17660/eJHS.2018/83.5.1},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/UD28IZH2/Lopez-Cruz et al_2018_Development and analysis of dynamical mathematical models of greenhouse climate.pdf}
}

@misc{lubarsCombiningReinforcementLearning2021,
  title = {Combining {{Reinforcement Learning}} with {{Model Predictive Control}} for {{On-Ramp Merging}}},
  author = {Lubars, Joseph and Gupta, Harsh and Chinchali, Sandeep and Li, Liyun and Raja, Adnan and Srikant, R. and Wu, Xinzhou},
  year = {2021},
  month = sep,
  number = {arXiv:2011.08484},
  eprint = {2011.08484},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-24},
  abstract = {We consider the problem of designing an algorithm to allow a car to autonomously merge on to a highway from an on-ramp. Two broad classes of techniques have been proposed to solve motion planning problems in autonomous driving: Model Predictive Control (MPC) and Reinforcement Learning (RL). In this paper, we first establish the strengths and weaknesses of state-of-the-art MPC and RL-based techniques through simulations. We show that the performance of the RL agent is worse than that of the MPC solution from the perspective of safety and robustness to out-of-distribution traffic patterns, i.e., traffic patterns which were not seen by the RL agent during training. On the other hand, the performance of the RL agent is better than that of the MPC solution when it comes to efficiency and passenger comfort. We subsequently present an algorithm which blends the model-free RL agent with the MPC solution and show that it provides better trade-offs between all metrics -- passenger comfort, efficiency, crash rate and robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/V7GPQTAP/Lubars et al_2021_Combining Reinforcement Learning with Model Predictive Control for On-Ramp.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/KG95RNEU/2011.html}
}

@article{lubbersAutonomousGreenhouseClimate2023,
  title = {Autonomous Greenhouse Climate Control with {{Q-learning}} Using {{ENMPC}} as a Function Approximator},
  author = {Lubbers, Seymour},
  year = {2023},
  urldate = {2023-12-01},
  abstract = {Greenhouses allow production of crops that would otherwise be impossible. Permitting more local, fresher and nutrient richer crop production. Eorts are taken to minimize societal harm due to energy and resource consumption by greenhouse production systems. One way to control such systems is by using model predictive control. Optimal crop yield and resource eciency can, in theory, be achieved by model predictive control. Unfortunately, one major drawback of model predictive control is that it is not well equipped to deal with parametric uncertainty. Significant prediction errors can occur when a mismatch between the model and the real system exists, resulting in deteriorated performance of the system. Strategies exist, such as robust MPC, that are designed to handle uncertainty, but those often result in conservative control policies. This thesis proposes to use model predictive control as a function approximator for RL in order to learn values for model and MPC parameters that can deliver optimal performance in the case of model mismatch.\&lt;br/\&gt;In this thesis, data-driven economic nonlinear model predictive control using Q-learning is proposed as a method to alter the model parameters. The performance of the system af- ter learning is compared to approaches using robust and nominal model predictive control. Three dierent goals are determined: maximizing economic profit, minimizing the constraint violations and maximizing the economic performance while minimizing constraint violations.\&lt;br/\&gt;In this work, an ENMPC scheme is used as a function approximator in a Q-learning envi- ronment. The optimization solution from the ENMPC scheme is used as the input to the system, while the Q-learning agent optimizes the parameter values of the ENMPC scheme and model for the environment. The performance of the system after learning is compared to approaches using robust and nominal model predictive control. The simulation results show that the data-driven ENMPC using reinforcement learning is able to decrease constraint vi- olations by up to 94\%, but unable to increase economic performance compared to nominal MPC, compared to robust MPC the EPI is increased by almost 10\% while keeping constraint violations at a similar level.},
  langid = {english},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:22.020Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LMBMYW4V/Lubbers - 2023 - Autonomous greenhouse climate control with Q-learn.pdf}
}

@article{maestriniMixingProcessbasedDatadriven2022,
  title = {Mixing Process-Based and Data-Driven Approaches in Yield Prediction},
  author = {Maestrini, Bernardo and Mimi{\'c}, Gordan and Van Oort, Pepijn A.J. and Jindo, Keiji and Brdar, Sanja and Athanasiadis, Ioannis N. and Van Evert, Frits K.},
  year = {2022},
  month = sep,
  journal = {European Journal of Agronomy},
  volume = {139},
  pages = {126569},
  issn = {11610301},
  doi = {10.1016/j.eja.2022.126569},
  urldate = {2024-01-15},
  abstract = {Yield prediction models can be divided between data-driven and process-based models (crop growth models). The first category contains many different types of models with parameters learned from the data themselves and where domain knowledge is only used to select the predictors and engineer features. In the second category, models are based upon biophysical principles, whose structure and parameters are derived primarily from domain knowledge. Here we investigate if the integration of the two approaches can be beneficial as it allows to overcome the limitations of the two approaches taken individually - lack of sufficiently large, reliable and orthogonal datasets for data-driven approaches and the need of many inputs for process-based models. The applications of the two categories of models have been reviewed, paying special attention to the cases where the two approaches have been mixed. By analysing the literature we identified three major cases of integration between the two approaches: (1) using crop growth models to engineer features and expand the predictors space, (2) use data-driven approaches to estimate missing inputs for process-based models (3) using data-driven ap\- proaches to produce meta-models to reduce computation burden. Finally we propose a methodology based on metamodels and transfer learning to integrate data-driven and process-based approaches.},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/636K2I5E/Maestrini et al. - 2022 - Mixing process-based and data-driven approaches in.pdf}
}

@article{mayneConstrainedModelPredictive2000,
  title = {Constrained Model Predictive Control: {{Stability}} and Optimality},
  shorttitle = {Constrained Model Predictive Control},
  author = {Mayne, D. Q. and Rawlings, J. B. and Rao, C. V. and Scokaert, P. O. M.},
  year = {2000},
  month = jun,
  journal = {Automatica},
  volume = {36},
  number = {6},
  pages = {789--814},
  issn = {0005-1098},
  doi = {10.1016/S0005-1098(99)00214-9},
  urldate = {2023-12-18},
  abstract = {Model predictive control is a form of control in which the current control action is obtained by solving, at each sampling instant, a finite horizon open-loop optimal control problem, using the current state of the plant as the initial state; the optimization yields an optimal control sequence and the first control in this sequence is applied to the plant. An important advantage of this type of control is its ability to cope with hard constraints on controls and states. It has, therefore, been widely applied in petro-chemical and related industries where satisfaction of constraints is particularly important because efficiency demands operating points on or close to the boundary of the set of admissible states and controls. In this review, we focus on model predictive control of constrained systems, both linear and nonlinear and discuss only briefly model predictive control of unconstrained nonlinear and/or time-varying systems. We concentrate our attention on research dealing with stability and optimality; in these areas the subject has developed, in our opinion, to a stage where it has achieved sufficient maturity to warrant the active interest of researchers in nonlinear control. We distill from an extensive literature essential principles that ensure stability and use these to present a concise characterization of most of the model predictive controllers that have been proposed in the literature. In some cases the finite horizon optimal control problem solved on-line is exactly equivalent to the same problem with an infinite horizon; in other cases it is equivalent to a modified infinite horizon optimal control problem. In both situations, known advantages of infinite horizon optimal control accrue.},
  keywords = {/unread,Model predictive control,Optimality,Robustness,Stability},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/E388NR2H/Mayne et al_2000_Constrained model predictive control.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/RKA8LYKQ/S0005109899002149.html}
}

@article{mismarFrameworkAutomatedCellular2019,
  title = {A {{Framework}} for {{Automated Cellular Network Tuning With Reinforcement Learning}}},
  author = {Mismar, Faris and Choi, Jinseok and Evans, Brian},
  year = {2019},
  month = oct,
  journal = {IEEE Transactions on Communications},
  volume = {67},
  pages = {7152--7167},
  doi = {10.1109/TCOMM.2019.2926715},
  abstract = {Tuning cellular network performance against always occurring wireless impairments can dramatically improve reliability to end users. In this paper, we formulate cellular network performance tuning as a reinforcement learning (RL) problem and provide a solution to improve the signal to interference-plus-noise ratio (SINR) for indoor and outdoor environments. By leveraging the ability of Q-learning to estimate future SINR improvement rewards, we propose two algorithms: (1) voice over LTE (VoLTE) downlink closed loop power control (PC) and (2) self-organizing network (SON) fault management. The VoLTE PC algorithm uses RL to adjust the indoor base station transmit power so that the effective SINR meets the target SINR. The SON fault management algorithm uses RL to improve the performance of an outdoor cluster by resolving faults in the network through configuration management. Both algorithms exploit measurements from the connected users, wireless impairments, and relevant configuration parameters to solve a non-convex SINR optimization problem using RL. Simulation results show that our proposed RL based algorithms outperform the industry standards today in realistic cellular communication environments.},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/8IMUDSRY/Mismar et al_2019_A Framework for Automated Cellular Network Tuning With Reinforcement Learning.pdf}
}

@misc{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  number = {arXiv:1602.01783},
  eprint = {1602.01783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.01783},
  urldate = {2023-12-11},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/P74LBWL7/Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/DU2M8YPW/1602.html}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-11},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/3A9B2AQU/Mnih et al_2013_Playing Atari with Deep Reinforcement Learning.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/39MC87S6/1312.html}
}

@article{montoyaHybridcontrolledApproachMaintaining2016,
  title = {A Hybrid-Controlled Approach for Maintaining Nocturnal Greenhouse Temperature: {{Simulation}} Study},
  shorttitle = {A Hybrid-Controlled Approach for Maintaining Nocturnal Greenhouse Temperature},
  author = {Montoya, Ana and Guzm{\'a}n, Jos{\'e} and Rodriguez, Francisco and {S{\'a}nchez-Molina}, Jorge},
  year = {2016},
  month = apr,
  journal = {Computers and Electronics in Agriculture},
  volume = {123},
  pages = {116--124},
  doi = {10.1016/j.compag.2016.02.014},
  keywords = {/unread}
}

@misc{morcegoReinforcementLearningModel2023,
  title = {Reinforcement {{Learning Versus Model Predictive Control}} on {{Greenhouse Climate Control}}},
  author = {Morcego, Bernardo and Yin, Wenjie and Boersma, Sjoerd and {van Henten}, Eldert and Puig, Vicen{\c c} and Sun, Congcong},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06110},
  eprint = {2303.06110},
  primaryclass = {math},
  publisher = {arXiv},
  urldate = {2023-12-01},
  abstract = {Greenhouse is an important protected horticulture system for feeding the world with enough fresh food. However, to maintain an ideal growing climate in a greenhouse requires resources and operational costs. In order to achieve economical and sustainable crop growth, efficient climate control of greenhouse production becomes essential. Model Predictive Control (MPC) is the most commonly used approach in the scientific literature for greenhouse climate control. However, with the developments of sensing and computing techniques, reinforcement learning (RL) is getting increasing attention recently. With each control method having its own way to state the control problem, define control goals, and seek for optimal control actions, MPC and RL are representatives of model-based and learning-based control approaches, respectively. Although researchers have applied certain forms of MPC and RL to control the greenhouse climate, very few effort has been allocated to analyze connections, differences, pros and cons between MPC and RL either from a mathematical or performance perspective. Therefore, this paper will 1) propose MPC and RL approaches for greenhouse climate control in an unified framework; 2) analyze connections and differences between MPC and RL from a mathematical perspective; 3) compare performance of MPC and RL in a simulation study and afterwards present and interpret comparative results into insights for the application of the different control approaches in different scenarios.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Optimization and Control},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:30.685Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/G5ZRK28N/Morcego et al. - 2023 - Reinforcement Learning Versus Model Predictive Con.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/H4SN9N5R/2303.html}
}

@article{morcegoReinforcementLearningModel2023a,
  title = {Reinforcement {{Learning}} versus {{Model Predictive Control}} on Greenhouse Climate Control},
  author = {Morcego, Bernardo and Yin, Wenjie and Boersma, Sjoerd and {van Henten}, Eldert and Puig, Vicen{\c c} and Sun, Congcong},
  year = {2023},
  month = dec,
  journal = {Computers and Electronics in Agriculture},
  volume = {215},
  pages = {108372},
  issn = {0168-1699},
  doi = {10.1016/j.compag.2023.108372},
  urldate = {2024-01-18},
  abstract = {The greenhouse system plays a crucial role to ensure an adequate supply of fresh food for the growing global population. However, maintaining an optimal growing climate within a greenhouse requires resources and operational costs. To achieve economical and sustainable crop growth, efficient climate control in greenhouse production is paramount. Model Predictive Control (MPC) and Reinforcement Learning (RL) are the two approaches representing model-based and learning-based control, respectively. Each one has its own way to formulate control problems, define control objectives, and seek for optimal control actions that provide sustainable crop growth. Although certain forms of MPC and RL have been applied to greenhouse climate control, limited research has comprehensively analyzed the connections, differences, advantages, and disadvantages between these two approaches, both mathematically and in terms of performance. Therefore, this paper aims to address this gap by: (1) introducing a novel RL approach that utilizes Deep Deterministic Policy Gradient (DDPG) for large and continuous state--action space environments; (2) formulating the MPC and RL approaches for greenhouse climate control within a unified framework; (3) exploring the mathematical connections and differences between MPC and RL; (4) conducting a simulation study to analyze and compare the performance of MPC and RL; (5) presenting and interpreting the comparative results to provide valuable insights for the application of these control approaches in different scenarios. By undertaking these objectives, this paper seeks to contribute to the understanding and advancement of both MPC and RL methods in greenhouse climate control, fostering more informed decision-making regarding their selection and implementation based on specific requirements and constraints.},
  keywords = {Greenhouse climate control,Model Predictive Control,Reinforcement Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/HUSX9AQQ/Morcego et al. - 2023 - Reinforcement Learning versus Model Predictive Con.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/8GIIEQ4H/S0168169923007603.html}
}

@article{munozComparingEnvironmentalImpacts2008,
  title = {Comparing the Environmental Impacts of Greenhouse versus Open-Field Tomato Production in the {{Mediterranean}} Region},
  author = {Mu{\~n}oz, Pere and Ant{\'o}n, Assumpci{\'o} and Nu{\~n}ez, M. and Paranjpe, Ashwin and Ari{\~n}o, J. and Castells, X. and Montera, J.I. and Rieradevall, Joan},
  year = {2008},
  month = nov,
  journal = {Acta Horticulturae},
  volume = {801},
  pages = {1591--1596},
  doi = {10.17660/ActaHortic.2008.801.197},
  abstract = {Greenhouse production is often perceived as an artificial process, characterized by low nutritional quality of the final product and the heavy use of chemical inputs. Moreover, large areas covered with greenhouses create a big visual impact, a factor which is especially important in the highly touristic Mediterranean Coastal. In contrast, open-field cultivation is generally perceived as an 'eco-friendly' activity, and one that has a much smaller visual impact. Setting aside these 'apparent' perceptions of the two cultivation systems, it is necessary to make an objective assessment and to quantify their respective impacts on the environment. Life cycle assessment (LCA) tool was used to compare the environmental burdens associated with greenhouse as opposed to open-field production processes for a spring season tomato crop grown in the Maresme region near Barcelona. Greenhouse structure, irrigation equipment, fertilizers, pesticides, cultural tasks and irrigation were all analyzed as subsystems. All inputs for each subsystem were traced back to primary resources. For each subsystem, emissions were quantified and aggregated into impact categories defined by CML 2001, using tomato yield (kg) as the functional unit. Preliminary results revealed that environmental burden per kg of tomato grown in open-field production was greater than that for tomatoes produced in greenhouses with respect to factors such as the use of water, fertilizers and pesticides. Notwithstanding the differences in environmental burden associated with the two production systems, if one considers the higher economic returns obtained from greenhouse production, their existence could constitute a reasonable trade-off.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/WPC4DUMN/Muñoz et al. - 2008 - Comparing the environmental impacts of greenhouse .pdf}
}

@misc{nishatGreenDealGreenhouse2020,
  title = {The ``{{Green Deal}}'' {{Greenhouse}}: {{A}} Promise for Sustainable Food Supply},
  shorttitle = {The ``{{Green Deal}}'' {{Greenhouse}}},
  author = {{Nishat}},
  year = {2020},
  month = sep,
  journal = {Open Access Government},
  urldate = {2023-12-01},
  abstract = {Brite Hellas S.A, discusses the ``Green Deal'' Greenhouse, A Promise for Sustainable Food Supply starting with problems and needs},
  langid = {british},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/QJFUKWV6/93949.html}
}

@techreport{OptimalPolicyExistencePdf,
  title = {{{OptimalPolicyExistence}}.Pdf},
  urldate = {2023-12-09},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/W7XMNF6W/OptimalPolicyExistence.pdf}
}

@article{ozturkDETERMINATIONHEATINGREQUIREMENTS2016,
  title = {{{DETERMINATION OF HEATING REQUIREMENTS AND ENERGY CONSUMPTION OF GREENHOUSES IN ADANA REGION OF TURKEY}}},
  author = {Ozturk, H. and Kucukerdem, Kaan},
  year = {2016},
  month = jun,
  journal = {AgroLife Scientific Journal},
  volume = {5},
  pages = {157--160},
  abstract = {In this study, the heating loads of plastic greenhouses were determined based on long term meteorological data in Adana region of Turkey. Considering the air temperature requirements of warm season species, energy consumptions were calculated for heating periods. If the monthly average low temperatures are considered as the outside air temperature, when the air temperature in the greenhouse is considered as in 18 C, the average greenhouse heating load is 64.4 W/m2, during the October-May heating period. However, when the air temperature in the greenhouse is considered as in 15 C, the average greenhouse heating load is 44.5 W/m2 during the same period. The average energy consumption for 1 da greenhouse heating (Tg=15C) is 176 MJ/h, during the October-May heating period.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/VMF9VJKJ/Ozturk and Kucukerdem - 2016 - DETERMINATION OF HEATING REQUIREMENTS AND ENERGY C.pdf}
}

@misc{pankajakshanHowClimateSmart2023,
  title = {How {{Climate Smart Agriculture Can Help Us Tackle Global Food Insecurity}}},
  author = {Pankajakshan, Praveen},
  year = {2023},
  month = may,
  journal = {Earth.Org},
  urldate = {2023-12-01},
  abstract = {Climate Smart Agriculture (CSA) is a practice that aims to address the interlinked challenges of food security and climate change. It can help farmers worldwide to prepare for the future by increasing productivity, enhancing resilience, and reducing emissions.},
  howpublished = {https://earth.org/climate-smart-agriculture/},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/229KE2WC/climate-smart-agriculture.html}
}

@misc{PartKindsRL,
  title = {Part 2: {{Kinds}} of {{RL Algorithms}} --- {{Spinning Up}} Documentation},
  urldate = {2023-12-11},
  howpublished = {https://spinningup.openai.com/en/latest/spinningup/rl\_intro2.html},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/2VW9U7R4/rl_intro2.html}
}

@article{raoOPTIMALPOLICYOPTIMAL,
  title = {{{OPTIMAL POLICY FROM OPTIMAL VALUE FUNCTION}}},
  author = {Rao, Ashwin},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/72UXP5YU/Rao - OPTIMAL POLICY FROM OPTIMAL VALUE FUNCTION.pdf}
}

@inproceedings{rawlingsFundamentalsEconomicModel2012,
  title = {Fundamentals of Economic Model Predictive Control},
  booktitle = {2012 {{IEEE}} 51st {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Rawlings, James B. and Angeli, David and Bates, Cuyler N.},
  year = {2012},
  month = dec,
  pages = {3851--3861},
  issn = {0743-1546},
  doi = {10.1109/CDC.2012.6425822},
  urldate = {2023-12-01},
  abstract = {The goal of most current advanced control systems is to guide a process to a target setpoint rapidly and reliably. Model predictive control has become a popular technology in many applications because it can handle large, multivariable systems subject to hard constraints on states and inputs. The optimal steady-state setpoint is usually provided by some other information management system that determines, among all steady states, which is the most profitable. For an increasing number of applications, however, this hierarchical separation of information and purpose is no longer optimal or desirable. A recently proposed alternative to the hierarchical decomposition is to take the economic objective directly as the objective function of the control system. In this approach, known as economic MPC, the controller optimizes directly in real time the economic performance of the process, rather than tracking to a setpoint. The purpose of this tutorial is to explain how to design these kinds of control systems and what kinds of closed-loop properties one can achieve with them. We cover the following issues: asymptotic average performance; closed-loop stability and convergence, strong duality and dissipativity; designing terminal costs, terminal regions, and terminal periodic constraints. Several examples are included to illustrate these results.},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:18:37.954Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/9SG4IQSN/Rawlings et al. - 2012 - Fundamentals of economic model predictive control.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/R9HKAS9M/6425822.html}
}

@book{rawlingsModelPredictiveControl2017,
  title = {Model Predictive Control: Theory, Computation, and Design},
  shorttitle = {Model Predictive Control},
  author = {Rawlings, James Blake and Mayne, David Q. and Diehl, Moritz},
  year = {2017},
  edition = {2nd edition},
  publisher = {Nob Hill Publishing},
  address = {Madison, Wisconsin},
  isbn = {978-0-9759377-3-0},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/4SUWRAMD/Rawlings et al. - 2017 - Model predictive control theory, computation, and.pdf}
}

@misc{ReinforcementLearningModel,
  title = {Reinforcement {{Learning Versus Model Predictive Control}}: {{A Comparison}} on a {{Power System Problem}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2023-12-01},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/4717266},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:33.455Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LLQ7KW83/4717266.html}
}

@article{risbeckEconomicModelPredictive2020,
  title = {Economic {{Model Predictive Control}} for {{Time-Varying Cost}} and {{Peak Demand Charge Optimization}}},
  author = {Risbeck, Michael J and Rawlings, James B},
  year = {2020},
  journal = {IEEE TRANSACTIONS ON AUTOMATIC CONTROL},
  volume = {65},
  number = {7},
  abstract = {With the increasing prevalence of variablesupply electricity production, dynamic market structures, including time-varying prices and/or peak demand charges are becoming more common for electricity consumers. This framework requires consumers to consider both the time-varying amount of electricity (i.e., energy) consumed throughout the day as well as the maximum rate of electricity purchase (i.e., power) over a given period, typically a month. Because of this complexity, online optimization techniques such as economic model predictive control (MPC) are a natural tool for consumers to use to minimize cost. However, while closed-loop optimization of these pricing structures is already being proposed for various applications, little has been established about stability or performance properties of the closed-loop system. Due in particular to the peak penalty (which violates the principle of optimality if naively included in the objective function), this theoretical gap leaves the potential for pathological closed-loop behavior despite high-quality open-loop solutions. In this paper, we derive asymptotic performance and stability results for general time-varying economic MPC. We then present a novel extended-state formulation to convert peak demand charges into a time-varying stage cost that can be optimized using economic MPC. In addition, we give a terminal cost and constraint for the augmented system that avoids reducing the feasible set in the original space. Finally, we demonstrate these structures and the closed-loop properties that they satisfy via two illustrative examples.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/JSDCVA7G/Risbeck and Rawlings - 2020 - Economic Model Predictive Control for Time-Varying.pdf}
}

@article{royPAPrecisionAgriculture2002,
  title = {{{PA}}---{{Precision Agriculture}}: {{Convective}} and {{Ventilation Transfers}} in {{Greenhouses}}, {{Part}} 1: The {{Greenhouse}} Considered as a {{Perfectly Stirred Tank}}},
  shorttitle = {{{PA}}---{{Precision Agriculture}}},
  author = {Roy, J. C. and Boulard, T. and Kittas, C. and Wang, S.},
  year = {2002},
  month = sep,
  journal = {Biosystems Engineering},
  volume = {83},
  number = {1},
  pages = {1--20},
  issn = {1537-5110},
  doi = {10.1006/bioe.2002.0107},
  urldate = {2024-01-16},
  abstract = {In this paper, the characterization and modelling of the most relevant convective transfers contributing to the elaboration of the greenhouse climate are reviewed. Convective transfers include heat and mass transfers between air and solid surfaces (walls, roof, leaves) along with air, heat, water vapour and tracer gas transfers to or from the inside air. Adopting the assumption that the greenhouse is a perfectly stirred tank, the specific characterization methods associated with this approach are reviewed. The perfectly stirred tank approach requires the assumption of uniform temperature, humidity and CO2 content inside the greenhouse and uses a `big leaf' model to treat the plant canopy and describe the exchanges of latent and sensible heat with inside air. The simulation of the ventilation processes associated with this simplified approach is based on the Bernoulli equation and on the experimental determination of semi-empirical parameters by means of air exchange rate measurements. The techniques used to measure temperature and air exchange rates measurements pertaining to the whole greenhouse volume are presented. A complete panorama of the studies in relation to the transfer coefficients between the different surfaces together with the ventilation performances of various greenhouse types are also presented. This paper is the first part of a review of the convective transfers in greenhouses and in the second paper, a similar study based on the approach of the distributed climate is presented.},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/5JSL3N9F/Roy et al_2002_PA—Precision Agriculture.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/PP5XF4WM/S1537511002901078.html}
}

@misc{rusnakWhatCurrentState2018,
  title = {What {{Is}} the {{Current State}} of {{Labor}} in the {{Greenhouse Industry}}?},
  author = {Rusnak, Paul},
  year = {2018},
  month = nov,
  journal = {Greenhouse Grower},
  urldate = {2023-12-01},
  abstract = {Growers are at a crossroads when it comes to finding a reliable labor supply at a price they can afford. For many operations, their future hangs in the balance.},
  langid = {american},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/YAXUEMW8/what-is-the-current-state-of-labor-in-the-greenhouse-industry.html}
}

@article{sahinExpertSystemDesign2014,
  title = {An Expert System Design and Application for Hydroponics Greenhouse Systems},
  author = {{{\c s}ahin}, Ismail and CALP, M. Hanefi and {\"O}zkan, Atacan},
  year = {2014},
  month = jan,
  journal = {Gazi University Journal of Science},
  volume = {27},
  pages = {809--822},
  abstract = {In this study, the hydroponics subject is briefly discussed and developed an expert system on the subject. Expert system, the process of upbringing of plants produced in the hydroponics systems has controlled. The system is able to determine to the values of input parameters by using output parameters entered by user. The input parameters preparing the optimum growing environment for plants by controlling the process of plant breeding. Thus, removal of the optimum level of efficiency in activities the hydroponics and minimize of the labor force and cost will be spent in the process of plant growth is aimed.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/3T2H4GIU/şahin et al. - 2014 - An expert system design and application for hydrop.pdf}
}

@misc{sahooSmartGreenhouseBoon2022,
  title = {Smart {{Greenhouse}}: {{A Boon}} or {{Bane}} for {{Agriculture}}?},
  shorttitle = {Smart {{Greenhouse}}},
  author = {Sahoo, Chiranjib},
  year = {2022},
  month = oct,
  journal = {Medium},
  urldate = {2023-12-01},
  abstract = {Smart Greenhouse is not only solving the problem of sudden climate changes but also helping enhance crop yield in agriculture. Read through{\dots}},
  howpublished = {https://write.agrevolution.in/smart-greenhouse-a-boon-or-bane-for-agriculture-dbd75fb606d3},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/HC48L2YK/smart-greenhouse-a-boon-or-bane-for-agriculture-dbd75fb606d3.html}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2023-12-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/E95GSGQL/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/VHCBYHNU/1707.html}
}

@misc{schulmanTrustRegionPolicy2017,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2017},
  month = apr,
  number = {arXiv:1502.05477},
  eprint = {1502.05477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1502.05477},
  urldate = {2023-12-11},
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arxiv},
  keywords = {/unread,Computer Science - Machine Learning},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/P3YT52TF/Schulman et al_2017_Trust Region Policy Optimization.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/M2SQXLZ5/1502.html}
}

@article{schwenzerReviewModelPredictive2021,
  title = {Review on Model Predictive Control: An Engineering Perspective},
  shorttitle = {Review on Model Predictive Control},
  author = {Schwenzer, Max and Ay, Muzaffer and Bergs, Thomas and Abel, Dirk},
  year = {2021},
  month = nov,
  journal = {The International Journal of Advanced Manufacturing Technology},
  volume = {117},
  number = {5},
  pages = {1327--1349},
  issn = {1433-3015},
  doi = {10.1007/s00170-021-07682-3},
  urldate = {2023-12-04},
  abstract = {Model-based predictive control (MPC) describes a set of advanced control methods, which make use of a process model to predict the future behavior of the controlled system. By solving a---potentially constrained---optimization problem, MPC determines the control law implicitly. This shifts the effort for the design of a controller towards modeling of the to-be-controlled process. Since such models are available in many fields of engineering, the initial hurdle for applying control is deceased with MPC. Its implicit formulation maintains the physical understanding of the system parameters facilitating the tuning of the controller. Model-based predictive control (MPC) can even control systems, which cannot be controlled by conventional feedback controllers. With most of the theory laid out, it is time for a concise summary of it and an application-driven survey. This review article should serve as such. While in the beginnings of MPC, several widely noticed review paper have been published, a comprehensive overview on the latest developments, and on applications, is missing today. This article reviews the current state of the art including theory, historic evolution, and practical considerations to create intuitive understanding. We lay special attention on applications in order to demonstrate what is already possible today. Furthermore, we provide detailed discussion on implantation details in general and strategies to cope with the computational burden---still a major factor in the design of MPC. Besides key methods in the development of MPC, this review points to the future trends emphasizing why they are the next logical steps in MPC.},
  langid = {english},
  keywords = {Application,Computation,Model predictive control,MPC,Optimization,Robustness,Stability},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:38.678Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/52XKMDRA/Schwenzer et al. - 2021 - Review on model predictive control an engineering.pdf}
}

@article{sharmaHydroponicsAdvancedTechnique2019,
  title = {Hydroponics as an Advanced Technique for Vegetable Production: {{An}} Overview},
  shorttitle = {Hydroponics as an Advanced Technique for Vegetable Production},
  author = {Sharma, Nisha and Acharya, Somen and Kumar, Kaushal and Singh, Narendra and Chaurasia, Om},
  year = {2019},
  month = jan,
  journal = {Journal of Soil and Water Conservation},
  volume = {17},
  pages = {364--371},
  doi = {10.5958/2455-7145.2018.00056.5},
  abstract = {Currently hydroponic cultivation is gaining popularity all over the world because of efficient resources management and quality food production. Soil based agriculture is now facing various challenges such as urbanization, natural disaster, climate change, indiscriminate use of chemicals and pesticides which is depleting the land fertility. In this article various hydroponic structures viz. wick, ebb and flow, drip, deep water culture and Nutrient Film Technique (NFT) system; their operations; benefits and limitations; performance of different crops like tomato, cucumber, pepper and leafy greens and water conservation by this technique have been discussed. Several benefits of this technique are less growing time of crops than conventional growing; round the year production; minimal disease and pest incidence and weeding, spraying, watering etc can be eliminated. Commercially NFT technique has been used throughout the world for successful production of leafy as well as other vegetables with 70 to 90\% savings of water. Leading countries in hydroponic technology are Netherland, Australia, France, England, Israel, Canada and USA. For successful implementation of commercial hydroponic technology, it is important to develop low cost techniques which are easy to operate and maintain; requires less labour and lower overall setup and operational cost.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/UBRXVFK7/Sharma et al. - 2019 - Hydroponics as an advanced technique for vegetable.pdf}
}

@misc{sikchiLearningOffPolicyOnline2021,
  title = {Learning {{Off-Policy}} with {{Online Planning}}},
  author = {Sikchi, Harshit and Zhou, Wenxuan and Held, David},
  year = {2021},
  month = oct,
  number = {arXiv:2008.10066},
  eprint = {2008.10066},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {Reinforcement learning (RL) in low-data and risk-sensitive domains requires performant and flexible deployment policies that can readily incorporate constraints during deployment. One such class of policies are the semi-parametric H-step lookahead policies, which select actions using trajectory optimization over a dynamics model for a fixed horizon with a terminal value function. In this work, we investigate a novel instantiation of H-step lookahead with a learned model and a terminal value function learned by a model-free off-policy algorithm, named Learning Off-Policy with Online Planning (LOOP). We provide a theoretical analysis of this method, suggesting a tradeoff between model errors and value function errors and empirically demonstrate this tradeoff to be beneficial in deep reinforcement learning. Furthermore, we identify the "Actor Divergence" issue in this framework and propose Actor Regularized Control (ARC), a modified trajectory optimization procedure. We evaluate our method on a set of robotic tasks for Offline and Online RL and demonstrate improved performance. We also show the flexibility of LOOP to incorporate safety constraints during deployment with a set of navigation environments. We demonstrate that LOOP is a desirable framework for robotics applications based on its strong performance in various important RL settings. Project video and details can be found at https://hari-sikchi.github.io/loop .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/8VQBER3U/Sikchi et al_2021_Learning Off-Policy with Online Planning.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/S6VB5MII/2008.html}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/9WRATLT6/Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@article{silverGeneralReinforcementLearning2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  journal = {Science (New York, N.Y.)},
  volume = {362},
  number = {6419},
  pages = {1140--1144},
  issn = {1095-9203},
  doi = {10.1126/science.aar6404},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.},
  langid = {english},
  pmid = {30523106},
  keywords = {/unread,Algorithms,Artificial Intelligence,Humans,Reinforcement Psychology,Software,Video Games},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/I5YDK529/Silver et al_2018_A general reinforcement learning algorithm that masters chess, shogi, and Go.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/XYYJXBX2/Silver et al_2018_A general reinforcement learning algorithm that masters chess, shogi, and Go.pdf}
}

@book{smitEnergiemonitorVanNederlandse2021,
  title = {{Energiemonitor van de Nederlandse glastuinbouw 2020}},
  author = {Smit, Pepijn and van der Velden, Nico},
  year = {2021},
  publisher = {Wageningen Economic Research},
  doi = {10.18174/555540},
  urldate = {2023-12-01},
  isbn = {978-94-6447-000-0},
  langid = {dutch},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/98W2D8HK/Smit and Velden - 2021 - Energiemonitor van de Nederlandse glastuinbouw 202.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/I5RQ5SEC/energiemonitor-van-de-nederlandse-glastuinbouw-2020.html}
}

@misc{StatLineEnergyBalance,
  title = {{{StatLine}} - {{Energy}} Balance; Supply, Conversion and Consumption},
  urldate = {2023-12-01},
  abstract = {Energieaanbod, omzetting, verbruik, verbruikssaldo, finaal verbruik Energiedragers, aardgas, hernieuwbaar, elektriciteit, olie, kolen, warmte},
  howpublished = {https://opendata.cbs.nl/\#/CBS/nl/dataset/83140NED/table?defaultview},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/N4NYICWK/opendata.cbs.nl.html}
}

@misc{StatLineVegetableCultivation,
  title = {{{StatLine}} - {{Vegetable}} Cultivation; Harvest and Cultivation Area per Vegetable Type},
  urldate = {2023-12-01},
  abstract = {Geoogste hoeveelheden groenten met bijbehorende teeltoppervlakten. Per groentesoort.},
  howpublished = {https://opendata.cbs.nl/statline/\#/CBS/nl/dataset/37738/table?fromstatweb},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LSS8RLIA/statline.html}
}

@misc{StockfishChessEngines,
  title = {Stockfish - {{Chess Engines}}},
  journal = {Chess.com},
  urldate = {2023-12-06},
  abstract = {Learn all about the Stockfish chess engine. Everything you need to know about Stockfish, including what it is, why it is important, and more!},
  howpublished = {https://www.chess.com/terms/stockfish-chess-engine},
  langid = {american},
  keywords = {/unread},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-06T09:54:28.990Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/YHCEAF8V/stockfish-chess-engine.html}
}

@book{suttonReinforcementLearningIntroduction2014,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew},
  year = {2014},
  series = {Adaptive Computation and Machine Learning},
  edition = {Nachdruck},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-19398-6},
  langid = {english},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:08.442Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/B3BSWSLK/Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf}
}

@book{taguchiFRUITVEGETABLESYour2020,
  title = {{{FRUIT AND VEGETABLES}} - Your Dietary Essentials. {{The International Year}} of {{Fruits}} and {{Vegetables}}, 2021 Background Paper},
  author = {Taguchi, Makiko and Beed, Fenton and Telemans, Bruno and Hassan, Sara},
  year = {2020},
  month = dec,
  doi = {10.4060/cb2395en},
  abstract = {Background paper for the International Year of Fruits and Vegetables (2021).},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/7KYS954E/Taguchi et al. - 2020 - FRUIT AND VEGETABLES - your dietary essentials. Th.pdf}
}

@inproceedings{taoOptimizingCropManagement2023,
  title = {Optimizing {{Crop Management}} with {{Reinforcement Learning}} and {{Imitation Learning}}},
  booktitle = {Proceedings of the {{Thirty-Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Tao, Ran and Zhao, Pan and Wu, Jing and Martin, Nicolas and Harrison, Matthew T. and Ferreira, Carla and Kalantari, Zahra and Hovakimyan, Naira},
  year = {2023},
  month = aug,
  pages = {6228--6236},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  address = {Macau, SAR China},
  doi = {10.24963/ijcai.2023/691},
  urldate = {2023-12-05},
  abstract = {Crop management has a significant impact on crop yield, economic profit, and the environment. Although management guidelines exist, finding the optimal management practices is challenging. Previous work used reinforcement learning (RL) and crop simulators to solve the problem, but the trained policies either have limited performance or are not deployable in the real world. In this paper, we present an intelligent crop management system that optimizes nitrogen fertilization and irrigation simultaneously via RL, imitation learning (IL), and crop simulations using the Decision Support System for Agrotechnology Transfer (DSSAT). We first use deep RL, in particular, deep Q-network, to train management policies that require a large number of state variables from the simulator as observations (denoted as full observation). We then invoke IL to train management policies that only need a few state variables that can be easily obtained or measured in the real world (denoted as partial observation) by mimicking the actions of the RL policies trained under full observation. Simulation experiments using the maize crop in Florida (US) and Zaragoza (Spain) demonstrate that the trained policies from both RL and IL techniques achieved more than 45\% improvement in economic profit while causing less environmental impact compared with a baseline method. Most importantly, the IL-trained management policies are directly deployable in the real world as they use readily available information.},
  isbn = {978-1-956792-03-4},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-05T19:35:10.081Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/Q2IZFUHD/Tao et al. - 2023 - Optimizing Crop Management with Reinforcement Lear.pdf}
}

@article{toubeauDeepReinforcementLearningBased2020,
  title = {Deep {{Reinforcement Learning-Based Voltage Control}} to {{Deal}} with {{Model Uncertainties}} in {{Distribution Networks}}},
  author = {Toubeau, Jean-Fran{\c c}ois and {bakhshideh zad}, Bashir and Hupez, Martin and De Greve, Zacharie and Vallee, Fran{\c c}ois},
  year = {2020},
  month = aug,
  journal = {Energies},
  volume = {13},
  doi = {10.3390/en13153928},
  abstract = {This paper addresses the voltage control problem in medium-voltage distribution networks. The objective is to cost-efficiently maintain the voltage profile within a safe range, in presence of uncertainties in both the future working conditions, as well as the physical parameters of the system. Indeed, the voltage profile depends not only on the fluctuating renewable-based power generation and load demand, but also on the physical parameters of the system components. In reality, the characteristics of loads, lines and transformers are subject to complex and dynamic dependencies, which are difficult to model. In such a context, the quality of the control strategy depends on the accuracy of the power flow representation, which requires to capture the non-linear behavior of the power network. Relying on the detailed analytical models (which are still subject to uncertainties) introduces a high computational power that does not comply with the real-time constraint of the voltage control task. To address this issue, while avoiding arbitrary modeling approximations, we leverage a deep reinforcement learning model to ensure an autonomous grid operational control. Outcomes show that the proposed model-free approach offers a promising alternative to find a compromise between calculation time, conservativeness and economic performance.},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/PJAZSC7J/Toubeau et al_2020_Deep Reinforcement Learning-Based Voltage Control to Deal with Model.pdf}
}

@misc{TwinDelayedDDPG,
  title = {Twin {{Delayed DDPG}} --- {{Spinning Up}} Documentation},
  urldate = {2023-12-13},
  howpublished = {https://spinningup.openai.com/en/latest/algorithms/td3.html},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/TTPGTANU/td3.html}
}

@phdthesis{vandenbemdRobustDeepReinforcement,
  title = {Robust {{Deep Reinforcement Learning}} for {{Greenhouse Control}} and {{Crop Yield Optimization}}},
  author = {{van den Bemd}, W.J.G.M.},
  urldate = {2023-12-01},
  abstract = {In this study we compare adaptations of two state of the art reinforcement learning algorithms, PPO and SAC, to optimize grower profits in our own simulated hydroponics lettuce greenhouse. We have shown that the application of physics randomization significantly boosts the worst case performance of both SAC and PPO. In our analysis, we trained the algorithms in a single greenhouse and evaluated it in many different greenhouses. While the bare version of PPO and SAC perform better than our baselines when evaluated in the greenhouse it was trained on we observed severe performance degradation when the evaluation greenhouse was altered. To solve this, we compared three different methods. We tested RARL: Robust Adversarial Reinforcement Learning, physics randomization and Gaussian observation noise as three adaptations for PPO and SAC. We have shown empirically that reinforcement learning agents using physics randomization perform better than the baselines in the greenhouse it was trained on. The adapted algorithms show improved performance in the worst case, maintaining performance in the average case, and improved performance in the best case.},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2023-12-06T11:26:02.711Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/HZT8EJG6/Bemd_W.pdf}
}

@misc{vandenboomCourseSlidesEE4C04,
  title = {Course Slides - {{EE4C04 Control System Design}} (2023/24 {{Q1}})},
  author = {{van den Boom}, Ton},
  urldate = {2023-12-18},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/Q6FY7ILC/Home.html}
}

@article{vandijkMetaanalysisProjectedGlobal2021,
  title = {A Meta-Analysis of Projected Global Food Demand and Population at Risk of Hunger for the Period 2010--2050},
  author = {{van Dijk}, Michiel and Morley, Tom and Rau, Marie Luise and Saghai, Yashar},
  year = {2021},
  month = jul,
  journal = {Nature Food},
  volume = {2},
  number = {7},
  pages = {494--501},
  publisher = {Nature Publishing Group},
  issn = {2662-1355},
  doi = {10.1038/s43016-021-00322-9},
  urldate = {2023-12-01},
  abstract = {Quantified global scenarios and projections are used to assess long-term future global food security under a range of socio-economic and climate change scenarios. Here, we conducted a systematic literature review and meta-analysis to assess the range of future global food security projections to 2050. We reviewed 57 global food security projection and quantitative scenario studies that have been published in the past two decades and discussed the methods, underlying drivers, indicators and projections. Across five representative scenarios that span divergent but plausible socio-economic futures, the total global food demand is expected to increase by 35\% to 56\% between 2010 and 2050, while population at risk of hunger is expected to change by -91\% to +8\% over the same period. If climate change is taken into account, the ranges change slightly (+30\% to +62\% for total food demand and -91\% to +30\% for population at risk of hunger) but with no statistical differences overall. The results of our review can be used to benchmark new global food security projections and quantitative scenario studies and inform policy analysis and the public debate on the future of food.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Environmental studies,Socioeconomic scenarios},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/UGPTBK2T/van Dijk et al. - 2021 - A meta-analysis of projected global food demand an.pdf}
}

@article{vanhentenTimescaleDecompositionOptimal2009,
  title = {Time-Scale Decomposition of an Optimal Control Problem in Greenhouse Climate Management},
  author = {Van Henten, E. J. and Bontsema, J.},
  year = {2009},
  month = jan,
  journal = {Control Engineering Practice},
  volume = {17},
  number = {1},
  pages = {88--96},
  issn = {0967-0661},
  doi = {10.1016/j.conengprac.2008.05.008},
  urldate = {2024-01-27},
  abstract = {Based on differences in dynamic response times in the crop production process, a hierarchical decomposition of greenhouse climate management is proposed. To a large extent the proposed decomposition builds on the time-scale decomposition of singularly perturbed systems commonly found in the literature. Main difference with these existing theoretical concepts is that the proposed decomposition is able to deal with rapidly fluctuating deterministic external inputs or disturbances acting on the fast sub-processes. For an example of economic optimal greenhouse climate management during one lettuce production cycle, the decomposition was successfully evaluated in simulations. Using these favourable results, a hierarchical concept for economic optimal greenhouse climate management is derived and discussed in view of application in horticultural practice.},
  keywords = {Climate control,Hierarchical control,Lettuce,Maximum principle,Singular perturbation},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/FHFSLWSL/Van Henten and Bontsema - 2009 - Time-scale decomposition of an optimal control pro.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/MA35Z7TC/S0967066108001019.html}
}

@article{vanhentenValidationDynamicLettuce1994,
  title = {Validation of a Dynamic Lettuce Growth Model for Greenhouse Climate Control},
  author = {Van Henten, E. J.},
  year = {1994},
  month = jan,
  journal = {Agricultural Systems},
  volume = {45},
  number = {1},
  pages = {55--72},
  issn = {0308-521X},
  doi = {10.1016/S0308-521X(94)90280-1},
  urldate = {2024-01-16},
  abstract = {Validation results are presented of a dynamic crop growth model of lettuce (Lactuca sativa L.) previously used in a greenhouse climate optimization study. The model describes the dynamic behaviour of two state variables, the non-structural dry weight and the structural dry weight, as affected by the incident photosynthetically active radiation, the carbon dioxide concentration and the air temperature in the greenhouse. Model equations and parameters have been collected from the literature. Because in the control study the economic return of a lettuce cultivation was considered to be determined by total crop dry weight, the model's ability to describe the dynamic behaviour of the dry matter content of a lettuce crop has been evaluated. Comparison of simulations with data obtained in two growth experiments showed that the growth model was able to describe with satisfactory accuracy the dry matter production of lettuces cultivated according to standard horticultural practice.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/ISQFIB8A/Van Henten_1994_Validation of a dynamic lettuce growth model for greenhouse climate control.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/QQ4DNEPL/S0308521X94902801.html}
}

@misc{vanmourikPlantPerformancePrecision2023,
  title = {Plant {{Performance}} in {{Precision Horticulture}}: {{Optimal}} Climate Control under Stochastic Uncertainty},
  shorttitle = {Plant {{Performance}} in {{Precision Horticulture}}},
  author = {{van Mourik}, Simon and van't Ooster, Bert and Vellekoop, Michel},
  year = {2023},
  month = jul,
  number = {arXiv:2303.14678},
  eprint = {2303.14678},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.14678},
  urldate = {2023-12-01},
  abstract = {This paper presents a risk mitigating, time-varying feedback control algorithm for crop production when state dynamics are subject to uncertainty. The model based case study concerns a 40 day production round of lettuce in a greenhouse where control input consists of daily and nightly temperature set points. The control problem was formulated in terms of a stochastic Markov decision process with the objective to maximize the expected net revenue at harvest time. The importance of time-varying feedback and of risk mitigation was investigated by making a comparison with a controller that takes uncertainty into account but is static and a controller which is dynamic but ignores the uncertainty in the state dynamics. For the case of heat limited crop growth, and strict requirements on harvest weight precision, the dynamic stochastic controller outperformed the static controller in terms of both maximal expected net revenue (by 19 \%) and state precision at harvest time (with 50 \% less standard deviation). It also outperformed the deterministic controller for both criteria (15 \% in maximal expected net revenue and 8 \% less standard deviation). A detailed sensitivity analysis showed that such improvements in performance levels are robust, since they hold over large ranges of uncertainty in state dynamics, required harvest precision levels, starting days, and initial weights. The results provide insights in potential of dynamic feedback and risk mitigation strategies for high precision requirements under uncertainty. Although the results should be interpreted with caution, they illustrate the considerable potential benefit for stochastic greenhouse climate control under uncertainty when high precision is required.},
  archiveprefix = {arxiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:02.117Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/GE9FBWUE/van Mourik et al. - 2023 - Plant Performance in Precision Horticulture Optim.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/6XZM2HLQ/2303.html}
}

@article{vanstratenOptimalGreenhouseCultivation2010,
  title = {Optimal {{Greenhouse Cultivation Control}}: {{Survey}} and {{Perspectives}}},
  shorttitle = {Optimal {{Greenhouse Cultivation Control}}},
  author = {Van Straten, G. and Henten, E. J. Van},
  year = {2010},
  journal = {IFAC Proceedings Volumes},
  volume = {43},
  number = {26},
  pages = {18--33},
  issn = {14746670},
  doi = {10.3182/20101206-3-JP-3009.00004},
  urldate = {2023-12-14},
  abstract = {A survey is presented of the literature on greenhouse climate control, positioning the various solutions and paradigms in the framework of optimal control. A separation of timescales allows the separation of the economic optimal control problem of greenhouse cultivation into an off-line problem at the tactical level, and an on-line problem at the operational level. This paradigm is used to classify the literature into three categories: focus on operational control, focus on the tactical level, and truly integrated control. Integrated optimal control warrants the best economical result, and provides a systematic way to design control systems for the innovative greenhouses of the future. Research issues and perspectives are listed as well.},
  langid = {english},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/LGQ2TGEQ/Van Straten and Henten - 2010 - Optimal Greenhouse Cultivation Control Survey and.pdf}
}

@book{vermeulenKwantitatieveInformatieVoor2008,
  title = {Kwantitatieve {{Informatie}} Voor de {{Glastuinbouw}} 2008: {{Groenten}} - {{Snijbloemen}} - {{Potplanten}}},
  shorttitle = {Kwantitatieve {{Informatie}} Voor de {{Glastuinbouw}} 2008},
  author = {Vermeulen, Peter and Glastuinbouw, Wageningen},
  year = {2008},
  month = jan,
  abstract = {Economic and technical key figures Dutch Greenhouse Horticulture},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/7SIDB4IL/Vermeulen and Glastuinbouw - 2008 - Kwantitatieve Informatie voor de Glastuinbouw 2008.pdf}
}

@inproceedings{wangDeepReinforcementLearning2020,
  title = {Deep {{Reinforcement Learning}} for {{Greenhouse Climate Control}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Knowledge Graph}} ({{ICKG}})},
  author = {Wang, Lu and He, Xiaofeng and Luo, Dijun},
  year = {2020},
  month = aug,
  pages = {474--480},
  publisher = {IEEE},
  address = {Nanjing, China},
  doi = {10.1109/ICBK50248.2020.00073},
  urldate = {2023-12-01},
  abstract = {Worldwide, the area of greenhouse production is increasing with the rapid growth of global population and demands for fresh food. However, the greenhouse industry encounters challenges to find automatic control policy. Reinforcement Learning (RL) is a powerful tool in solving the autonomous decision making problems. In this paper, we propose a novel Deep Reinforcement Learning framework for cucumber climate control. Although some machine learning methods have been proposed to address the dynamic climate control problem, these methods have two major issues. First, they only consider the current reward (e.g., the fruit weight of the cucumber). Second, previous study only considers one control variable. However, the growth of crops are impacted by multiple factors synchronously (e.g., CO2 and Temperature).To solve these challenges, we propose a Deep Reinforcement learning based climate control method, which can model future reward explicitly. We further consider the fruit weight and the cost of the planting in order to improve the cumulative fruit weight and reduce the costs.},
  isbn = {978-1-72818-156-1},
  langid = {english},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2023-12-06T09:21:17.066Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/XRAEPNLS/Wang et al. - 2020 - Deep Reinforcement Learning for Greenhouse Climate.pdf}
}

@article{winklerGlobalLandUse2021,
  title = {Global Land Use Changes Are Four Times Greater than Previously Estimated},
  author = {Winkler, Karina and Fuchs, Richard and Rounsevell, Mark and Herold, Martin},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2501},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22702-2},
  urldate = {2023-12-01},
  abstract = {Quantifying the dynamics of land use change is critical in tackling global societal challenges such as food security, climate change, and biodiversity loss. Here we analyse the dynamics of global land use change at an unprecedented spatial resolution by combining multiple open data streams (remote sensing, reconstructions and statistics) to create the HIstoric Land Dynamics Assessment\,+\,(HILDA\,+). We estimate that land use change has affected almost a third (32\%) of the global land area in just six decades (1960-2019) and, thus, is around four times greater in extent than previously estimated from long-term land change assessments. We also identify geographically diverging land use change processes, with afforestation and cropland abandonment in the Global North and deforestation and agricultural expansion in the South. Here, we show that observed phases of accelerating ({\textasciitilde}1960--2005) and decelerating (2006--2019) land use change can be explained by the effects of global trade on agricultural production.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Agriculture,Climate-change mitigation,Environmental impact,Geography},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/JH4PSNQM/Winkler et al. - 2021 - Global land use changes are four times greater tha.pdf}
}

@misc{yoonUnderstandingActorCritic2019,
  title = {Understanding {{Actor Critic Methods}}},
  author = {Yoon, Chris},
  year = {2019},
  month = jul,
  journal = {Medium},
  urldate = {2023-12-13},
  abstract = {Preliminaries},
  howpublished = {https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f},
  langid = {english},
  keywords = {/unread},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/YU5CQE2V/understanding-actor-critic-methods-931b97b6df3f.html}
}

@article{zhangMethodologiesControlStrategies2020,
  title = {Methodologies of Control Strategies for Improving Energy Efficiency in Agricultural Greenhouses},
  author = {Zhang, Shanhong and Guo, Yu and Zhao, Huajian and Wang, Yang and Chow, David and Fang, Yuan},
  year = {2020},
  month = nov,
  journal = {Journal of Cleaner Production},
  volume = {274},
  pages = {122695},
  issn = {09596526},
  doi = {10.1016/j.jclepro.2020.122695},
  urldate = {2023-12-01},
  abstract = {The greenhouse sector accounts for the largest portion of total final energy consumption in agriculture in most countries. One efficient way to minimize the total energy consumption in greenhouses is through the acceptable and efficient control strategy. The control strategy plays a very important role in maintaining comfortable inside climate and reducing energy consumption for the greenhouse, which could effectively adjust the equipment such as the heating/cooling, ventilation, shading system, and coordinate them with low energy operation. The objective of this article is to systematically review the methodologies of control strategies for improving energy efficiency in agricultural greenhouses, particularly for the low energy greenhouses. The methods section, including review methodology and brief methodology description for control strategies in greenhouses, have been first presented. Subsequently, results section introduces the significance of control strategy and types of control strategies in greenhouses; detailed methodologies of greenhouse control strategies including mathematical modelling study; physical experimental study; numerical simulations and parametric sensitivity analysis have been then systematically reviewed. Furthermore, more than 30 parameters affecting greenhouse performance have been analyzed and evaluated. This review could provide a guidance to probe into the advanced control strategies to reduce the energy consumption for the greenhouse and maintain suitable growing environment simultaneously. This work has also demonstrated several control perspectives on the future low energy greenhouse trends.},
  langid = {english},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:29:01.281Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/7AASVN7A/Zhang et al. - 2020 - Methodologies of control strategies for improving .pdf}
}

@article{zhangNetworkArchitectureOptimizing2022,
  title = {Network {{Architecture}} for {{Optimizing Deep Deterministic Policy Gradient Algorithms}}},
  author = {Zhang, Haifei and Xu, Jian and Zhang, Jian and Liu, Quan},
  year = {2022},
  month = nov,
  journal = {Computational Intelligence and Neuroscience},
  volume = {2022},
  pages = {1--10},
  doi = {10.1155/2022/1117781},
  abstract = {The traditional Deep Deterministic Policy Gradient (DDPG) algorithm has been widely used in continuous action spaces, but it still suffers from the problems of easily falling into local optima and large error fluctuations. Aiming at these deficiencies, this paper proposes a dual-actor-dual-critic DDPG algorithm (DN-DDPG). First, on the basis of the original actor-critic network architecture of the algorithm, a critic network is added to assist the training, and the smallest Q value of the two critic networks is taken as the estimated value of the action in each update. Reduce the probability of local optimal phenomenon; then, introduce the idea of dual-actor network to alleviate the underestimation of value generated by dual-evaluator network, and select the action with the greatest value in the two-actor networks to update to stabilize the training of the algorithm process. Finally, the improved method is validated on four continuous action tasks provided by MuJoCo, and the results show that the improved method can reduce the fluctuation range of error and improve the cumulative return compared with the classical algorithm.},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/S2RFNA8S/Zhang et al_2022_Network Architecture for Optimizing Deep Deterministic Policy Gradient.pdf}
}

@misc{zhangRobustModelbasedReinforcement2021,
  title = {Robust {{Model-based Reinforcement Learning}} for {{Autonomous Greenhouse Control}}},
  author = {Zhang, Wanpeng and Cao, Xiaoyan and Yao, Yao and An, Zhicheng and Xiao, Xi and Luo, Dijun},
  year = {2021},
  month = oct,
  number = {arXiv:2108.11645},
  eprint = {2108.11645},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.11645},
  urldate = {2023-12-01},
  abstract = {Due to the high efficiency and less weather dependency, autonomous greenhouses provide an ideal solution to meet the increasing demand for fresh food. However, managers are faced with some challenges in finding appropriate control strategies for crop growth, since the decision space of the greenhouse control problem is an astronomical number. Therefore, an intelligent closed-loop control framework is highly desired to generate an automatic control policy. As a powerful tool for optimal control, reinforcement learning (RL) algorithms can surpass human beings' decision-making and can also be seamlessly integrated into the closed-loop control framework. However, in complex real-world scenarios such as agricultural automation control, where the interaction with the environment is time-consuming and expensive, the application of RL algorithms encounters two main challenges, i.e., sample efficiency and safety. Although model-based RL methods can greatly mitigate the efficiency problem of greenhouse control, the safety problem has not got too much attention. In this paper, we present a model-based robust RL framework for autonomous greenhouse control to meet the sample efficiency and safety challenges. Specifically, our framework introduces an ensemble of environment models to work as a simulator and assist in policy optimization, thereby addressing the low sample efficiency problem. As for the safety concern, we propose a sample dropout module to focus more on worst-case samples, which can help improve the adaptability of the greenhouse planting policy in extreme cases. Experimental results demonstrate that our approach can learn a more effective greenhouse planting policy with better robustness than existing methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2023-12-05T19:19:18.047Z},
  file = {/home/murray/snap/zotero-snap/common/Zotero/storage/5SQP6WGN/Zhang et al. - 2021 - Robust Model-based Reinforcement Learning for Auto.pdf;/home/murray/snap/zotero-snap/common/Zotero/storage/5IGCU44T/2108.html}
}
