\contentsline {chapter}{Preface}{i}{chapter*.1}%
\contentsline {chapter}{Summary}{ii}{chapter*.2}%
\contentsline {chapter}{Nomenclature}{v}{chapter*.4}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Problem Statement}{2}{section.1.1}%
\contentsline {section}{\numberline {1.2}Thesis Contribution}{3}{section.1.2}%
\contentsline {section}{\numberline {1.3}Recent and Related Developments}{3}{section.1.3}%
\contentsline {section}{\numberline {1.4}Thesis Outline}{3}{section.1.4}%
\contentsline {chapter}{\numberline {2}Background}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Greenhouse Model}{5}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Model Description}{6}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Uncertainty}{7}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Model State Equations}{7}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Optimization Goal}{10}{subsection.2.1.4}%
\contentsline {section}{\numberline {2.2}Reinforcement Learning}{10}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Why RL for Greenhouse Control?}{10}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}The RL problem}{11}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Q learning}{13}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Policy Optimization}{13}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Actor-Critic}{14}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}SAC}{14}{subsection.2.2.6}%
\contentsline {section}{\numberline {2.3}MPC}{14}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Why MPC for Greenhouse Control?}{14}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}The General MPC problem}{14}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}EMPC}{16}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}RL and MPC in tandem}{16}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}The Optimality}{17}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}Computational Effort}{17}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}The Prediction Horizon}{18}{subsection.2.4.3}%
\contentsline {chapter}{\numberline {3}Reinforcement Learning Setup}{19}{chapter.3}%
\contentsline {section}{\numberline {3.1}Environment Description}{19}{section.3.1}%
\contentsline {paragraph}{Observation Space}{19}{section.3.1}%
\contentsline {paragraph}{Action Space}{20}{equation.3.1.3}%
\contentsline {paragraph}{Initial Conditions}{20}{equation.3.1.3}%
\contentsline {paragraph}{Reward Function}{20}{equation.3.1.4}%
\contentsline {paragraph}{Uncertainty}{21}{table.caption.9}%
\contentsline {section}{\numberline {3.2}Experimental Setup}{21}{section.3.2}%
\contentsline {paragraph}{Weather Data}{22}{equation.3.2.8}%
\contentsline {paragraph}{Deterministic and Stochastic Case}{22}{figure.caption.10}%
\contentsline {paragraph}{Performance Metrics}{23}{figure.caption.10}%
\contentsline {section}{\numberline {3.3}Hyper-parameter Tuning}{23}{section.3.3}%
\contentsline {paragraph}{Activation Function}{23}{table.caption.11}%
\contentsline {paragraph}{Discount Factor}{23}{table.caption.11}%
\contentsline {section}{\numberline {3.4}Deterministic Results}{23}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Discount Factor}{24}{subsection.3.4.1}%
\contentsline {paragraph}{Performance}{24}{figure.caption.12}%
\contentsline {paragraph}{Value Function Approximation}{24}{figure.caption.12}%
\contentsline {subsection}{\numberline {3.4.2}Activation Function}{25}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Observation Tuples}{26}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Final Results and Conclusion}{26}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Stochastic Results}{28}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Conclusion}{29}{subsection.3.5.1}%
\contentsline {section}{\numberline {3.6}Trained Value Function}{29}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Temporal Difference Learning}{29}{subsection.3.6.1}%
\contentsline {paragraph}{Obtaining Data}{29}{subsection.3.6.1}%
\contentsline {paragraph}{Training}{30}{table.caption.21}%
\contentsline {subsection}{\numberline {3.6.2}Expected Return Learning}{30}{subsection.3.6.2}%
\contentsline {paragraph}{Obtaining Data}{30}{subsection.3.6.2}%
\contentsline {paragraph}{Training}{31}{equation.3.6.13}%
\contentsline {paragraph}{Experimental Setup}{31}{equation.3.6.14}%
\contentsline {subsection}{\numberline {3.6.3}Results}{33}{subsection.3.6.3}%
\contentsline {section}{\numberline {3.7}Conclusion}{36}{section.3.7}%
\contentsline {chapter}{\numberline {4}Model Predictive Control Setup}{38}{chapter.4}%
\contentsline {section}{\numberline {4.1}Greenhouse MPC problem formulation}{38}{section.4.1}%
\contentsline {paragraph}{Experimental Setup}{39}{equation.4.1.2}%
\contentsline {section}{\numberline {4.2}Deterministic Results}{39}{section.4.2}%
\contentsline {section}{\numberline {4.3}Stochastic Results}{41}{section.4.3}%
\contentsline {section}{\numberline {4.4}Conclusion}{43}{section.4.4}%
\contentsline {chapter}{\numberline {5}Deterministic RL-MPC}{44}{chapter.5}%
\contentsline {section}{\numberline {5.1}Implementation}{44}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}RL-MPC problem formulations}{44}{subsection.5.1.1}%
\contentsline {paragraph}{Implementation 1}{44}{subsection.5.1.1}%
\contentsline {paragraph}{Implementation 2}{45}{equation.5.1.3}%
\contentsline {paragraph}{Implementation 3}{45}{equation.5.1.5}%
\contentsline {paragraph}{Implementation 4}{46}{equation.5.1.6}%
\contentsline {paragraph}{Implementation 5}{46}{equation.5.1.7}%
\contentsline {paragraph}{Implementation 6}{47}{equation.5.1.8k}%
\contentsline {subsection}{\numberline {5.1.2}Initial RL and MPC performance}{47}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Results - Implementations 1}{47}{section.5.2}%
\contentsline {section}{\numberline {5.3}Results - Implementations 2}{48}{section.5.3}%
\contentsline {section}{\numberline {5.4}Results - Implementation 3}{49}{section.5.4}%
\contentsline {section}{\numberline {5.5}Results - Implementation 4}{50}{section.5.5}%
\contentsline {section}{\numberline {5.6}Results - Implementation 5 and 6}{51}{section.5.6}%
\contentsline {section}{\numberline {5.7}Final Result and Conclusion}{52}{section.5.7}%
\contentsline {chapter}{\numberline {6}Stochastic RL-MPC}{54}{chapter.6}%
\contentsline {section}{\numberline {6.1}Initial RL and MPC Performance}{54}{section.6.1}%
\contentsline {section}{\numberline {6.2}Results - VF and Terminal Region}{56}{section.6.2}%
\contentsline {section}{\numberline {6.3}Conclusion}{57}{section.6.3}%
\contentsline {chapter}{\numberline {7}Computational Speed Up of RL-MPC}{59}{chapter.7}%
\contentsline {section}{\numberline {7.1}Reducing Neurons and Hidden Layers}{59}{section.7.1}%
\contentsline {section}{\numberline {7.2}Taylor Approximation}{62}{section.7.2}%
\contentsline {section}{\numberline {7.3}Combined}{62}{section.7.3}%
\contentsline {section}{\numberline {7.4}Discussion and Conclusion}{63}{section.7.4}%
\contentsline {chapter}{\numberline {8}Discussion and Conclusion}{65}{chapter.8}%
\contentsline {section}{\numberline {8.1}Discussion}{65}{section.8.1}%
\contentsline {section}{\numberline {8.2}Conclusion}{65}{section.8.2}%
\contentsline {section}{\numberline {8.3}Recommendations \& Future Work}{65}{section.8.3}%
\contentsline {chapter}{References}{66}{chapter*.52}%
\contentsline {chapter}{\numberline {A}RL \& RL Training }{70}{appendix.A}%
\contentsline {section}{\numberline {A.1}Selection of RL Algorithm}{70}{section.A.1}%
\contentsline {section}{\numberline {A.2}Agent Training}{70}{section.A.2}%
\contentsline {section}{\numberline {A.3}Value Function Training}{70}{section.A.3}%
\contentsline {chapter}{\numberline {B}RL-MPC}{71}{appendix.B}%
\contentsfinish 
