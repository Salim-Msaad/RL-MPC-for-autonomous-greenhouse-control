\chapter{Discussion and Conclusion}
\label{chapter:conclusion}

In this thesis, an RL-MPC algorithm was proposed in order to assess the performance gains as compared to the standalone RL and MPC controllers on a greenhouse environment. Two similar performing policies were created with RL and MPC respectively both for a deterministic and stochastic environment. The RL-MPC algorithm was developed in the deterministic environment by testing various combinations of the two controllers. The resulting RL-MPC was tested on a deterministic and stochastic setting. Lastly, modifications were investigated that could reduce the computational demand of the RL-MPC controller. This chapter assess the research questions asked in the beginning of this thesis and draws final conclusions of this RL-MPC controllers, with recommendations for future research . 

\section{Conclusion}
This thesis aimed to answer two research questions, namely:

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms in a deterministic environment?} 
	\\The results presented in this thesis clearly demonstrate that the RL-MPC algorithm greatly enhances economic performance in comparison to both the standalone RL and MPC algorithms. The MPC policy outperformed the policy generated from RL in this particular setting, and its superiority was further enhanced with an increase in the prediction horizon.The RL-MPC algorithm demonstrated superior economic performance across all prediction horizons of the MPC, with particularly notable results at shorter prediction horizons.
\end{itemize}

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms in a stochastic environment?} 
	\\Evidence demonstrated that RL exhibits superior ability to manage uncertainty when trained using the corresponding stochastic data. The MPC was not adjusted to address this uncertainty, resulting in the RL outperforming the MPC, particularly under conditions of high uncertainty.The study demonstrated that the developed RL-MPC algorithm can achieve superior performance compared to both RL and MPC controllers under conditions of low uncertainty, similar to the deterministic scenario. Nevertheless, when faced with high levels of uncertainty, the performance of RL-MPC is hindered by the subpar performance of MPC in the stochastic setting. While it continues to surpass MPC in terms of performance, it faces challenges in achieving the same level of performance as RL in situations with high levels of uncertainty. Additionally, increasing the prediction horizon for the RL-MPC scheme may have negative consequences. This is because a longer prediction horizon causes it to behave more similarly to the MPC, which is undesirable in situations with high uncertainty.
\end{itemize}

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{What modifications and/or approximations can be employed to reduce the computational time of the RL-MPC algorithm?}
	\\  Additional modifications can be made to reduce the computational burden of the RL-MPC algorithm. These modifications include training a simpler neural network with less neurons and hidden layers, as well as using a 1st and 2nd order Taylor approximation of the neural network around the terminal guess. It was shown that the computational demand and performance is dependent on the size and complexity of the neural network, however no direct relationship was noticed between reduced computational burden and complexity of the neural network. However it was found that a simpler neural network did result in decreased computational time with no impact on performance. Moreover, it was shown that using both a 1st and 2nd order Taylor approximation noticeably decreased computational burden while resulting in very little to no performance losses. The combination of both techniques yielded a RL-MPC controller that exhibited computational times similar to those of MPC for short prediction horizons, and faster computational times for longer prediction horizons. Furthermore, the overall computational times of the RL-MPC new controller were faster than those of the original RL-MPC, while preserving its original performance. 
\end{itemize}

The superiority of the RL-MPC controller over its standalone counterparts is evident, but the most significant enhancement in performance is achieved by incorporating a terminal constraint region. While incorporating a suitable value function does enhance performance, it also substantially increases the computational demand. The terminal region constraint enhances performance and decreases the computational load. Even so, it should be acknowledged that the greenhouse model used in this thesis is relatively uncomplicated. It is probable that including this value function may produce greater benefits in larger and more complex systems. 

The computational burden of the RL-MPC algorithm can be similarly argued. Despite the RL-MPC algorithm demonstrating relatively fast computational speeds, even outperforming MPC at long prediction horizons, one might question the need to introduce modifications to further enhance its speed. The RL-MPC algorithm provides real-time control for a relatively simple non-linear system. However, when dealing with much larger systems, achieving real-time control may become difficult and essential for performance. Moreover, the modifications made are simple to incorporate and have no impact on the performance, thus there is no reason to not implement them.

It was noted that in this thesis, training an RL policy was significantly more difficult and time consuming than generating a policy through MPC, and thus in a real world application an MPC would likely be used. However results show even an okay RL policy can be integrated integrated into MPC to produce an RL-MPC algorithm that surpasses MPC's performance for all prediction horizons, showcasing the importance of integrating the two methods.





\section{Recommendations \& Future Work}
A further study into the hyperparameters of the RL agent may be necessary to generate an RL policy capable of outperforming the MPC, even in a deterministic environment.  Since the value function is learned separately to the training of the RL agent, it is possible to use other RL algorithms such as PPO, TRPO or other policy optimization algorithms to try increase the policies performance. Moreover, since the problem is episodic it would be desirable to find a set of hyperparameters that learns a very good policy for a discount factor of one, however since the value function can be learned separately, this may not be necessary  and thus lower discount factors can be used in order to stabilize the RL training. In addition, it is important to note that a neural network is not the only non-linear function approximator available, and there may be other forms that are more suitable for effectively learning a value function in relation to the dynamics of the greenhouse. Additional types of non-linear function approximators include radial basis functions or gaussian processes. However, an in-depth investigation is necessary to determine which is most suitable.

It is also noted that soft constraints on the states are used for the MPC formulation. It is possible to implement hard constraints to ensure that no constraint violations (or very little) occur. This takes away one of MPC's strengths and a possible recommendation would be to compare the RL-MPC algorithm with MPC under hard constraints to determine whether the RL may still provide useful knowledge to the MPC. However it makes it difficult to compare to the RL's policy since the objective function changes. Moreover, a more accurate uncertainty model can be used in order to better predict the controllers behaviour in a realistic environment. Additionally, the studies performed in this thesis can be conducted on a more complex environment too such as in \cite{GreenLightOpenSource2020}. 

RL-MPC 6 is also a promising combination of RL and MPC. Guesses and terminal region constraints can be provided by multiple agents and/or other policies generated by other controllers and the best one selected by a common value function. Determining the policy on which this value function is trained on can also be investigated to ensure the best performance of the resulting RL-MPC controller. 

In the case of the stochastic environment, RL outperforms MPC since it is given information about the uncertainty present in the environment, whereas MPC is not. This was done to determine whether RL is able to transfer its knowledge about this uncertainty into the MPC's framework through a terminal region constraint and a value function. However, estimation techniques such as the Moving Horizon Estimator (MHE) is commonly used in MPC to mitigate the noise on the output. Moreover stochastic MPC controllers such as in \cite{boersmaRobustSamplebasedModel2022} targets parametric uncertainty as used in this thesis. One can expect the performance of MPC to significantly increase in a stochastic environment if these techniques are used. Therefore a study may be performed whereby the RL-MPC framework also incorporates these and the resulting performance gains examined. 

Finally, in order to implement this in a real greenhouse, a theoretical foundation must be established to ensure that the combination of RL-MPC will yield performance guarantees. This thesis serves as an initial investigation into the performance gains expected and the resulting benefits of the RL-MPC.
