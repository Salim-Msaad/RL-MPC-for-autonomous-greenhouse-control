\chapter{Discussion and Conclusion}
\label{chapter:conclusion}

This thesis proposed an RL-MPC algorithm to assess the performance gains compared to the standalone RL and MPC controllers on a greenhouse environment. Two similar performing policies were created with RL and MPC for a deterministic and stochastic environment. The RL-MPC algorithm was developed in the deterministic environment by testing various combinations of the two controllers to determine whether RL is able to provide the RL-MPC useful information in deciding control inputs. The resulting RL-MPC was tested on a stochastic environment to determine whether RL can impart knowledge of the uncertainty to the RL-MPC framework. Lastly, modifications were investigated that could reduce the computational demand of the RL-MPC controller. This chapter assessed the research questions asked at the beginning of this thesis and drew final conclusions about the RL-MPC controllers, with recommendations for future research.

\section{Conclusion}
This thesis aimed to answer two research questions, namely:

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms?} 
	
	\begin{itemize}
		\item \textit{Deterministic Environment:} 
		\\The results presented in this thesis demonstrate that the RL-MPC algorithm greatly enhances economic performance compared to the standalone RL and MPC algorithms. The MPC policy outperformed the policy generated from RL in this particular setting, and its superiority was further enhanced with an increase in the prediction horizon. While the MPC's performance could be deemed satisfactory, the RL-MPC algorithm demonstrated superior economic performance across all prediction horizons of the MPC, with particularly notable results at shorter prediction horizons. Results show a $24\$$ increase in performance over MPC and a $5\%$ over RL at a 1 hour prediction horizon with greater performance over RL over and less performance over MPC at longer prediction horizons. This concludes that even a worse-performing policy (in this case, the RL agent) can provide useful information to an MPC through terminal constraints generated by the actor and a value function as a cost function.
		
		\item \textit{Stochastic Environment:} 
		\\	
		Evidence demonstrated that RL exhibits a superior ability to manage uncertainty when trained with corresponding stochastic data compared to MPC. However, since the MPC was not adjusted to address this uncertainty, RL outperformed MPC, especially under high uncertainty conditions. This thesis aimed to demonstrated that RL can transfer knowledge of this uncertainty to MPC through the development of the RL-MPC framework. The developed RL-MPC algorithm was shown to achieve superior performance compared to both RL and MPC controllers under low uncertainty, similar to the deterministic scenario. Although RL-MPC's performance is hindered by the subpar performance of MPC in high uncertainty settings, it remains more robust than MPC with a slower decline in performance as uncertainty is increased. While RL-MPC continues to surpass MPC in performance, it faces challenges in achieving the same level of performance as RL in situations with high levels of uncertainty.
	\end{itemize}
	
\end{itemize}


\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{What modifications and/or approximations can be employed to reduce the computational time of the RL-MPC algorithm?}
	\\Additional modifications can be made to reduce the computational burden of the RL-MPC algorithm. These modifications include training a simpler neural network with fewer neurons and hidden layers and using a first- and second-order Taylor approximation of the neural network around the terminal guess. It was shown that the computational demand and performance depend on the size and complexity of the neural network; however, no direct relationship was noticed between reduced computational burden and size of the neural network. However, it was found that it is possible to use a  simpler neural network for decreased computational times with no impact on performance. Moreover, it was shown that using both a first- and second-order Taylor approximation noticeably decreased computational burden while resulting in very little to no performance losses. The combination of both techniques yielded an RL-MPC controller that exhibited computational times significantly faster than those of the original RL-MPC while preserving its original performance. Furthermore, the achieved computational times were similar to those of MPC for short prediction horizons and faster for longer prediction horizons. 
\end{itemize}

The superiority of the RL-MPC controller over its standalone counterparts is evident, with the most significant performance enhancement achieved by incorporating a terminal region constraint. While incorporating a suitable value function does enhance performance, it also substantially increases computational demand. In contrast, the terminal region constraint not only enhances performance but also decreases computational load. It should be acknowledged that the greenhouse model used in this thesis is relatively uncomplicated. Including the value function may produce more significant benefits in larger and more complex systems.

The computational burden of the RL-MPC algorithm can be similarly argued. Despite the RL-MPC algorithm demonstrating relatively fast computational speeds, even outperforming MPC at long prediction horizons, one might question the need for modifications to enhance its speed further. The RL-MPC algorithm provides real-time control for a relatively simple non-linear system. However, achieving real-time control for much larger systems may become challenging and essential for performance. Moreover, the modifications are simple to incorporate and do not impact performance, so there is no reason not to implement them.

It was noted in this thesis that training an RL policy was significantly more difficult and time-consuming than generating a policy using MPC. Thus, MPC would likely be preferred in real-world applications, if a model is readily available. However, results show that even a suboptimal RL policy can be integrated into MPC to create an RL-MPC algorithm that surpasses MPC's performance and increases robustness to uncertainty for all prediction horizons. This highlights the importance of integrating the two methods.


\section{Recommendations and Future Work}

\begin{itemize}
	\item \textit{Improvements to the RL algorithm}
	\\Further study into the hyper-parameters of the RL agent may be necessary to generate an RL policy capable of outperforming MPC in a deterministic environment. This could help determine how an increase in RL policy performance can affect the RL-MPC performance.  Moreover, different RL algoritms such as PPO, TRPO, or other advanced optimization algorithms  can be used to improve the policy's performance. Furthermore, in learning a value function, it is important to note that neural networks are not the only non-linear function approximators available. Other forms, such as radial basis functions or Gaussian processes, may be more suitable for effectively learning a value function for the dynamics of the greenhouse and may be more stable (i.e. display a lower degree of non-linearity) when implementing it into the RL-MPC framework. An in-depth investigation is necessary to determine which function approximator is most suitable.
	
	\item \textit{Improvements to the MPC formulation}
	\\In the stochastic environment, RL outperforms MPC because it is provided with information about the uncertainty present, whereas MPC is not. This was done to assess whether RL can transfer its knowledge of this uncertainty into the RL-MPC framework through a terminal region constraint and a value function. However, estimation techniques such as Moving Horizon Estimation (MHE) are commonly used in MPC to mitigate output noise. Additionally, stochastic MPC controllers, like those addressing parametric uncertainty, can significantly enhance MPC performance in a stochastic environment. Therefore, a study incorporating these techniques into the RL-MPC framework should be performed to examine potential performance gains and improved robustness.
	
	Finally, it was demonstrated that the performance degradation of the MPC and RL-MPC controllers in a stochastic environment was primarily due to the increase in constraint violations. Since soft constraints were imposed in this thesis, it may be beneficial to impose hard constraints and investigate their impact on both MPC and RL-MPC. This would help determine whether RL can continue to provide valuable information to RL-MPC for performance improvements.
	
	
	\item \textit{Improvements to the model}
	\\A more accurate uncertainty model could better predict the controllers' behavior in a realistic environment by accounting for uncertainty in energy costs, lettuce prices, and weather. Additionally, the studies performed in this thesis could be conducted in a more complex environment, such as in \citet{GreenLightOpenSource2020}, to better determine the real-world applicability of RL-MPC. Further research could involve training an RL policy on the more complex model and implementing RL-MPC on the simplified model, as this would likely be done in practice. Moreover, RL and MPC could optimize different objective functions—for example, RL could optimize for sparse rewards while MPC optimizes for stage costs—and the effect of combining these approaches should be examined.
	
	\item \textit{RL-MPC 6}
	\\RL-MPC 6 is also a promising combination of RL and MPC. Guesses and terminal region constraints can be provided by multiple agents and/or other policies generated by other controllers, and the best one is selected by a common value function. Determining the policy on which this value function is trained could also be investigated to ensure the best performance of the resulting RL-MPC controller.
	
	\item \textit{Theoretical Foundation}
	\\Finally, a theoretical foundation must be established to ensure that the combination of RL and MPC yields performance guarantees. This thesis serves as an initial investigation into the expected performance gains and benefits of RL-MPC.
\end{itemize}

Finally, all written code, generated results and used data can be found here:\\
\href{https://github.com/mharraway/RL-MPC-for-autonomous-greenhouse-control}{\textcolor{blue}{\underline{RL-MPC for Autonomous Greenhouse Control - Github}}}







