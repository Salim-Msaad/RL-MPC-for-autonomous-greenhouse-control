\chapter{Introduction}
\label{chapter:introduction, written in the present tense}
The world population is set to increase to a staggering 10 billion people in the year 2050 \cite{blazhevskaGrowingSlowerPace2019}, substantially increasing food demand. Currently, 800 million people are chronically hungry, with 2 billion people suffering from micronutrient deficiencies \cite{faoFutureFoodAgriculture2017}. The situation is compounded by the anticipated rise in food demand, which is expected to increase from 30\% to 62\% between 2010 and 2050, resulting in 30\% of the population being at risk of hunger \cite{vandijkMetaanalysisProjectedGlobal2021}. Therefore, there is a pressing need to enhance food production by at least 70\% \cite{nishatGreenDealGreenhouse2020}. Despite significant investments in increasing food productivity, major difficulties such as food losses, waste, and climate change continue to persist\cite{faoFutureFoodAgriculture2017}. The agriculture space has drastically increased to meet these food demands \cite{winklerGlobalLandUse2021}; however, this increase means the sector accounts for almost 15\% of the worldâ€™s energy and more than 70\% of water consumption \cite{nishatGreenDealGreenhouse2020}. Although greenhouses have been extensively used to combat these problems and have been shown to reduce the environmental burden compared to typical open-land production \cite{munozComparingEnvironmentalImpacts2008}, they still consume about ten times more energy than traditional farming \cite{nishatGreenDealGreenhouse2020} due to the operating costs of a greenhouse. Moreover, growers are under increasing pressure to adopt more effective growing methods with the soaring operating energy costs associated with greenhouses and a global trend indicating an increase in gas and electricity prices \cite{alvarezWhatSoaringEnergy2021}. As a result, agreement policies have been signed to reduce the $C0_2$ emissions of these greenhouses to an acceptable level \cite{breukersPowerDutchGreenhouse}. \\

Smart greenhouses are designed to enhance crop yield per hectare using climate-controlled environments \cite{morcegoReinforcementLearningModel2023}. These smart greenhouses are essential in combating the degrading effects of climate change on crop quality and yield. However, efficiently maintaining such an environment requires advanced control methods, especially for economic profit. These control methods must be able to adjust factors such as temperature, humidity, lighting, and C02 levels to accommodate ideal conditions for crop growth \cite{devopsGreenhouseClimateControl2021} whilst keeping energy costs at a minimum. Growing crops in a controlled environment can ensure the extension of their growing season and protection from outside temperature and weather changes. The advent of smart and advanced greenhouses necessitates skilled labour for operation, yet there is a scarcity of qualified personnel \cite{rusnakWhatCurrentState2018}. Coupled with the escalating labour costs, the move to autonomous greenhouses is attractive.\\


 It is common  to use controllers, such as PID, to control actuators, adjusting conditions based on set points manually specified by the grower \cite{zhangMethodologiesControlStrategies2020}. Although these techniques exist, often called automatic greenhouses, growing crops still relies heavily on the grower's expertise. Due to the numerous factors that affect crop growth, determining the optimal set-points becomes highly complex. The complexity is further intensified by the fact that the development of a plant is highly influenced by the control inputs taken days or even weeks in advance. Therefore, it is crucial to choose control inputs strategically in order to maximise future rewards such as economic profit. Control strategies such as reinforcement learning (RL) and model predictive control (MPC) can be implemented \cite{zhangMethodologiesControlStrategies2020} to achieve this. Both strategies provide optimal control to pursue the same goal, optimising a reward/cost function. This cost function gives an indication of the quality of a particular action or decision, and in this thesis, it determines the extent to which an action is favourable or unfavourable in relation to its economic benefit. Both control schemes have advantages and disadvantages. However, there is a notable similarity between the two, suggesting that combining them could lead to a more efficient solution for autonomous greenhouse control.

Although  RL and MPC are both used for optimal control, RL focuses on learning from interactions with an environment to maximise long-term rewards. At the same time, MPC leverages model-based predictions and optimisation to determine optimal control actions over a finite time horizon. Several methods seek to integrate the two control schemes, shedding light on the strengths and weaknesses of the resulting controller concerning its specific application. There are two main methods for combining RL and MPC: using MPC as the function approximator for the RL agent, or modifying the MPC's objective function and terminal constraints to incorporate RL knowledge. The latter approach allows the learned knowledge from RL to guide and improve the performance of the MPC controller, potentially leading to more effective and adaptive control strategies. This approach will be investigated in this thesis.

\section{Problem Statement}

The ability of RL to learn from interactions with a highly complex environment, even in the presence of uncertainty, leads to its ability to learn a policy that aims to be optimal within that particular environment. However, the quality of control is strongly influenced by the training of the RL algorithm. Moreover, RL does not directly impose state constraints. While it is possible to indirectly incorporate these constraints  in the reward function with penalty functions, such an approach does not ensure that the optimal policy obtained will always adhere to these constraints. Moreover, the value function obtained using RL is only an approximation. This becomes an issue when the problem at hand is safety critical. Finally, RL faces a limitation in online flexibility due to its reliance on a feed-forward pass for policy evaluation.

In contrast to the policy obtained using RL, the quality of the policy obtained using MPC is subject to the accuracy of the prediction model, which is often simplified to reduce the computational burden, especially with non-convex dynamics, which may lead to sub-optimal control. Nevertheless, MPC is recognised for its easier implementation, superior constraint handling, and sample-efficiency as compared to RL. However, model mismatch and unforeseen uncertainties in forecasted disturbances can lead to a significant deterioration in the MPC's performance. 

Arguably, the most crucial characteristic of both control strategies lies in their respective prediction horizon. Both controllers use future information to determine the optimal control actions. MPC achieves this by employing explicit optimisation over a finite prediction horizon, while RL explores and interacts with its environment to optimise for immediate and long-term rewards. Hence, a shortcoming of MPC is the finite prediction horizon. This limitation becomes even more pronounced when dealing with sparse rewards and slow system dynamics. In such cases, actions taken at the current time step may only yield rewards past the prediction horizon, causing the MPC controller to be myopic. It is possible to counteract this drawback with an extended prediction horizon, but to the detriment of simplicity and computational efficiency of the controller. Moreover, with no terminal constraint or cost function, there are no closed-loop performance guarantees for an MPC that optimises for economic benefit (EMPC). Importantly, two main methods exist to assure closed-loop performance: to use a sufficiently large horizon or the application of an appropriate terminal constraint and terminal cost function.

While MPC's prediction horizon is finite, RL uses a discounted infinite prediction horizon, allowing the RL agent to weigh the benefit of future rewards against current actions. The exploration present in RL will enable it to discover patterns and optimal policies that a typical MPC might not be able to achieve, particularly in environments characterised by non-linear dynamics. A synergistic approach to combining the two control strategies would be to have an MPC controller optimise a short prediction horizon while propagating future information provided by RL. This integration of the two controllers is further justified in the context of greenhouse dynamics, where actions executed at the current time step may result in rewards that manifest over the long term. The terminal  cost function in the MPC formulation, which must encapsulate information beyond the prediction horizon, underscores the clear connection with the value function obtained through RL to supply the information required for achieving the desired system performance. Notably, this may also be viewed as unrolling the value function and performing an N-step look-ahead minimisation on the resulting equation \cite{bertsekasNewtonMethodReinforcement2022}. This is a common application of improving on the policy that generated the value function by value iterations. Nevertheless, incorporating the learned value function (typically a neural network) in the MPC's formulation introduces additional complexity to the problem due to its highly non-linear nature. Consequently, a naive implementation of this control strategy could have severe detrimental effects. Alternatively, the RL policy can also provide the MPC with a terminal region or constraint to guide it towards a control policy that is closer to the optimal solution more effectively. Ultimately, the myopic nature of MPC for optimising economic benefit can be counteracted by providing knowledge of the future, through a cost function and terminal constraints as provided my RL. This approach enables the use of a short prediction horizon to meet computational requirements, while avoiding myopic decision-making. Therefore, in the development of the RL-MPC algorithm, for greenhouse control, the following research questions will be answered:

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms?} \begin{itemize}
		\item \textit{In a deterministic environment}
		\item \textit{In a stochastic environment}
	\end{itemize}
	\item \textit{What modifications and/or approximations can be employed to reduce the computational time of the RL-MPC algorithm?}
\end{itemize}




\section{Thesis Contribution}

Answering the above questions leads to several contributions in the field. Notably, among the existing works, there is a scarcity of algorithms that independently train a RL agent and subsequently employ MPC for the N-step look-ahead minimization. Specifically, none of the algorithms identified in the related literature use MPC for the specified N-step look-ahead minimisation on the pre-trained value function during online play, particularly in scenarios involving continuous state and action spaces. Lastly, it is worth noting that all the proposed RL-MPC algorithms in recent and related developments are applied in the context of set-point regulation or trajectory tracking and are not explicitly geared to maximising economic benefits.
Therefore, the main contribution of this thesis will involve developing and implementing a framework that incorporates a learned value function and terminal constraints provided by RL into a economic non-linear model predictive controller (ENMPC) for a continuous state and action space and making such an algorithm generate on-time control actions. A greenhouse is a suitable environment and system for developing such an algorithm due to its non-linear dynamics, continuous state and action spaces and slow dynamics that result in sparse rewards. Importantly, the objective of a greenhouse is to maximise profits from the cultivation and sale of crops. In the context of greenhouse operations, it is noteworthy that the primary aim is to maximise profits. This goal typically does not entail tracking specific setpoints for crop growth.


\section{Recent and Related Developments}
Various literature explores the implementation of RL and MPC  such as \cite{arroyoReinforcedModelPredictive2022,beckenbachAddressingInfinitehorizonOptimization2018,lubarsCombiningReinforcementLearning2021,lubbersAutonomousGreenhouseClimate2023,sikchiLearningOffPolicyOnline2021,} whereas  \cite{beckenbachAddressingInfinitehorizonOptimization2018,bertsekasLessonsAlphaZeroOptimal,bertsekasNewtonMethodReinforcement2022,linReinforcementLearningBasedModel2023} examine the theoretical background of such a controller.
Most notably, the works in \citet{sikchiLearningOffPolicyOnline2021,arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,bertsekasLessonsAlphaZeroOptimal} are the most similar to what is proposed in this thesis, whereby an RL agent is trained, and the resulting learned value function is unrolled with the Bellman equation. The optimal action is computed using an N-step look-ahead minimisation on the unrolled equation during online play. Works from \cite{arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,beckenbachAddressingInfinitehorizonOptimization2018} are the only ones incorporating an MPC for the N-step look-ahead minimisation. These authors propose that the reinforcement learning process could be assisted with MPC and show the implementation of such an algorithm, but doing this might impact the agents exploratory nature. Nonetheless, in all cases, the various RL-MPC algorithms have shown to outperform its RL counterpart and in most cases, its MPC counterpart to. Furthermore, a concise explanation will be provided to differentiate this thesis from the concept of employing MPC as a function approximator, which is a common application of a RL-MPC algorithm. This aims to clarify what the thesis does not focus on. As discussed in \autoref{section:RL}, the DQN is usually implemented as a neural network and is parameterized by a ${\theta}$. The approximated value of a state $s$, parameterised by a weight vector  $\theta$ is then given as $ \hat{v}(s,\theta) = v^*(s)$ where $\hat{v}$ \cite{lubbersAutonomousGreenhouseClimate2023}. However, it is possible to use an MPC scheme instead of a neural network to facilitate parametrisation for approximating the value function and policy. This is the goal of using MPC as a function approximator for RL. This thesis does not explore this concept. Instead, it explores the design and implementation of an (E)NMPC algorithm that uses the value function learned by RL in its optimal control problem formulation to propagate information beyond the prediction horizon. 


\section{Thesis Outline}
This thesis begins by discussing the background knowledge in \cref{chapter:Background}.  This chapter presents the greenhouse model that will be used and the rationale behind its selection. It also outlines the optimisation objective of the RL, MPC, and combined RL-MPC controllers. In addition, this chapter will present the concepts of RL, MPC, and RL-MPC. \cref{chapter:RL} explores establishing and training the RL agent in two environments: one without uncertainty (the nominal case) and one with uncertainty (the stochastic case). The chapter also examines the performance of the resulting agent in its respective environments. Moreover, this chapter explores the process of training accurate value functions using a fixed RL policy and discusses the obtained outcomes. \cref{chapter:MPC} entails the formulation of the optimal control problem for the MPC and discusses the performance of the MPC in an environment with and without uncertainty. \cref{chapter:deterministic_RL_MPC} examines the various implementations of the RL-MPC controller and evaluates their performance on the nominal case. \cref{chapter:stochastic_RL_MPC} applies the best RL-MPC implementations from \cref{chapter:deterministic_RL_MPC} to a stochastic environment and discusses the findings.\cref{chapter:speed-up}, explores how the RL-MPC implementation can be optimised for computational efficiency and emphasises the significance of these optimisations. The thesis concludes with a presentation of the conclusion and future research in \cref{chapter:conclusion}, and a draft research paper provided in Appendix C.

