\chapter{Introduction}
\label{chapter:introduction, written in the present tense}

The world population is set to increase to a staggering 10 billion people in the year 2050 \cite{blazhevskaGrowingSlowerPace2019}, increasing food demand substantially. Currently, 800 million people are chronically hungry, with 2 billion people suffering from micronutrient deficiencies \cite{faoFutureFoodAgriculture2017}.The situation is compounded by the anticipated rise in food demand, which is expected to increase from 30\% to 62\% between the years 2010 and 2050, resulting in 30\% of the population being at risk of hunger \cite{vandijkMetaanalysisProjectedGlobal2021}. As such, there is a pressing need to enhance food production by at least 70\% \cite{nishatGreenDealGreenhouse2020}. Although large investments have been made to increase food productivity, food losses, waste, and climate change continue to serve as significant constraints \cite{faoFutureFoodAgriculture2017}. To meet these food demands, agriculture space has drastically increased \cite{winklerGlobalLandUse2021}; however, an increase in agriculture land space has led the sector to account for almost 15\% of the world's energy consumption while also accounting for more than 70\% of water consumption \cite{nishatGreenDealGreenhouse2020}. The need for more efficient use of space and resources is clear to increase food demands while limiting space and resource usage. Although greenhouses have been extensively used to combat these problems and have been shown to reduce the environmental burden as compared to typical open-land production \cite{munozComparingEnvironmentalImpacts2008}, they still require about 10 times more energy consumption compared to traditional farming \cite{nishatGreenDealGreenhouse2020}. This increase in energy is owed to the drastic increase in greenhouse operating costs.  Moreover, with the soaring operating energy costs associated with greenhouses and a global trend indicating an increase in gas and electricity prices \cite{alvarezWhatSoaringEnergy2021}, growers are under increasing pressure to adopt more effective growing methods. As a result, agreement policies have been signed to reduce the $CO_2$ emissions of these greenhouses to an acceptable level \cite{breukersPowerDutchGreenhouse}. \\

These structures are designed to enhance crop yield per hectare by utilizing climate-controlled environments \cite{morcegoReinforcementLearningModel2023}. These smart greenhouses are essential in combating the degrading effects of climate change on crop quality and yield. However, efficiently maintaining such an environment, especially for economic profit, requires advanced control methods. These control methods must be able to adjust factors such as temperature, humidity, lighting, and C02 levels to accommodate ideal conditions for crop growth \cite{devopsGreenhouseClimateControl2021} whilst keeping energy costs at a minimum. Growing crops in a controlled environment can ensure the extension of their growing season as well as protection from outside temperature and weather changes. The advent of smart and advanced greenhouses necessitates skilled labor for operation, contributing to a scarcity of qualified personnel \cite{rusnakWhatCurrentState2018}. Coupled with the escalating labour costs, the move to autonomous greenhouses is an attractive idea. \\


 Common controllers nowadays is the use of computers for the control of actuators, adjusting conditions based on set points manually specified by the grower \cite{zhangMethodologiesControlStrategies2020}. While these techniques exist, often called automatic greenhouses, the growth of crops still heavily relies on the expertise of the grower. Due to the numerous factors that affect crop growth, determining the optimal set-points becomes highly complex. The complexity is further intensified by the fact that the development of a plant is highly influenced by the control inputs taken days or even weeks in advance. Therefore, it is crucial to strategically choose control inputs now in order to maximise future rewards, such as economic profit. In order to achieve this, control strategies such as Reinforcement Learning (RL) and Model Predictive Control (MPC) can be implemented \cite{zhangMethodologiesControlStrategies2020}. Both strategies provide optimal control to pursue the same goal, which is optimizing a reward/cost function. Both control schemes have their own advantages and disadvantages. However, there is a notable similarity between the two, suggesting that combining them could lead to a more efficient solution for autonomous greenhouse control.

While both RL and MPC are used for optimal control, RL focuses on learning from interactions with an environment to maximize long-term rewards, while MPC leverages model-based predictions and optimization to determine optimal control actions over a finite time horizon. Several methods are available that seek to integrate the two control schemes, shedding light on the strengths and weaknesses of the resulting controller with respect to its specific application. There are two main methods for combining RL and MPC: using MPC as the function approximator for the RL agent, or modifying the MPC's objective function and terminal constraints to incorporate RL knowledge. The latter approach allows the learned knowledge from RL to guide and improve the performance of the MPC controller, potentially leading to more effective and adaptive control strategies. It is this approach that will be investigated in this thesis.

\section{Problem Statement}

RL's ability to learn from interactions with a highly complex environment leads to its ability to learn the optimal behaviour, even in the presence of uncertainty. However, the quality of control is strongly influenced by the training of the RL algorithm. Moreover, RL does not directly impose state constraints. While it is possible to indirectly incorporate these constraints  in the reward function with penalty functions, such an approach does not ensure that the optimal policy obtained will always adhere to these constraints. Moreover, the value function obtained through RL is only an approximation, this becomes an issue when the problem at hand is safety critical. Finally, RL faces a limitation in online flexibility due to its reliance on a feed-forward pass for policy evaluation.

In contrast, the quality of the solution obtained through MPC is subject to the accuracy of the prediction model, which is often simplified to reduce the computational burden, especially with non-convex dynamics, which may lead to sub-optimal control. Nevertheless, MPC is recognized for its sample efficiency, robustness and constraint handling. However model mismatch and uncertainties in forecasted disturbances can lead to a significant deterioration in control effectiveness. 

Arguably, the most crucial characteristic of both control strategies lies in their respective prediction horizon. Both controllers utilise future information to determine the optimal control actions. MPC achieves this by employing explicit optimization over a finite prediction horizon, while RL explores and interacts with its environment to optimise for both immediate and long-term rewards.
Hence, a shortcoming of MPC is the finite prediction horizon. This limitation becomes even more pronounced when dealing with sparse rewards and slow system dynamics. In such cases, actions taken at the current time step may only yield rewards past the prediction horizon, causing the MPC controller to be myopic. It is possible to counteract this drawback with an extended prediction horizon, but at the detriment of simplicity and computational efficiency of the controller. In general, there are no closed-loop performance guarantees for an MPC that optimises for economic benefit (EMPC). Importantly, two main methods exist to assure closed-loop performance: to use a sufficiently large horizon or the application of an appropriate terminal constraint and/or terminal cost function.

While MPC's prediction horizon is finite, RL utilizes a discounted infinite prediction horizon which allows the RL agent to weigh the benefit of future rewards on current actions. The exploration present in RL allows it to discover patterns and optimal policies, that a typical MPC might not be able to achieve, particularly in environments characterised by non-linear dynamics. A synergistic approach to combining the two control strategies would be to have a MPC controller optimize a short prediction horizon while propagating future information provided by RL. This integration of the two controllers is further justified in the context of greenhouse dynamics, where actions executed at the current time step may result in rewards that manifest over the long term. The terminal  cost function in the MPC formulation, which must encapsulate information beyond the prediction horizon, underscores the clear connection with the value function obtained through RL to to supply the necessary information for achieving the desired system performance. Notably, this may also be viewed as unrolling the value function $N$ times, and performing an N-step look-ahead minimization on the resulting equation. Nevertheless, incorporating the learned value function (typically a neural network RL) in the MPC's formulation introduces additional complexity to the problem due to its highly non-linear nature. Consequently a naive implementation of this control strategy could have severe detrimental effects. Alternatively, the RL policy can also provide the MPC with a terminal region or constraint to more effectively guide it towards a control policy that is closer to the optimal solution.

Therefore, in the development of the RL-MPC algorithm, for greenhouse control, the following research questions will be answered:

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms?} \begin{itemize}
		\item \textit{In a deterministic environment}
		\item \textit{In a stochastic environment}
	\end{itemize}
	\item \textit{What modifications and/or approximations can be employed to reduce the computational time of the RL-MPC algorithm?}
\end{itemize}




\section{Thesis Contribution}

In answering the above questions leads to several contributions in the field. Notably, among the existing works, there is a scarcity of algorithms that independently train a RL agent and subsequently employ MPC for the N-step look-ahead minimization. Specifically, none of the algorithms identified in the related literature utilize MPC for the specified N-step look-ahead minimization on the pretrained value function during online play, particularly in scenarios involving continuous state and action spaces. Lastly, it's worth noting that all the proposed RL-MPC algorithms are applied in the context of set-point or tracking regulation and are not explicitly geared towards maximizing economic benefits.
Therefore, the main contribution of this thesis will involve developing and implementing a framework that incorporates a learned value function of RL into a economic non-linear model predictive controller (ENMPC) for a continuous state and action space, and making such an algorithm generate on-time control actions. A greenhouse poses as a suitable environment and system for the development of such an algorithm due to its non-linear dynamics and continuous state and action spaces. Importantly, the objective of a greenhouse is to maximize profits from the cultivation and sale of crops. In the context of greenhouse operations, it is noteworthy that the primary objective is to maximize profits. This goal typically does not entail specific setpoint and/or tracking regulations for crop growth.


\section{Recent and Related Developments}
Various literature exists that explores the implementation of RL and MPC ,\cite{arroyoReinforcedModelPredictive2022,beckenbachAddressingInfinitehorizonOptimization2018,lubarsCombiningReinforcementLearning2021,lubbersAutonomousGreenhouseClimate2023,sikchiLearningOffPolicyOnline2021,} as well as some that delve into the theoretical background of such a controller \cite{beckenbachAddressingInfinitehorizonOptimization2018,bertsekasLessonsAlphaZeroOptimal,bertsekasNewtonMethodReinforcement2022,linReinforcementLearningBasedModel2023}.
Most notably, the works in \cite{sikchiLearningOffPolicyOnline2021,arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,bertsekasLessonsAlphaZeroOptimal} are the most similar to what is proposed in this thesis. Whereby a RL agent is trained and resulting learned value function is unrolled with the bellman equation. During online play, the optimal action to take is computed by performing an l-step look-ahead minimization on the unrolled equation. However works from \cite{arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,beckenbachAddressingInfinitehorizonOptimization2018} are the only that incorporate a MPC for the N-step look-ahead minimization. However the authors propose that the reinforcement learning process could be learned assisted with MPC, however this might impact the agents exploratory nature. Nonetheless, in all cases, the RL-MPC algorithm has shown to outperform its RL counterpart. Furthermore, to differentiate this thesis from the concept of employing MPC as a function approximator, a concise explanation will be provided. This aims to clarify to the reader what the thesis does not focus on. As discussed in \autoref{section:RL}, the DQN is usually implemented as a neural network and is parameterized by a ${\theta}$. The approximated value of a state $s$, parameterized by a weight vector  $\theta$ is then given as $ \hat{v}(s,\theta) = v^*(s)$ where $\hat{v}$ \cite{lubbersAutonomousGreenhouseClimate2023}. However, it is possible to use a MPC scheme instead of a neural network to facilitate parametrization for approximating the value function and policy. This is the goal of using MPC as a function approximator for RL. This thesis does not explore this concept. It instead explores the design and implementation of an (E)NMPC algorithm that utilizes the value function learned by RL in its optimal control problem formulation to propagate information beyond the prediction horizon. 




\section{Thesis Outline}
The subsequent sections of the thesis are outlined as follows. This thesis begins by discussing the necessary background knowledge in \autoref{chapter:Background}.  This chapter presents the greenhouse model that will be utilised and the rationale behind its selection. It also outlines the optimisation objective of the RL, MPC, and the combined RL-MPC controllers. In addition, this chapter will present the concepts of RL, MPC, and RL-MPC to the readers. \autoref{chapter:RL} explores the procedure of establishing and training the RL agent in two different environments: one without uncertainty (the nominal case) and one with uncertainty (the stochastic case). The chapter also examines the performance of the resulting agent in its respective environments. Moreover, this chapter explores the process of training accurate value functions using a fixed RL policy and discusses the obtained outcomes. \autoref{chapter:MPC} entails the formulation of the optimal control problem for the MPC and discusses the performance of the MPC in an environment with and without uncertainty. \autoref{chapter:deterministic_RL_MPC} examines the various implementations of the RL-MPC controller and evaluates their performance on the nominal case. \autoref{chapter:stochastic_RL_MPC} applies the best RL-MPC implementations from \autoref{chapter:deterministic_RL_MPC} to a stochastic environment and discusses the findings. The penulitimate chapter, \autoref{chapter:speed-up}, explores the ways in which the RL-MPC implementation can be optimised for computational efficiency and emphasises the significance of these optimisations. The thesis concludes with a presentation of the conclusion and future research in \autoref{chapter:conclusion}, along with a draft research paper provided in Appendix C.

