\chapter{Introduction}
\label{chapter:introduction, written in the present tense}

The world population is set to increase to a staggering 10 billion people in the year 2050 \cite{blazhevskaGrowingSlowerPace2019}, increasing food demand substantially. Currently, 800 million people are chronically hungry, with 2 billion people suffering from micronutrient deficiencies \cite{faoFutureFoodAgriculture2017}.The situation is compounded by the anticipated rise in food demand, which is expected to increase from 30\% to 62\% between the years 2010 and 2050, resulting in 30\% of the population being at risk of hunger \cite{vandijkMetaanalysisProjectedGlobal2021}. As such, there is a pressing need to enhance food production by at least 70\% \cite{nishatGreenDealGreenhouse2020}. Although large investments have been made to increase food productivity, food losses, waste, and climate change continue to serve as significant constraints \cite{faoFutureFoodAgriculture2017}. To meet these food demands, agriculture space has drastically increased \cite{winklerGlobalLandUse2021}; however, an increase in agriculture land space has led the sector to account for almost 15\% of the world's energy consumption while also accounting for more than 70\% of water consumption \cite{nishatGreenDealGreenhouse2020}. The need for more efficient use of space and resources is clear to increase food demands while limiting space and resource usage. Although greenhouses have been extensively used to combat these problems and have been shown to reduce the environmental burden as compared to typical open-land production \cite{munozComparingEnvironmentalImpacts2008}, they still require about 10 times more energy consumption compared to traditional farming \cite{nishatGreenDealGreenhouse2020}. This increase in energy is owed to the drastic increase in greenhouse operating costs.  Moreover, with the soaring operating energy costs associated with greenhouses and a global trend indicating an increase in gas and electricity prices \cite{alvarezWhatSoaringEnergy2021}, growers are under increasing pressure to adopt more effective growing methods. As a result, agreement policies have been signed to reduce the $CO_2$ emissions of these greenhouses to an acceptable level \cite{breukersPowerDutchGreenhouse}. \\


The imperative shift towards green practices has led to the emergence of smart greenhouses. These innovative structures are designed to enhance crop yield per hectare and improve food quality by utilizing climate-controlled environments \cite{morcegoReinforcementLearningModel2023}. Such smart greenhouses are essential in combating the degrading effects of climate change on crop quality and yield; however, maintaining such an environment requires advanced control methods. These control methods must be able to adjust factors such as temperature, humidity, lighting, and C02 levels to accommodate ideal conditions for crop growth \cite{devopsGreenhouseClimateControl2021}. Growing crops in a controlled environment can ensure the extension of their growing season as well as protection from outside temperature and weather changes. Moreover, these smart greenhouses must address the additional challenges associated with monitoring, fertilization, irrigation, and pest and disease control of plants \cite{sahooSmartGreenhouseBoon2022}, further necessitating advanced controllers. The advent of smart and advanced greenhouses necessitates skilled labor for operation, contributing to a scarcity of qualified personnel \cite{rusnakWhatCurrentState2018}. Coupled with the escalating labour costs, the move to autonomous greenhouses is an attractive idea. \\


Numerous advanced control strategies for greenhouses have been developed to address the previously mentioned challenges with the advancement of technology. Common nowadays is the use of computers for the control of actuators, adjusting conditions based on set points manually specified by the grower \cite{zhangMethodologiesControlStrategies2020}. While such techniques exist, such as automatic greenhouses, the growth of crops still heavily relies on the expertise of the grower. Given the multitude of factors influencing crop growth, the decision space for setting optimal points becomes immensely complex.  Moreover, this control scheme falls behind state-of-the-art technologies, and it is argued that integrated optimal control ensures the best economic results \cite{vanstratenOptimalGreenhouseCultivation2010}.Therefore, to achieve greater autonomy while staying abreast of technology, control strategies such as Reinforcement Learning (RL) and Model Predictive Control (MPC) have been implemented \cite{zhangMethodologiesControlStrategies2020}. Both strategies provide optimal control to pursue the same goal.  Both types of control schemes offer their respective advantages and disadvantages; however, there is a keen similarity between the two, whereby the combination of the two could result in a more effective solution for autonomous greenhouse control.

RL and MPC are both techniques used in the field of control theory, but they have distinct approaches and strategies. While RL and MPC share the overarching goal through optimal control, their methodologies differ, with RL focusing on learning from interactions and MPC relying on model-based optimization over a finite time horizon. 
Several methods are available that seek to integrate the two control schemes, shedding light on the strengths and weaknesses of the resulting controller with respect to its specific application. There exist two primary approaches to integrate Reinforcement Learning (RL) and Model Predictive Control (MPC): employing MPC as the function approximator for the learning agent or truncating the MPC's objective function while integrating RL knowledge into the MPC formulation. The latter strategy allows the learned knowledge from RL to guide and improve the performance of the MPC controller, potentially leading to more effective and adaptive control strategies.

\section{Problem Statement}

RL's ability to learn from interactions with a highly complex environment leads to its high adaptability and uncertainty handling. However, the quality of control is strongly influenced by the training of the reinforcement learning (RL) algorithm. Moreover, RL does not directly impose state and control constraints. While it is possible to indirectly incorporate these constraints through the reward function, such an approach does not ensure that the optimal policy obtained will inherently adhere to these constraints. Moreover, the value function obtained through RL is only an approximation, this becomes an issue when the problem at hand is safety critical. Finally, Reinforcement Learning (RL) faces a limitation in online flexibility due to its reliance on a straightforward feed-forward pass for policy evaluation.

In contrast, the quality of the solution obtained through MPC is subject to the accuracy of the prediction model, which is often simplified to reduce the computational burden, however non-convex dynamics may lead to sub-optimal control. MPC is recognized for its sample efficiency, robustness and constraint handling. However, it falls short in terms of adaptability. Finally, model mismatch and uncertainties in forecasted disturbances can lead to a significant deterioration in control effectiveness. 

Arguably, the most crucial characteristic of both control strategies lies in their respective prediction horizon. Both are predictive controllers, with Model Predictive Control (MPC) employing explicit optimization over a finite prediction horizon to determine the optimal control action. On the other hand, Reinforcement Learning (RL) explores and interacts with its environment to learn the optimal action, optimizing for both immediate and infinite discounted future rewards.
Hence, a shortcoming of MPC is the finite prediction horizon. This limitation becomes even more pronounced when dealing with sparse rewards and slow system dynamics. In such cases, actions taken at the current time step may only yield rewards past the prediction horizon, causing the MPC controller to be myopic. It is possible to counteract this drawback with an extended prediction horizon, but at the detriment of simplicity and computational efficiency of the controller. Furthermore, the closed-loop performance of an MPC that optimizes for economic benefit (EMPC) does not consider the system's dynamics, and although it optimizes the process economics, it does so over a finite time horizon. Therefore, over a long period of operation, there is in general no closed-loop performance guarantees under EMPC. Importantly, only two methodologies exist to assure closed-loop performance: to use a sufficiently large horizon or the application of an appropriate terminal constraint.

Reinforcement Learning (RL) utilizes a discounted infinite prediction horizon, typically leading to enhanced long-term behavior. The exploration present in RL allows it to discover global patterns, that a typical nonlinear model predicitve controller (NMPC) might not be able to achieve.  A synergistic approach to combining the two control strategies would be to have a MPC controller optimize a short prediction horizon while propagating global information provided by RL. This integration of the two controllers is further justified in the context of greenhouse dynamics, where actions executed at the current time step may result in rewards that manifest over the long term. The terminal constraint in the MPC formulation, which must encapsulate information beyond the prediction horizon, underscores the evident synergy with the value function acquired through RL to provide the required information for the desired system performance. Notably, this may also be viewed as unrolling the bellman equation (as already approximated by RL) $l$ times, and performing an l-step look-ahead minimization on the resulting equation.

\begin{itemize}[itemsep=7pt] % Adjust the value of itemsep to change spacing
	\item \textit{How can the value function learned by RL be incorporated into an (E)NMPC formulation to propagate information beyond the prediction horizon to maximize economic benefit?}
	\item \textit{What modifications/approximations can be employed to reduce computational time?}
	\item \textit{How does the economic performance of the RL-MPC algorithm compare to the standalone RL and MPC algorithms?}
\end{itemize}




\section{Thesis Contribution}

In answering the above questions leads to several contributions in the field. Notably, among the existing works, there is a scarcity of algorithms that independently train a RL agent and subsequently employ MPC for the l-step look-ahead minimization. Specifically, none of the algorithms identified in the related literature utilize MPC for the specified l-step look-ahead minimization on the pretrained value function during online play, particularly in scenarios involving continuous state and action spaces. Lastly, it's worth noting that all the proposed RL-MPC algorithms are applied in the context of set-point or tracking regulation and are not explicitly geared towards maximizing economic benefits.
Therefore, the main contribution of this thesis will involve developing and implementing a framework that incorporates the learned value function of RL into a economic non-linear model predictive controller (ENMPC) for a continuous state and action space, and making such an algorithm generate on-time control actions. A greenhouse poses as a suitable environment and system for the development of such an algorithm due to its non-linear dynamics and continuous state and action spaces. Importantly, the objective of a greenhouse is to maximize profits from the cultivation and sale of crops. In the context of greenhouse operations, it is noteworthy that the primary objective is to maximize profits. This goal typically does not entail specific setpoint and/or tracking regulations for crop growth.


\section{Recent and Related Developments}
Various literature exists that explores the implementation of RL and MPC ,\cite{arroyoReinforcedModelPredictive2022,beckenbachAddressingInfinitehorizonOptimization2018,lubarsCombiningReinforcementLearning2021,lubbersAutonomousGreenhouseClimate2023,sikchiLearningOffPolicyOnline2021,} as well as some that delve into the theoretical background of such a controller \cite{beckenbachAddressingInfinitehorizonOptimization2018,bertsekasLessonsAlphaZeroOptimal,bertsekasNewtonMethodReinforcement2022,linReinforcementLearningBasedModel2023}.
Most notably, the works in \cite{sikchiLearningOffPolicyOnline2021,arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,bertsekasLessonsAlphaZeroOptimal} are the most similar to what is proposed in this thesis. Whereby a RL agent is trained and resulting learned value function is unrolled with the bellman equation. During online play, the optimal action to take is computed by performing an l-step look-ahead minimization on the unrolled equation. However works from \cite{arroyoReinforcedModelPredictive2022,linReinforcementLearningBasedModel2023,beckenbachAddressingInfinitehorizonOptimization2018} are the only that incorporate a MPC for the l-step look-ahead minimization. However the authors propose that the reinforcement learning process could be learned assisted with MPC, however this might impact the agents exploratory nature. Nonetheless, in all cases, the RL-MPC algorithm has shown to outperform its RL counterpart. A more comprehensive analyses of the related literature may be found in the literature review.




\section{Thesis Outline}
The subsequent sections of the thesis are outlined as follows. This thesis begins by discussing the necessary background knowledge in \autoref{chapter:Background}.  This chapter presents the greenhouse model that will be utilised and the rationale behind its selection. It also outlines the optimisation objective of the RL, MPC, and the combined RL-MPC controllers. In addition, this chapter will present the concepts of RL, MPC, and RL-MPC to the readers. \autoref{chapter:RL} explores the procedure of establishing and training the RL agent in two different environments: one without uncertainty (the nominal case) and one with uncertainty (the stochastic case). The chapter also examines the performance of the resulting agent in its respective environments. Moreover, this chapter explores the process of training accurate value functions using a fixed RL policy and discusses the obtained outcomes. \autoref{chapter:MPC} entails the formulation of the optimal control problem for the MPC and discusses the performance of the MPC in an environment with and without uncertainty. The chapter titled \autoref{chapter:deterministic_RL_MPC} examines the various implementations of the RL-MPC controller and evaluates their performance in the nominal case. \autoref{chapter:stochastic_RL_MPC} applies the best RL-MPC implementations from \autoref{chapter:deterministic_RL_MPC} to a stochastic environment and discusses the findings. Second to last, \autoref{chapter:speed-up} explores the ways in which the RL-MPC implementation can be optimised for computational efficiency and emphasises the significance of this optimisation. The thesis concludes with a presentation of the conclusion and future research in \autoref{chapter:conclusion}, along with a draft research paper provided in Appendix C.

